import{j as e}from"./vendor-animation-6GFvN5rC.js";import{L as t,D as a,T as n,E as r}from"./ContentBlocks-ByGNY4hF.js";import{I as s,M as i}from"./MathBlock-BTK9YHZB.js";import"./vendor-react-ByzHzWFU.js";import"./index-CIaFkRw9.js";import"./vendor-math-ClxlXyPc.js";import"./vendor-firebase-core-DIJkQv9Q.js";import"./index-Djvuv9mj.js";function p(){return e.jsxs(t,{sectionId:30,children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Principal Component Analysis (PCA)"})," is one of the most important applications of the SVD. It finds the directions of maximum variance in data, enabling dimensionality reduction."]}),e.jsx("h2",{children:"The Data Matrix"}),e.jsxs(a,{title:"Centered Data Matrix",className:"my-6",children:[e.jsxs("p",{children:["Given ",e.jsx(s,{children:"n"})," data points in ",e.jsx(s,{children:"\\mathbb{R}^m"}),", form the data matrix:"]}),e.jsx(i,{children:"A = \\begin{bmatrix} | & | & & | \\\\ \\mathbf{a}_1 & \\mathbf{a}_2 & \\cdots & \\mathbf{a}_n \\\\ | & | & & | \\end{bmatrix}"}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Center the data"})," by subtracting the mean from each column:"]}),e.jsx(i,{children:"\\bar{\\mathbf{a}} = \\frac{1}{n}(\\mathbf{a}_1 + \\cdots + \\mathbf{a}_n)"})]}),e.jsx("h2",{children:"Covariance and Variance"}),e.jsxs(a,{title:"Sample Covariance Matrix",className:"my-6",children:[e.jsxs("p",{children:["For centered data, the ",e.jsx("strong",{children:"sample covariance matrix"})," is:"]}),e.jsx(i,{children:"S = \\frac{1}{n-1}AA^T"}),e.jsxs("p",{className:"mt-2",children:[e.jsx(s,{children:"S_{ij}"})," measures how variables ",e.jsx(s,{children:"i"})," and ",e.jsx(s,{children:"j"})," vary together. Diagonal entries are variances."]})]}),e.jsx("h2",{children:"Principal Components"}),e.jsxs(n,{title:"Principal Component Analysis",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:["The variance of centered data projected onto unit vector ",e.jsx(s,{children:"q"})," is:"]}),e.jsx(i,{children:"\\text{Var}_q = \\frac{1}{n-1}\\sum_{i=1}^n (q^T a_i)^2 = \\frac{1}{n-1}q^T A A^T q = q^T S q"}),e.jsxs("p",{className:"mt-2",children:["To maximize variance subject to ",e.jsx(s,{children:"\\\\|q\\\\| = 1"}),", use Lagrange multipliers:"]}),e.jsx(i,{children:"\\nabla(q^T S q - \\lambda(q^T q - 1)) = 0 \\implies 2Sq = 2\\lambda q"}),e.jsxs("p",{className:"mt-2",children:["So the maximum variance is achieved at eigenvectors of ",e.jsx(s,{children:"S"}),". The eigenvalue ",e.jsx(s,{children:"\\\\lambda"})," equals the variance: ",e.jsx(s,{children:"q^T S q = q^T (\\\\lambda q) = \\\\lambda"}),"."]}),e.jsxs("p",{className:"mt-2",children:["The largest eigenvalue ",e.jsx(s,{children:"\\\\lambda_1"})," gives maximum variance with eigenvector ",e.jsx(s,{children:"q_1"}),". The second PC ",e.jsx(s,{children:"q_2"})," is orthogonal to ",e.jsx(s,{children:"q_1"})," (since eigenvectors of symmetric matrices are orthogonal) and maximizes variance in the orthogonal complement."]})]}),children:[e.jsxs("p",{children:["The ",e.jsx("strong",{children:"principal components"})," are the eigenvectors of the covariance matrix ",e.jsx(s,{children:"S"}),":"]}),e.jsx(i,{children:"S = Q\\Lambda Q^T"}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-2",children:[e.jsxs("li",{children:["The first PC ",e.jsx(s,{children:"q_1"})," points in the direction of maximum variance"]}),e.jsxs("li",{children:["The second PC ",e.jsx(s,{children:"q_2"})," is orthogonal to ",e.jsx(s,{children:"q_1"})," with next-highest variance"]}),e.jsxs("li",{children:["Eigenvalue ",e.jsx(s,{children:"\\lambda_i"})," = variance in direction ",e.jsx(s,{children:"q_i"})]})]})]}),e.jsxs(r,{title:"2D Data Example",className:"my-6",children:[e.jsx("p",{children:"For data scattered in an elliptical pattern:"}),e.jsxs("ul",{className:"list-disc list-inside mt-2",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"First PC:"})," direction along the long axis of the ellipse"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Second PC:"})," perpendicular, along the short axis"]}),e.jsxs("li",{children:["The ratio ",e.jsx(s,{children:"\\lambda_1/\\lambda_2"})," tells how elongated the ellipse is"]})]})]}),e.jsx("h2",{children:"PCA via SVD"}),e.jsxs(n,{title:"PCA from the SVD",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:["Given ",e.jsx(s,{children:"A = U\\Sigma V^T"}),", the covariance matrix is:"]}),e.jsx(i,{children:"S = \\frac{1}{n-1}AA^T = \\frac{1}{n-1}U\\Sigma V^T V \\Sigma^T U^T = \\frac{1}{n-1}U(\\Sigma\\Sigma^T)U^T"}),e.jsxs("p",{className:"mt-2",children:["This is a spectral decomposition: columns of ",e.jsx(s,{children:"U"})," are eigenvectors of ",e.jsx(s,{children:"S"}),", with eigenvalues ",e.jsx(s,{children:"\\lambda_i = \\sigma_i^2/(n-1)"}),"."]}),e.jsxs("p",{className:"mt-2",children:["For the projected coordinates, if we project data point ",e.jsx(s,{children:"a_i"})," onto PC ",e.jsx(s,{children:"u_j"}),":"]}),e.jsx(i,{children:"\\text{score}_{ij} = u_j^T a_i"}),e.jsxs("p",{className:"mt-2",children:["In matrix form: ",e.jsx(s,{children:"U^T A = U^T U \\Sigma V^T = \\Sigma V^T"}),". So the matrix of scores is ",e.jsx(s,{children:"\\Sigma V^T"}),"."]})]}),children:[e.jsxs("p",{children:["If ",e.jsx(s,{children:"A = U\\Sigma V^T"})," is the SVD of the centered data matrix:"]}),e.jsxs("ul",{className:"list-disc list-inside space-y-2",children:[e.jsxs("li",{children:["Principal components are the columns of ",e.jsx(s,{children:"U"})]}),e.jsxs("li",{children:["Singular values relate to eigenvalues: ",e.jsx(s,{children:"\\lambda_i = \\sigma_i^2/(n-1)"})]}),e.jsxs("li",{children:["The coordinates in the new basis are ",e.jsx(s,{children:"\\Sigma V^T"})]})]}),e.jsx("p",{className:"mt-2 text-primary-400",children:"PCA via SVD is numerically more stable than eigendecomposition."})]}),e.jsx("h2",{children:"Dimensionality Reduction"}),e.jsxs(a,{title:"Low-Dimensional Projection",className:"my-6",children:[e.jsxs("p",{children:["To reduce from ",e.jsx(s,{children:"m"})," dimensions to ",e.jsx(s,{children:"k"}),":"]}),e.jsx(i,{children:"\\mathbf{y} = Q_k^T \\mathbf{x}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"Q_k"})," contains the first ",e.jsx(s,{children:"k"})," principal components. The ",e.jsx("strong",{children:"reconstruction"})," is ",e.jsx(s,{children:"\\hat{\\mathbf{x}} = Q_k \\mathbf{y}"}),"."]})]}),e.jsxs(n,{title:"Optimal Low-Rank Approximation",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsx("p",{children:"The total variance equals the sum of all eigenvalues:"}),e.jsx(i,{children:"\\text{Total variance} = \\text{tr}(S) = \\lambda_1 + \\cdots + \\lambda_m"}),e.jsxs("p",{className:"mt-2",children:["When we project onto the first ",e.jsx(s,{children:"k"})," PCs, the reconstruction is:"]}),e.jsx(i,{children:"\\hat{A} = Q_k Q_k^T A"}),e.jsxs("p",{className:"mt-2",children:["By the Eckart-Young theorem, this minimizes ",e.jsx(s,{children:"\\|A - \\hat{A}\\|_F^2"}),"."]}),e.jsxs("p",{className:"mt-2",children:["The variance captured by the first ",e.jsx(s,{children:"k"})," components is ",e.jsx(s,{children:"\\lambda_1 + \\cdots + \\lambda_k"}),". The reconstruction error in variance is ",e.jsx(s,{children:"\\lambda_{k+1} + \\cdots + \\lambda_m"}),"."]}),e.jsx("p",{className:"mt-2",children:"Thus the fraction of variance retained is:"}),e.jsx(i,{children:"\\frac{\\lambda_1 + \\cdots + \\lambda_k}{\\lambda_1 + \\cdots + \\lambda_m} = \\frac{\\sigma_1^2 + \\cdots + \\sigma_k^2}{\\sigma_1^2 + \\cdots + \\sigma_m^2}"})]}),children:[e.jsxs("p",{children:["The first ",e.jsx(s,{children:"k"})," principal components give the best ",e.jsx(s,{children:"k"}),"-dimensional representation of the data (in the least-squares sense)."]}),e.jsx(i,{children:"\\text{Fraction of variance retained} = \\frac{\\lambda_1 + \\cdots + \\lambda_k}{\\lambda_1 + \\cdots + \\lambda_m}"})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 my-6 border border-primary-500/20",children:[e.jsx("p",{className:"font-semibold text-primary-400 mb-2",children:"PCA Applications"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"Image recognition:"})," Eigenfaces for face recognition"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Gene expression:"})," Find patterns in high-dimensional gene data"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Finance:"})," Identify factors driving stock returns"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Visualization:"})," Project high-dimensional data to 2D or 3D"]})]})]}),e.jsxs(r,{title:"Image Compression with PCA",className:"my-6",children:[e.jsx("p",{children:"Treat each image as a vector. For 1000 face images (64×64 pixels = 4096 dimensions):"}),e.jsxs("ul",{className:"list-disc list-inside mt-2",children:[e.jsx("li",{children:"Stack images as columns: 4096 × 1000 matrix"}),e.jsx("li",{children:"Find principal components (eigenfaces)"}),e.jsx("li",{children:"First 50-100 components capture most variation"}),e.jsx("li",{children:"Represent each face as 50-100 numbers instead of 4096"})]})]}),e.jsx("h2",{children:"Key Ideas"}),e.jsx("div",{className:"bg-gradient-to-br from-primary-500/10 to-dark-800/50 rounded-xl p-6 my-6 border border-primary-500/20",children:e.jsxs("ul",{className:"space-y-3 text-dark-200",children:[e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"1."}),e.jsx("span",{children:"PCA finds orthogonal directions of maximum variance."})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"2."}),e.jsx("span",{children:"Principal components are eigenvectors of the covariance matrix."})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"3."}),e.jsxs("span",{children:["Compute PCA via SVD: ",e.jsx(s,{children:"A = U\\Sigma V^T"}),", PCs are columns of ",e.jsx(s,{children:"U"}),"."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"4."}),e.jsxs("span",{children:["Reduce dimensions by keeping only the first ",e.jsx(s,{children:"k"})," components."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"5."}),e.jsx("span",{children:"Eigenvalues measure variance; sum of retained eigenvalues = variance explained."})]})]})})]})}export{p as default};
