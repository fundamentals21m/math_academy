import{j as s}from"./vendor-animation-CmGrHCVd.js";import{L as t,D as a,T as l,E as r}from"./ContentBlocks-CmxP2e2i.js";import{M as i,I as e}from"./MathBlock-DHbtlfU5.js";import"./vendor-react-C1UuhE6f.js";import"./index-BWj-jU2X.js";import"./index-Djvuv9mj.js";import"./vendor-math-ClxlXyPc.js";function b(){return s.jsxs(t,{sectionId:49,children:[s.jsxs("p",{children:[s.jsx("strong",{children:"Linear regression"}),' is the most important statistical model. It finds the best linear relationship between variablesâ€”and "best" means least squares, which is pure linear algebra.']}),s.jsx("h2",{children:"The Linear Model"}),s.jsxs(a,{title:"Linear Regression Model",className:"my-6",children:[s.jsx("p",{children:"The model assumes:"}),s.jsx(i,{children:"y = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}"}),s.jsxs("p",{className:"mt-2",children:[s.jsx(e,{children:"y"})," = response vector (",s.jsx(e,{children:"n \\times 1"}),")",s.jsx("br",{}),s.jsx(e,{children:"X"})," = design matrix (",s.jsx(e,{children:"n \\times p"}),")",s.jsx("br",{}),s.jsx(e,{children:"\\boldsymbol{\\\\beta}"})," = unknown coefficients (",s.jsx(e,{children:"p \\times 1"}),")",s.jsx("br",{}),s.jsx(e,{children:"\\boldsymbol{\\\\varepsilon}"})," = random errors"]})]}),s.jsx("h2",{children:"Least Squares Solution"}),s.jsxs(l,{title:"Normal Equations",className:"my-6",proof:s.jsxs(s.Fragment,{children:[s.jsxs("p",{children:[s.jsx("strong",{children:"Minimize squared error:"})," Let ",s.jsx(e,{children:"f(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - X\\boldsymbol{\\beta}\\|^2 = (\\mathbf{y} - X\\boldsymbol{\\beta})^T(\\mathbf{y} - X\\boldsymbol{\\beta})"}),"."]}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Expand:"})," ",s.jsx(e,{children:"f = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T X^T \\mathbf{y} + \\boldsymbol{\\beta}^T X^T X \\boldsymbol{\\beta}"}),"."]}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Take derivative:"})," ",s.jsx(e,{children:"\\frac{\\partial f}{\\partial \\boldsymbol{\\beta}} = -2X^T\\mathbf{y} + 2X^TX\\boldsymbol{\\beta} = 0"}),"."]}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Solve:"})," ",s.jsx(e,{children:"X^TX\\hat{\\boldsymbol{\\beta}} = X^T\\mathbf{y}"}),", giving ",s.jsx(e,{children:"\\hat{\\boldsymbol{\\beta}} = (X^TX)^{-1}X^T\\mathbf{y}"}),"."]}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Projection view:"})," The error ",s.jsx(e,{children:"\\mathbf{e} = \\mathbf{y} - X\\hat{\\boldsymbol{\\beta}}"})," satisfies ",s.jsx(e,{children:"X^T\\mathbf{e} = 0"}),", meaning ",s.jsx(e,{children:"\\mathbf{e}"})," is orthogonal to ",s.jsx(e,{children:"C(X)"}),"."]})]}),children:[s.jsxs("p",{children:["The ",s.jsx("strong",{children:"least squares estimate"})," ",s.jsx(e,{children:"\\hat{\\boldsymbol{\\beta}}"})," minimizes ",s.jsx(e,{children:"\\|y - X\\boldsymbol{\\beta}\\|^2"}),":"]}),s.jsx(i,{children:"X^TX\\hat{\\boldsymbol{\\beta}} = X^Ty"}),s.jsx(i,{children:"\\hat{\\boldsymbol{\\beta}} = (X^TX)^{-1}X^Ty"}),s.jsxs("p",{className:"mt-2 text-primary-400",children:["This is exactly the projection formula! ",s.jsx(e,{children:"\\hat{y} = X\\hat{\\boldsymbol{\\beta}}"})," is the projection of ",s.jsx(e,{children:"y"})," onto ",s.jsx(e,{children:"C(X)"}),"."]})]}),s.jsxs(r,{title:"Simple Linear Regression",className:"my-6",children:[s.jsxs("p",{children:["For ",s.jsx(e,{children:"y = \\beta_0 + \\beta_1 x"}),", the design matrix is:"]}),s.jsx(i,{children:"X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}"}),s.jsx("p",{className:"mt-2",children:"Solving gives the familiar formulas for slope and intercept."})]}),s.jsx("h2",{children:"Geometric Interpretation"}),s.jsx(l,{title:"Projection Interpretation",className:"my-6",proof:s.jsxs(s.Fragment,{children:[s.jsxs("p",{children:[s.jsx("strong",{children:"Hat matrix:"})," ",s.jsxs(e,{children:["H = X(X^TX)^",-1,"X^T"]})," satisfies ",s.jsx(e,{children:"H^2 = H"})," (idempotent) and ",s.jsx(e,{children:"H^T = H"})," (symmetric)."]}),s.jsx("p",{className:"mt-2",children:"These are the defining properties of an orthogonal projection matrix."}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Projects onto C(X):"})," For any ",s.jsx(e,{children:"\\mathbf{v} \\in C(X)"}),", write ",s.jsx(e,{children:"\\mathbf{v} = X\\mathbf{c}"}),". Then ",s.jsx(e,{children:"H\\mathbf{v} = X(X^TX)^{-1}X^TX\\mathbf{c} = X\\mathbf{c} = \\mathbf{v}"}),"."]}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Orthogonal residuals:"})," ",s.jsx(e,{children:"\\mathbf{e} = (I-H)\\mathbf{y}"}),". Since ",s.jsx(e,{children:"(I-H)H = 0"}),", the residual is orthogonal to the fitted values and hence to ",s.jsx(e,{children:"C(X)"}),"."]})]}),children:s.jsxs("ul",{className:"list-disc list-inside space-y-2",children:[s.jsxs("li",{children:[s.jsx(e,{children:"\\hat{y} = X(X^TX)^{-1}X^Ty"})," = projection of ",s.jsx(e,{children:"y"})," onto column space of ",s.jsx(e,{children:"X"})]}),s.jsxs("li",{children:[s.jsx(e,{children:"H = X(X^TX)^{-1}X^T"}),' = "hat matrix" (projection matrix)']}),s.jsxs("li",{children:["Residuals ",s.jsx(e,{children:"e = y - \\hat{y}"})," are orthogonal to columns of ",s.jsx(e,{children:"X"})]})]})}),s.jsx("h2",{children:"Variance of Estimates"}),s.jsxs(l,{title:"Covariance of Beta-Hat",className:"my-6",proof:s.jsxs(s.Fragment,{children:[s.jsxs("p",{children:[s.jsx("strong",{children:"Setup:"})," ",s.jsx(e,{children:"\\hat{\\boldsymbol{\\beta}} = (X^TX)^{-1}X^T\\mathbf{y} = (X^TX)^{-1}X^T(X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = \\boldsymbol{\\beta} + (X^TX)^{-1}X^T\\boldsymbol{\\varepsilon}"}),"."]}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Apply covariance transformation:"})," Since ",s.jsx(e,{children:"\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta} = (X^TX)^{-1}X^T\\boldsymbol{\\varepsilon}"}),":"]}),s.jsx(i,{children:"\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = (X^TX)^{-1}X^T \\cdot \\text{Cov}(\\boldsymbol{\\varepsilon}) \\cdot X(X^TX)^{-1}"}),s.jsxs("p",{className:"mt-2",children:["With ",s.jsx(e,{children:"\\text{Cov}(\\boldsymbol{\\varepsilon}) = \\sigma^2 I"}),":"]}),s.jsx(i,{children:"= \\sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1} = \\sigma^2(X^TX)^{-1}"})]}),children:[s.jsxs("p",{children:["If ",s.jsx(e,{children:"\\text{Cov}(\\boldsymbol{\\\\varepsilon}) = \\sigma^2 I"}),":"]}),s.jsx(i,{children:"\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(X^TX)^{-1}"}),s.jsxs("p",{className:"mt-2",children:["The estimate ",s.jsx(e,{children:"\\hat{\\sigma}^2 = \\|e\\|^2/(n-p)"})," from residuals."]})]}),s.jsx("h2",{children:"R-Squared"}),s.jsxs(a,{title:"Coefficient of Determination",className:"my-6",children:[s.jsx(i,{children:"R^2 = 1 - \\frac{\\|e\\|^2}{\\|y - \\bar{y}\\|^2} = \\frac{\\|\\hat{y} - \\bar{y}\\|^2}{\\|y - \\bar{y}\\|^2}"}),s.jsxs("p",{className:"mt-2",children:[s.jsx(e,{children:"R^2"})," = fraction of variance explained by the model (0 to 1)."]})]}),s.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 my-6 border border-primary-500/20",children:[s.jsx("p",{className:"font-semibold text-primary-400 mb-2",children:"Pythagorean Theorem in Regression"}),s.jsxs("p",{className:"text-dark-300 text-sm",children:[s.jsx(e,{children:"\\|y - \\bar{y}\\|^2 = \\|\\hat{y} - \\bar{y}\\|^2 + \\|e\\|^2"}),s.jsx("br",{}),s.jsx("br",{}),"Total variance = Explained variance + Unexplained variance",s.jsx("br",{}),"This is orthogonal decomposition!"]})]}),s.jsx("h2",{children:"Multiple Regression"}),s.jsxs(r,{title:"Multiple Regression",className:"my-6",children:[s.jsxs("p",{children:["With multiple predictors ",s.jsx(e,{children:"y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots"}),":"]}),s.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[s.jsx("li",{children:"Coefficients account for other variables (partial effects)"}),s.jsxs("li",{children:["Multicollinearity: if columns of ",s.jsx(e,{children:"X"})," are nearly dependent, ",s.jsx(e,{children:"(X^TX)^{-1}"})," is ill-conditioned"]}),s.jsxs("li",{children:["Regularization (ridge regression) adds ",s.jsx(e,{children:"\\lambda I"})," to stabilize"]})]})]}),s.jsx("h2",{children:"Key Ideas"}),s.jsx("div",{className:"bg-gradient-to-br from-primary-500/10 to-dark-800/50 rounded-xl p-6 my-6 border border-primary-500/20",children:s.jsxs("ul",{className:"space-y-3 text-dark-200",children:[s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"1."}),s.jsxs("span",{children:["Least squares: ",s.jsx(e,{children:"\\hat{\\boldsymbol{\\beta}} = (X^TX)^{-1}X^Ty"}),"."]})]}),s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"2."}),s.jsxs("span",{children:[s.jsx(e,{children:"\\hat{y}"})," is the projection of ",s.jsx(e,{children:"y"})," onto column space of ",s.jsx(e,{children:"X"}),"."]})]}),s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"3."}),s.jsxs("span",{children:["Residuals ",s.jsx(e,{children:"e"})," are orthogonal to columns of ",s.jsx(e,{children:"X"}),"."]})]}),s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"4."}),s.jsxs("span",{children:[s.jsx(e,{children:"R^2"})," = fraction of variance explained."]})]}),s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"5."}),s.jsxs("span",{children:["Variance of ",s.jsx(e,{children:"\\hat{\\boldsymbol{\\beta}}"}),": ",s.jsx(e,{children:"\\sigma^2(X^TX)^{-1}"}),"."]})]})]})})]})}export{b as default};
