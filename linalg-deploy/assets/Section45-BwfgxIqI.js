import{j as e}from"./vendor-animation-8XabJWu9.js";import{L as t,D as r,T as a,E as l}from"./ContentBlocks-KQXPsK8X.js";import{I as s,M as i}from"./MathBlock-ByDekCQ-.js";import"./vendor-react-dnKiRYoA.js";import"./index-D4aEnt1z.js";import"./vendor-math-ClxlXyPc.js";import"./vendor-firebase-core-DIJkQv9Q.js";import"./index-CHJUcX0S.js";function j(){return e.jsxs(t,{sectionId:45,children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Numerical linear algebra"})," deals with computing solutions efficiently and accurately. In practice, we never work with infinite precision—understanding roundoff errors and stability is essential."]}),e.jsx("h2",{children:"Floating Point Arithmetic"}),e.jsxs(r,{title:"Machine Epsilon",className:"my-6",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Machine epsilon"})," ",e.jsx(s,{children:"\\epsilon_{\\text{mach}}"})," is the smallest number such that:"]}),e.jsx(i,{children:"1 + \\epsilon_{\\text{mach}} > 1"}),e.jsxs("p",{className:"mt-2",children:["For double precision: ",e.jsx(s,{children:"\\epsilon_{\\text{mach}} \\approx 10^{-16}"})," (about 16 digits of accuracy)."]})]}),e.jsxs(a,{title:"Fundamental Axiom",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Rounding model:"})," Floating-point numbers are represented as ",e.jsx(s,{children:"\\pm m \\times 2^e"})," where the mantissa ",e.jsx(s,{children:"m"})," has finite precision."]}),e.jsxs("p",{className:"mt-2",children:["When we compute ",e.jsx(s,{children:"a \\circ b"}),", the exact result must be rounded to the nearest representable number."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Rounding error:"})," The relative error from rounding is at most ",e.jsx(s,{children:"\\epsilon_{\\text{mach}}/2"})," (round to nearest) or ",e.jsx(s,{children:"\\epsilon_{\\text{mach}}"})," (truncation)."]}),e.jsxs("p",{className:"mt-2",children:["For IEEE double precision: 52 mantissa bits means ",e.jsx(s,{children:"\\epsilon_{\\text{mach}} = 2^{-52} \\approx 2.2 \\times 10^{-16}"}),"."]})]}),children:[e.jsxs("p",{children:["Every floating-point operation introduces relative error at most ",e.jsx(s,{children:"\\epsilon_{\\text{mach}}"}),":"]}),e.jsx(i,{children:"\\text{fl}(a \\circ b) = (a \\circ b)(1 + \\delta), \\quad |\\delta| \\leq \\epsilon_{\\text{mach}}"})]}),e.jsx("h2",{children:"Condition Number"}),e.jsxs(r,{title:"Condition Number of a Matrix",className:"my-6",children:[e.jsxs("p",{children:["The ",e.jsx("strong",{children:"condition number"})," measures sensitivity to errors:"]}),e.jsx(i,{children:"\\kappa(A) = \\|A\\| \\|A^{-1}\\| = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}"}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[e.jsxs("li",{children:[e.jsx(s,{children:"\\kappa \\approx 1"}),": well-conditioned"]}),e.jsxs("li",{children:[e.jsx(s,{children:"\\kappa \\gg 1"}),": ill-conditioned"]}),e.jsxs("li",{children:[e.jsx(s,{children:"\\kappa = \\infty"}),": singular"]})]})]}),e.jsxs(l,{title:"Hilbert Matrix",className:"my-6",children:[e.jsxs("p",{children:["The ",e.jsx(s,{children:"n \\times n"})," Hilbert matrix ",e.jsx(s,{children:"H_{ij} = 1/(i+j-1)"})," is notoriously ill-conditioned:"]}),e.jsx(i,{children:"H_5 = \\begin{bmatrix} 1 & 1/2 & 1/3 & 1/4 & 1/5 \\\\ 1/2 & 1/3 & 1/4 & 1/5 & 1/6 \\\\ \\vdots & & & & \\vdots \\end{bmatrix}"}),e.jsxs("p",{className:"mt-2",children:[e.jsx(s,{children:"\\kappa(H_5) \\approx 4.8 \\times 10^5"}),". For ",e.jsx(s,{children:"H_{10}"}),", ",e.jsx(s,{children:"\\kappa \\approx 10^{13}"}),"!"]})]}),e.jsxs(a,{title:"Error Bound",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Setup:"})," Let ",e.jsx(s,{children:"A(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}"}),". Then ",e.jsx(s,{children:"A\\delta\\mathbf{x} = \\delta\\mathbf{b}"}),", so ",e.jsx(s,{children:"\\delta\\mathbf{x} = A^{-1}\\delta\\mathbf{b}"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Bound on error:"})," Taking norms:"]}),e.jsx(i,{children:"\\|\\delta\\mathbf{x}\\| = \\|A^{-1}\\delta\\mathbf{b}\\| \\leq \\|A^{-1}\\| \\|\\delta\\mathbf{b}\\|"}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Relative error:"})," From ",e.jsx(s,{children:"\\mathbf{b} = A\\mathbf{x}"}),": ",e.jsx(s,{children:"\\|\\mathbf{b}\\| \\leq \\|A\\|\\|\\mathbf{x}\\|"}),", so ",e.jsx(s,{children:"1/\\|\\mathbf{x}\\| \\leq \\|A\\|/\\|\\mathbf{b}\\|"}),"."]}),e.jsx(i,{children:"\\frac{\\|\\delta\\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\leq \\|A^{-1}\\| \\|\\delta\\mathbf{b}\\| \\cdot \\frac{\\|A\\|}{\\|\\mathbf{b}\\|} = \\kappa(A) \\frac{\\|\\delta\\mathbf{b}\\|}{\\|\\mathbf{b}\\|}"})]}),children:[e.jsxs("p",{children:["When solving ",e.jsx(s,{children:"Ax = b"})," with small perturbation ",e.jsx(s,{children:"\\delta b"}),":"]}),e.jsx(i,{children:"\\frac{\\|\\delta x\\|}{\\|x\\|} \\leq \\kappa(A) \\frac{\\|\\delta b\\|}{\\|b\\|}"}),e.jsxs("p",{className:"mt-2",children:["If ",e.jsx(s,{children:"\\kappa = 10^{10}"}),", a tiny error in ",e.jsx(s,{children:"b"})," can cause a huge error in ",e.jsx(s,{children:"x"}),"."]})]}),e.jsx("h2",{children:"Stability"}),e.jsxs(r,{title:"Backward Stable Algorithm",className:"my-6",children:[e.jsxs("p",{children:["An algorithm is ",e.jsx("strong",{children:"backward stable"})," if its computed answer is the exact answer for a slightly perturbed problem:"]}),e.jsx(i,{children:"\\tilde{x} = (A + \\delta A)^{-1}b \\quad \\text{with } \\|\\delta A\\| \\text{ small}"}),e.jsx("p",{className:"mt-2",children:"Gaussian elimination with partial pivoting is backward stable."})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 my-6 border border-amber-500/20",children:[e.jsx("p",{className:"font-semibold text-amber-400 mb-2",children:"Loss of Significance"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Subtracting nearly equal numbers causes ",e.jsx("strong",{children:"catastrophic cancellation"}),":",e.jsx("br",{}),e.jsx(s,{children:"1.23456789 - 1.23456788 = 0.00000001"}),e.jsx("br",{}),"The result has only 1 significant digit even though inputs had 8!"]})]}),e.jsx("h2",{children:"Pivoting Strategies"}),e.jsxs(a,{title:"Partial Pivoting",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Problem without pivoting:"})," If the pivot ",e.jsx(s,{children:"a_{kk}"})," is small, we divide by a tiny number, amplifying errors."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Strategy:"})," At step ",e.jsx(s,{children:"k"}),", find ",e.jsx(s,{children:"\\max_{i \\geq k} |a_{ik}|"})," and swap rows to put this maximum in position ",e.jsx(s,{children:"(k,k)"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Bounded multipliers:"})," The multipliers are ",e.jsx(s,{children:"l_{ik} = a_{ik}/a_{kk}"}),". After pivoting, ",e.jsx(s,{children:"|a_{ik}| \\leq |a_{kk}|"})," for all ",e.jsx(s,{children:"i > k"}),", so ",e.jsx(s,{children:"|l_{ik}| \\leq 1"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Stability:"})," Bounded multipliers prevent exponential error growth. The factorization ",e.jsx(s,{children:"PA = LU"})," is backward stable: the computed ",e.jsx(s,{children:"\\tilde{L}\\tilde{U} = PA + E"})," where ",e.jsx(s,{children:"\\|E\\|"})," is small."]})]}),children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Partial pivoting"}),": at each step, swap rows to put the largest entry in the pivot position."]}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[e.jsx("li",{children:"Prevents division by small numbers"}),e.jsxs("li",{children:["Keeps multipliers ",e.jsx(s,{children:"|l_{ij}| \\leq 1"})]}),e.jsxs("li",{children:["Makes ",e.jsx(s,{children:"PA = LU"})," backward stable"]})]})]}),e.jsx("h2",{children:"Key Ideas"}),e.jsx("div",{className:"bg-gradient-to-br from-primary-500/10 to-dark-800/50 rounded-xl p-6 my-6 border border-primary-500/20",children:e.jsxs("ul",{className:"space-y-3 text-dark-200",children:[e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"1."}),e.jsxs("span",{children:["Machine epsilon ",e.jsx(s,{children:"\\approx 10^{-16}"})," limits precision."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"2."}),e.jsxs("span",{children:["Condition number ",e.jsx(s,{children:"\\kappa(A) = \\sigma_{\\\\max}/\\sigma_{\\\\min}"})," measures sensitivity."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"3."}),e.jsxs("span",{children:["Relative error in solution ≤ ",e.jsx(s,{children:"\\kappa(A)"})," × relative error in data."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"4."}),e.jsx("span",{children:"Backward stability: computed answer is exact for nearby problem."})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"5."}),e.jsx("span",{children:"Partial pivoting is essential for numerical stability."})]})]})})]})}export{j as default};
