const n=[{id:"s01-q1",question:"What is the result of adding vectors v = (2, 3) and w = (1, -1)?",options:["(3, 2)","(1, 4)","(3, 4)","(2, 2)"],correctIndex:0,difficulty:"easy",explanation:"Vector addition is done component-wise: (2+1, 3+(-1)) = (3, 2)."},{id:"s01-q2",question:"If v = (1, 2), what is 3v?",options:["(3, 2)","(3, 6)","(1, 6)","(4, 5)"],correctIndex:1,difficulty:"easy",explanation:"Scalar multiplication multiplies each component: 3·(1, 2) = (3, 6)."},{id:"s01-q3",question:"Which expression represents a linear combination of vectors u and v?",options:["u · v","u × v","cu + dv","u/v"],correctIndex:2,difficulty:"medium",explanation:"A linear combination is of the form cu + dv where c and d are scalars."},{id:"s01-q4",question:"The linear combinations of v = (1, 0) and w = (0, 1) fill what set?",options:["A line through the origin","A single point","A circle","The entire plane ℝ²"],correctIndex:3,difficulty:"medium",explanation:"These are the standard basis vectors; their combinations cv + dw = (c, d) cover all of ℝ²."},{id:"s01-q5",question:"If v = (2, 4), which vector is parallel to v?",options:["(-1, -2)","(4, 2)","(1, 2)","(2, -4)"],correctIndex:0,difficulty:"hard",explanation:"Parallel vectors are scalar multiples. (-1, -2) = -½·(2, 4), so it's parallel to v."}],t=[{id:"s02-q1",question:"What is the dot product of v = (3, 4) and w = (1, 2)?",options:["7","11","14","5"],correctIndex:1,difficulty:"easy",explanation:"v · w = 3·1 + 4·2 = 3 + 8 = 11."},{id:"s02-q2",question:"What is the length (norm) of v = (3, 4)?",options:["7","25","5","12"],correctIndex:2,difficulty:"easy",explanation:"||v|| = √(3² + 4²) = √(9 + 16) = √25 = 5."},{id:"s02-q3",question:"Two vectors are perpendicular (orthogonal) when their dot product equals:",options:["1","-1","Their product of lengths","0"],correctIndex:3,difficulty:"medium",explanation:"Orthogonal vectors have v · w = 0 because cos(90°) = 0."},{id:"s02-q4",question:"Which vector is a unit vector?",options:["(3/5, 4/5)","(1, 1)","(2, 0)","(0.5, 0.5)"],correctIndex:0,difficulty:"medium",explanation:"(3/5)² + (4/5)² = 9/25 + 16/25 = 25/25 = 1, so the length is 1."},{id:"s02-q5",question:"If v · w = ||v|| ||w||, what is the angle between v and w?",options:["90° (perpendicular)","0° (parallel)","180° (opposite)","45°"],correctIndex:1,difficulty:"hard",explanation:"v · w = ||v|| ||w|| cos θ. If v · w = ||v|| ||w||, then cos θ = 1, so θ = 0°."}],o=[{id:"s03-q1",question:"A matrix with 3 rows and 2 columns is called a:",options:["2×3 matrix","6×1 matrix","3×2 matrix","Square matrix"],correctIndex:2,difficulty:"easy",explanation:"Matrix dimensions are given as rows × columns, so 3 rows and 2 columns = 3×2 matrix."},{id:"s03-q2",question:"In the matrix equation Ax = b, what does A represent?",options:["A vector","A scalar","The solution","A coefficient matrix"],correctIndex:3,difficulty:"easy",explanation:"A is the coefficient matrix containing the coefficients of the linear system."},{id:"s03-q3",question:"How many solutions does a consistent system with infinitely many solutions have?",options:["Infinitely many","Zero","Exactly one","Cannot be determined"],correctIndex:0,difficulty:"medium",explanation:"A consistent system either has exactly one solution (unique) or infinitely many solutions."},{id:"s03-q4",question:"What is a linear system where b = 0 called?",options:["Consistent system","Homogeneous system","Overdetermined system","Singular system"],correctIndex:1,difficulty:"medium",explanation:"A homogeneous system has the form Ax = 0, meaning all constant terms are zero."},{id:"s03-q5",question:"To multiply matrix A (2×3) by vector x, x must have how many components?",options:["2","5","3","6"],correctIndex:2,difficulty:"hard",explanation:"For Ax to be defined, the number of columns of A must equal the number of rows (components) of x."}],a=[{id:"ch01-q1",question:"What is the result of 2(1, 3) + 3(2, -1)?",options:["(4, 6)","(8, 6)","(5, 2)","(8, 3)"],correctIndex:3,difficulty:"easy",explanation:"2(1, 3) + 3(2, -1) = (2, 6) + (6, -3) = (8, 3)."},{id:"ch01-q2",question:"The dot product v · v equals:",options:["||v||²","||v||","2||v||","v²"],correctIndex:0,difficulty:"medium",explanation:"v · v = ||v||² (the square of the length)."},{id:"ch01-q3",question:"If vectors u and v are orthogonal, what is u · v?",options:["1","0","||u|| ||v||","-1"],correctIndex:1,difficulty:"easy",explanation:"Orthogonal (perpendicular) vectors have zero dot product."},{id:"ch01-q4",question:"A 4×3 matrix has how many entries?",options:["7","16","12","9"],correctIndex:2,difficulty:"easy",explanation:"A 4×3 matrix has 4 × 3 = 12 entries."},{id:"ch01-q5",question:"What is the angle between (1, 0) and (0, 1)?",options:["0°","45°","180°","90°"],correctIndex:3,difficulty:"easy",explanation:"(1, 0) · (0, 1) = 0, so they are perpendicular (90°)."},{id:"ch01-q6",question:"The Cauchy-Schwarz inequality states that |v · w| is at most:",options:["||v|| ||w||","||v|| + ||w||","||v||² + ||w||²","||v - w||"],correctIndex:0,difficulty:"hard",explanation:"|v · w| ≤ ||v|| ||w||, with equality when v and w are parallel."},{id:"ch01-q7",question:"To find a unit vector in the direction of v, you compute:",options:["v · v","v/||v||","v - ||v||","||v||/v"],correctIndex:1,difficulty:"medium",explanation:"Dividing v by its length gives a unit vector: u = v/||v||."},{id:"ch01-q8",question:"Linear combinations of (1, 1) and (2, 2) fill:",options:["The entire plane","A point","A line through the origin","A circle"],correctIndex:2,difficulty:"hard",explanation:"(2, 2) = 2(1, 1), so they are parallel. Linear combinations fill only a line."},{id:"ch01-q9",question:"If A is a 3×2 matrix and x is a 2×1 vector, what are the dimensions of Ax?",options:["2×1","3×2","2×3","3×1"],correctIndex:3,difficulty:"medium",explanation:"A (3×2) times x (2×1) gives a 3×1 result (rows of A × columns of x)."},{id:"ch01-q10",question:"Ax = b can be viewed as a linear combination of which vectors?",options:["Columns of A","Rows of A","Rows of b","The vector x"],correctIndex:0,difficulty:"hard",explanation:"Ax is a linear combination of the columns of A, with coefficients from x."}],s=[{id:"s04-q1",question:'The "row picture" of Ax = b shows:',options:["Column vectors","Intersecting planes/lines","A single vector","The inverse matrix"],correctIndex:1,difficulty:"easy",explanation:"The row picture shows each equation as a plane (or line in 2D). The solution is their intersection."},{id:"s04-q2",question:'The "column picture" of Ax = b expresses b as:',options:["A row of A","The determinant","A linear combination of columns of A","A pivot"],correctIndex:2,difficulty:"easy",explanation:"Ax = b means b = x₁(col 1) + x₂(col 2) + ... , a linear combination of A's columns."},{id:"s04-q3",question:"A 2×2 system has no solution when the row picture shows:",options:["Intersecting lines","Identical lines","Perpendicular lines","Parallel lines"],correctIndex:3,difficulty:"medium",explanation:"Parallel lines never intersect, so there is no solution."},{id:"s04-q4",question:'In the column picture, "b is in the column space" means:',options:["The system has at least one solution","The system has no solution","A is invertible","b = 0"],correctIndex:0,difficulty:"medium",explanation:"If b is a linear combination of A's columns, then Ax = b has a solution."},{id:"s04-q5",question:"For a 3×3 system, the row picture shows intersection of:",options:["Three lines","Three planes","Three points","Three circles"],correctIndex:1,difficulty:"hard",explanation:"Each equation in 3 unknowns represents a plane. The solution is where all three planes meet."}],r=[{id:"s05-q1",question:"The goal of elimination is to transform A into what type of matrix?",options:["Diagonal","Lower triangular","Upper triangular","Identity"],correctIndex:2,difficulty:"easy",explanation:"Gaussian elimination creates zeros below the diagonal, producing upper triangular form U."},{id:"s05-q2",question:"What is the first pivot in the elimination process?",options:["Any nonzero entry","The largest entry","The entry a₁ₙ","The entry a₁₁"],correctIndex:3,difficulty:"easy",explanation:"The first pivot is a₁₁ (if nonzero), used to eliminate entries below it in column 1."},{id:"s05-q3",question:"When does elimination fail (require a row exchange)?",options:["When the pivot is 0","When the pivot is 1","When the pivot is negative","Never"],correctIndex:0,difficulty:"medium",explanation:"A zero pivot requires a row exchange to find a nonzero pivot below it."},{id:"s05-q4",question:"The multiplier ℓᵢⱼ used to eliminate entry (i,j) equals:",options:["aⱼⱼ/aᵢⱼ","aᵢⱼ/aⱼⱼ","aᵢⱼ × aⱼⱼ","aᵢⱼ - aⱼⱼ"],correctIndex:1,difficulty:"medium",explanation:"The multiplier ℓᵢⱼ = (entry to eliminate)/(pivot) = aᵢⱼ/aⱼⱼ."},{id:"s05-q5",question:"After elimination, if a row becomes 0 = c (where c ≠ 0), the system is:",options:["Consistent","Has infinitely many solutions","Inconsistent","Homogeneous"],correctIndex:2,difficulty:"hard",explanation:"0 = c (c ≠ 0) is a contradiction, indicating no solution exists."}],c=[{id:"s06-q1",question:"An elimination matrix Eᵢⱼ subtracts a multiple of row j from row i. Eᵢⱼ differs from I in position:",options:["(i, i)","(j, j)","(j, i)","(i, j)"],correctIndex:3,difficulty:"easy",explanation:"Eᵢⱼ has -ℓᵢⱼ in position (i, j), which causes row j to be subtracted from row i."},{id:"s06-q2",question:"The product E₃₁E₂₁A eliminates entries in column 1. Which elimination happens first?",options:["E₂₁ (row 2)","E₃₁ (row 3)","Both simultaneously","Neither"],correctIndex:0,difficulty:"medium",explanation:"Matrix multiplication is right to left: E₂₁ acts first, then E₃₁."},{id:"s06-q3",question:"The augmented matrix [A | b] combines:",options:["A and its inverse","A and the right-hand side b","A and U","Two copies of A"],correctIndex:1,difficulty:"easy",explanation:"The augmented matrix appends b as an extra column to track the right-hand side."},{id:"s06-q4",question:"Back substitution solves equations starting from:",options:["The first equation","Any equation","The last equation","The middle equation"],correctIndex:2,difficulty:"medium",explanation:"After elimination to upper triangular form, solve from bottom to top."},{id:"s06-q5",question:"The product of all elimination matrices E...E₂E₁ has an inverse that equals:",options:["U","A","I","L (lower triangular)"],correctIndex:3,difficulty:"hard",explanation:"The inverse of the elimination product is L, giving A = LU."}],l=[{id:"s07-q1",question:"For matrices A and B, when is A + B defined?",options:["When they have the same dimensions","Always","When A is square","When B is square"],correctIndex:0,difficulty:"easy",explanation:"Matrix addition requires matching dimensions (same number of rows and columns)."},{id:"s07-q2",question:"For AB to be defined, the number of columns of A must equal:",options:["The number of columns of B","The number of rows of B","The number of rows of A","1"],correctIndex:1,difficulty:"easy",explanation:"Matrix multiplication A(m×n) · B(n×p) requires columns of A = rows of B."},{id:"s07-q3",question:"If A is 3×4 and B is 4×2, what are the dimensions of AB?",options:["4×4","3×4","3×2","2×3"],correctIndex:2,difficulty:"medium",explanation:"A(3×4) · B(4×2) = C(3×2). Result has rows of A and columns of B."},{id:"s07-q4",question:"Is matrix multiplication commutative (AB = BA)?",options:["Always","Never","Only for square matrices","Generally not"],correctIndex:3,difficulty:"medium",explanation:"AB ≠ BA in general. Even when both products exist, they usually differ."},{id:"s07-q5",question:"(AB)C equals:",options:["A(BC)","C(BA)","ACB","CAB"],correctIndex:0,difficulty:"medium",explanation:"Matrix multiplication is associative: (AB)C = A(BC)."}],d=[{id:"s08-q1",question:"If A⁻¹ exists, then A⁻¹A equals:",options:["A","I (identity)","2A","0"],correctIndex:1,difficulty:"easy",explanation:"By definition, A⁻¹A = AA⁻¹ = I (the identity matrix)."},{id:"s08-q2",question:"A matrix with an inverse is called:",options:["Singular","Orthogonal","Invertible (nonsingular)","Symmetric"],correctIndex:2,difficulty:"easy",explanation:"An invertible (or nonsingular) matrix has an inverse."},{id:"s08-q3",question:"Which condition guarantees that A is NOT invertible?",options:["All pivots are nonzero","A is square","A = Aᵀ","det(A) = 0"],correctIndex:3,difficulty:"medium",explanation:"A singular matrix has det(A) = 0 and has no inverse."},{id:"s08-q4",question:"The inverse of AB (when it exists) is:",options:["B⁻¹A⁻¹","A⁻¹B⁻¹","BA⁻¹","A⁻¹ + B⁻¹"],correctIndex:0,difficulty:"hard",explanation:"(AB)⁻¹ = B⁻¹A⁻¹. The order reverses!"},{id:"s08-q5",question:"For a 2×2 matrix A = [[a,b],[c,d]], the inverse involves dividing by:",options:["a + d","ad - bc","ad + bc","a - d"],correctIndex:1,difficulty:"hard",explanation:"A⁻¹ = (1/(ad-bc))[[d,-b],[-c,a]]. The denominator is det(A) = ad - bc."}],u=[{id:"s09-q1",question:"In the LU factorization A = LU, L is:",options:["Upper triangular","Diagonal","Lower triangular","Orthogonal"],correctIndex:2,difficulty:"easy",explanation:"L is lower triangular (with 1s on diagonal), U is upper triangular."},{id:"s09-q2",question:"The entries below the diagonal of L are:",options:["The pivots","Always 1","Always 0","The multipliers from elimination"],correctIndex:3,difficulty:"medium",explanation:"L stores the multipliers ℓᵢⱼ used during elimination."},{id:"s09-q3",question:"The main advantage of LU factorization is:",options:["Solving Ax = b for multiple b vectors efficiently","It never fails","It works for any matrix","It is faster than elimination"],correctIndex:0,difficulty:"medium",explanation:"Once A = LU is computed, solving for new b only requires forward/back substitution."},{id:"s09-q4",question:"The number of operations for LU factorization of an n×n matrix is approximately:",options:["n²","n³/3","n³","2ⁿ"],correctIndex:1,difficulty:"hard",explanation:"LU factorization requires about n³/3 multiplications and additions."},{id:"s09-q5",question:"To solve Ax = b using LU factorization, we solve:",options:["Lx = b, then Ux = c","Ux = b, then Lx = c","Lc = b, then Ux = c","LUx = b directly"],correctIndex:2,difficulty:"hard",explanation:"First solve Lc = b (forward substitution), then Ux = c (back substitution)."}],m=[{id:"s10-q1",question:"The transpose of a matrix exchanges:",options:["Diagonal entries","First and last rows","Positive and negative entries","Rows and columns"],correctIndex:3,difficulty:"easy",explanation:"(Aᵀ)ᵢⱼ = Aⱼᵢ: rows become columns and columns become rows."},{id:"s10-q2",question:"A permutation matrix P has the property:",options:["P⁻¹ = Pᵀ","P⁻¹ = P","P² = 0","P + Pᵀ = I"],correctIndex:0,difficulty:"medium",explanation:"Permutation matrices are orthogonal: P⁻¹ = Pᵀ."},{id:"s10-q3",question:"The equation (AB)ᵀ equals:",options:["AᵀBᵀ","BᵀAᵀ","AᵀBᵀ only if A = B","BA"],correctIndex:1,difficulty:"medium",explanation:"The transpose of a product reverses order: (AB)ᵀ = BᵀAᵀ."},{id:"s10-q4",question:"A symmetric matrix satisfies:",options:["A = -Aᵀ","A = A⁻¹","A = Aᵀ","Aᵀ = 0"],correctIndex:2,difficulty:"easy",explanation:"A symmetric matrix equals its transpose: A = Aᵀ."},{id:"s10-q5",question:"When row exchanges are needed, LU factorization becomes:",options:["Impossible","A = PLU","A = UP","PA = LU"],correctIndex:3,difficulty:"hard",explanation:"With row exchanges, we get PA = LU where P is a permutation matrix."}],p=[{id:"ch02-q1",question:"Gaussian elimination transforms A into:",options:["U (upper triangular)","L (lower triangular)","D (diagonal)","I (identity)"],correctIndex:0,difficulty:"easy",explanation:"Elimination creates zeros below the diagonal, giving upper triangular U."},{id:"ch02-q2",question:"If elimination produces a zero pivot with no nonzero entry below it, then:",options:["The system has a unique solution","The matrix is singular","The system is inconsistent","We multiply by -1"],correctIndex:1,difficulty:"medium",explanation:"A zero pivot that cannot be fixed means the matrix is singular (not invertible)."},{id:"ch02-q3",question:"The product of the pivots equals:",options:["0","rank(A)","det(A)","n"],correctIndex:2,difficulty:"medium",explanation:"For triangular matrices, det = product of diagonal entries. det(A) = det(U) = product of pivots."},{id:"ch02-q4",question:"If A and B are both invertible n×n matrices, then AB is:",options:["Always singular","Invertible only if A = B","Sometimes invertible","Always invertible"],correctIndex:3,difficulty:"medium",explanation:"The product of invertible matrices is invertible: (AB)⁻¹ = B⁻¹A⁻¹."},{id:"ch02-q5",question:"The identity matrix I has the property that IA equals:",options:["A","I","AI","2A"],correctIndex:0,difficulty:"easy",explanation:"The identity matrix satisfies IA = AI = A for any matrix A."},{id:"ch02-q6",question:"A matrix with linearly dependent columns:",options:["Is always invertible","Has det = 0","Has det ≠ 0","Must be square"],correctIndex:1,difficulty:"hard",explanation:"Dependent columns mean the matrix is singular, so det = 0."},{id:"ch02-q7",question:"The row picture of a 3×3 system shows:",options:["Three lines","A single plane","Three planes","A line"],correctIndex:2,difficulty:"medium",explanation:"Each equation in 3 unknowns defines a plane. The solution is where they intersect."},{id:"ch02-q8",question:"For a symmetric matrix, A = Aᵀ means:",options:["A is invertible","All entries are positive","A² = I","aᵢⱼ = aⱼᵢ for all i, j"],correctIndex:3,difficulty:"medium",explanation:"Symmetry means the (i,j) entry equals the (j,i) entry."},{id:"ch02-q9",question:"If A⁻¹ = A, then A is called:",options:["Involutory","Symmetric","Orthogonal","Nilpotent"],correctIndex:0,difficulty:"hard",explanation:"A matrix satisfying A⁻¹ = A (or A² = I) is called involutory."},{id:"ch02-q10",question:"LU factorization is useful because:",options:["It gives the eigenvalues","It solves Ax = b for many different b efficiently","It works for non-square matrices","L and U are always integers"],correctIndex:1,difficulty:"hard",explanation:"Once A = LU is computed, each new b requires only O(n²) work instead of O(n³)."}],f=[{id:"s11-q1",question:"Which is a vector space?",options:["All vectors with positive components","Vectors with integer components only","ℝⁿ","The empty set"],correctIndex:2,difficulty:"easy",explanation:"ℝⁿ satisfies all vector space axioms. The others fail closure under scalar multiplication or addition."},{id:"s11-q2",question:"A subspace must contain:",options:["At least two vectors","Only unit vectors","Exactly n vectors","The zero vector"],correctIndex:3,difficulty:"easy",explanation:"Every subspace contains 0 (it's closed under scalar multiplication by 0)."},{id:"s11-q3",question:"The column space of A contains all vectors that can be written as:",options:["Ax for some x","xᵀA for some x","Solutions to Ax = 0","Rows of A"],correctIndex:0,difficulty:"medium",explanation:"C(A) = {Ax : x ∈ ℝⁿ} = all linear combinations of columns of A."},{id:"s11-q4",question:"Which is NOT a subspace of ℝ³?",options:["A plane through the origin","A plane not through the origin","A line through the origin","The zero vector alone"],correctIndex:1,difficulty:"medium",explanation:"A plane not through origin doesn't contain 0, so it's not a subspace."},{id:"s11-q5",question:"The row space of A is the column space of:",options:["A⁻¹","A²","Aᵀ","-A"],correctIndex:2,difficulty:"hard",explanation:"Row space = C(Aᵀ) since the rows of A become columns of Aᵀ."}],h=[{id:"s12-q1",question:"The null space of A consists of all solutions to:",options:["Ax = b","xA = 0","A = 0","Ax = 0"],correctIndex:3,difficulty:"easy",explanation:"N(A) = {x : Ax = 0}, the set of all solutions to the homogeneous equation."},{id:"s12-q2",question:"The null space is always a:",options:["Subspace","Full space ℝⁿ","Single vector","Empty set"],correctIndex:0,difficulty:"easy",explanation:"N(A) is a subspace of ℝⁿ (contains 0, closed under addition and scalar multiplication)."},{id:"s12-q3",question:"If A is invertible, its null space contains:",options:["Infinitely many vectors","Only the zero vector","All of ℝⁿ","No vectors"],correctIndex:1,difficulty:"medium",explanation:"Invertible A means Ax = 0 has only x = 0 as solution."},{id:"s12-q4",question:"Free variables in Ax = 0 correspond to:",options:["Pivot columns","Rows of A","Non-pivot columns","Zero rows"],correctIndex:2,difficulty:"hard",explanation:"Free variables come from columns without pivots; they can take any value."},{id:"s12-q5",question:"The dimension of the null space is called the:",options:["Rank","Trace","Determinant","Nullity"],correctIndex:3,difficulty:"medium",explanation:"Nullity = dim(N(A)) = number of free variables = n - rank."}],x=[{id:"s13-q1",question:"The complete solution to Ax = b is written as:",options:["x = xₚ + xₙ","xₚ only","xₙ only","x = xₚ × xₙ"],correctIndex:0,difficulty:"easy",explanation:"Complete solution = particular solution + any null space vector: x = xₚ + xₙ."},{id:"s13-q2",question:"A particular solution xₚ satisfies:",options:["Axₚ = 0","Axₚ = b","xₚ = 0","xₚ is in N(A)"],correctIndex:1,difficulty:"easy",explanation:"A particular solution is any specific solution to Ax = b."},{id:"s13-q3",question:"If adding any null space vector to xₚ still solves Ax = b, this is because:",options:["A(xₙ) = b","xₚ = 0","A(xₙ) = 0","N(A) is empty"],correctIndex:2,difficulty:"medium",explanation:"A(xₚ + xₙ) = Axₚ + Axₙ = b + 0 = b."},{id:"s13-q4",question:"For Ax = b to have a solution, b must be in:",options:["N(A)","Row space","N(Aᵀ)","C(A) (column space)"],correctIndex:3,difficulty:"medium",explanation:"Ax = b solvable ⟺ b is a linear combination of columns ⟺ b ∈ C(A)."},{id:"s13-q5",question:"If rank(A) = rank([A|b]), then Ax = b:",options:["Is consistent (has at least one solution)","Has no solution","Has a unique solution","Has exactly rank solutions"],correctIndex:0,difficulty:"hard",explanation:"Equal ranks means b is in the column space of A, so a solution exists."}],y=[{id:"s14-q1",question:"Vectors v₁, v₂, ..., vₖ are linearly independent if:",options:["They span ℝⁿ","c₁v₁ + ... + cₖvₖ = 0 has only the trivial solution","They are orthogonal","k = n"],correctIndex:1,difficulty:"easy",explanation:"Independence means the only linear combination giving 0 has all coefficients = 0."},{id:"s14-q2",question:"A basis for a subspace V is a set that:",options:["Spans V only","Is independent only","Spans V and is independent","Contains all vectors of V"],correctIndex:2,difficulty:"easy",explanation:"A basis must both span the space and be linearly independent."},{id:"s14-q3",question:"The dimension of a subspace equals:",options:["The number of vectors in any spanning set","The number of equations","n","The number of vectors in a basis"],correctIndex:3,difficulty:"medium",explanation:"Dimension = number of vectors in any basis (all bases have the same size)."},{id:"s14-q4",question:"If a set has more vectors than the dimension of the space, the vectors are:",options:["Dependent","Independent","Orthogonal","A basis"],correctIndex:0,difficulty:"medium",explanation:"More vectors than dimension ⟹ at least one vector is a combination of others."},{id:"s14-q5",question:"The pivot columns of A form a basis for:",options:["N(A)","C(A)","Row space","ℝⁿ"],correctIndex:1,difficulty:"hard",explanation:"Pivot columns are independent and span the column space."}],q=[{id:"s15-q1",question:"The four fundamental subspaces are C(A), N(A), C(Aᵀ), and:",options:["C(A⁻¹)","R(A)","N(Aᵀ)","S(A)"],correctIndex:2,difficulty:"easy",explanation:"The four subspaces: column space, null space, row space, and left null space N(Aᵀ)."},{id:"s15-q2",question:"The rank of a matrix equals:",options:["Number of rows","Number of columns","Number of entries","Number of pivots"],correctIndex:3,difficulty:"easy",explanation:"Rank = number of pivots = dim(C(A)) = dim(C(Aᵀ))."},{id:"s15-q3",question:"If rank(A) = r for an m×n matrix, the null space has dimension:",options:["n - r","r","m - r","m"],correctIndex:0,difficulty:"medium",explanation:"The rank-nullity theorem: dim(N(A)) = n - r."},{id:"s15-q4",question:"The left null space N(Aᵀ) has dimension:",options:["r","m - r","n - r","n"],correctIndex:1,difficulty:"hard",explanation:"N(Aᵀ) is in ℝᵐ and dim(N(Aᵀ)) = m - r."},{id:"s15-q5",question:"For a 5×7 matrix of rank 3, dim(N(A)) equals:",options:["2","3","4","5"],correctIndex:2,difficulty:"hard",explanation:"dim(N(A)) = n - r = 7 - 3 = 4."}],A=[{id:"ch03-q1",question:"Which is required for a set to be a subspace?",options:["Contains only positive vectors","Contains exactly n vectors","Contains the vector (1,1,...,1)","Closed under addition and scalar multiplication"],correctIndex:3,difficulty:"easy",explanation:"A subspace must be closed under addition and scalar multiplication (and contain 0)."},{id:"ch03-q2",question:"The intersection of two subspaces is:",options:["Always a subspace","Never a subspace","A subspace only if they're equal","Empty"],correctIndex:0,difficulty:"medium",explanation:"The intersection of any subspaces is also a subspace."},{id:"ch03-q3",question:"For A (m×n), if rank(A) = n, then N(A) = :",options:["ℝⁿ","{0}","ℝᵐ","C(A)"],correctIndex:1,difficulty:"medium",explanation:"Full column rank means no free variables, so only x = 0 solves Ax = 0."},{id:"ch03-q4",question:"The rank-nullity theorem states:",options:["rank + nullity = m","rank × nullity = n","rank + nullity = n","rank = nullity"],correctIndex:2,difficulty:"hard",explanation:"For an m×n matrix: rank(A) + dim(N(A)) = n."},{id:"ch03-q5",question:"The four fundamental subspaces are related to matrix A and:",options:["A⁻¹","A²","A + I","Aᵀ"],correctIndex:3,difficulty:"medium",explanation:"The four subspaces: C(A), N(A), C(Aᵀ), N(Aᵀ)."},{id:"ch03-q6",question:"If Ax = b is inconsistent, then b is:",options:["Not in the column space of A","In the null space of A","In the column space of A","Zero"],correctIndex:0,difficulty:"medium",explanation:"Ax = b solvable ⟺ b ∈ C(A). Inconsistent means b ∉ C(A)."},{id:"ch03-q7",question:"Special solutions to Ax = 0 form a:",options:["Basis for C(A)","Basis for N(A)","Basis for ℝⁿ","Single vector"],correctIndex:1,difficulty:"hard",explanation:"Setting each free variable to 1 (others to 0) gives basis vectors for N(A)."},{id:"ch03-q8",question:"The row space of A has the same dimension as:",options:["N(A)","N(Aᵀ)","C(A)","ℝᵐ"],correctIndex:2,difficulty:"hard",explanation:"dim(row space) = dim(column space) = rank(A)."},{id:"ch03-q9",question:"A matrix is full rank if:",options:["rank = 0","rank = max(m,n)","rank = m + n","rank = min(m,n)"],correctIndex:3,difficulty:"easy",explanation:"Full rank means rank equals the smaller dimension: min(m, n)."},{id:"ch03-q10",question:"The column space and left null space are:",options:["Orthogonal complements in ℝᵐ","Equal","The same dimension","Both subspaces of ℝⁿ"],correctIndex:0,difficulty:"hard",explanation:"C(A) and N(Aᵀ) are orthogonal complements: C(A) ⊕ N(Aᵀ) = ℝᵐ."}],v=[{id:"s16-q1",question:"Two vectors are orthogonal when their dot product is:",options:["1","0","-1","Their product of lengths"],correctIndex:1,difficulty:"easy",explanation:"u · v = 0 defines orthogonality (perpendicular vectors)."},{id:"s16-q2",question:"The row space and null space of A are orthogonal in:",options:["ℝᵐ","ℝʳ","ℝⁿ","They are not orthogonal"],correctIndex:2,difficulty:"medium",explanation:"Row space and null space are orthogonal complements in ℝⁿ."},{id:"s16-q3",question:"Orthogonal complement of a subspace V is written as:",options:["V⁻¹","Vᵀ","V*","V⊥"],correctIndex:3,difficulty:"easy",explanation:'V⊥ ("V perp") contains all vectors orthogonal to every vector in V.'},{id:"s16-q4",question:"If V has dimension k in ℝⁿ, then V⊥ has dimension:",options:["n - k","k","n + k","n/k"],correctIndex:0,difficulty:"hard",explanation:"V and V⊥ are complements: dim(V) + dim(V⊥) = n."},{id:"s16-q5",question:"The column space C(A) is orthogonal to:",options:["N(A)","N(Aᵀ)","C(Aᵀ)","Row space"],correctIndex:1,difficulty:"hard",explanation:"C(A) ⊥ N(Aᵀ) in ℝᵐ. They are orthogonal complements."}],g=[{id:"s17-q1",question:"The projection of b onto a line through a is:",options:["a·b","a/b","(a·b/a·a)a","b - a"],correctIndex:2,difficulty:"medium",explanation:"proj_a(b) = (a·b/a·a)a = (aᵀb/aᵀa)a."},{id:"s17-q2",question:"The error vector e = b - p is orthogonal to:",options:["b","The null space","Nothing","The subspace being projected onto"],correctIndex:3,difficulty:"medium",explanation:"The error is perpendicular to the subspace: that's what makes p the closest point."},{id:"s17-q3",question:"The projection matrix P satisfies P² = :",options:["P","2P","I","0"],correctIndex:0,difficulty:"hard",explanation:"Projecting twice is the same as projecting once: P² = P (idempotent)."},{id:"s17-q4",question:"If P projects onto a subspace, then I - P projects onto:",options:["The same subspace","The orthogonal complement","The null space","The whole space"],correctIndex:1,difficulty:"hard",explanation:"P + (I-P) = I, and they project onto complementary subspaces."},{id:"s17-q5",question:"The projection matrix onto the column space of A is:",options:["AᵀA","AAᵀ","A(AᵀA)⁻¹Aᵀ","Aᵀ(AAᵀ)⁻¹A"],correctIndex:2,difficulty:"hard",explanation:"P = A(AᵀA)⁻¹Aᵀ projects onto C(A)."}],I=[{id:"s18-q1",question:"Least squares minimizes:",options:["||Ax - b||","|Ax - b|","Ax - b","||Ax - b||²"],correctIndex:3,difficulty:"easy",explanation:"We minimize the squared error ||Ax - b||² (sum of squared residuals)."},{id:"s18-q2",question:"The normal equations are:",options:["AᵀAx̂ = Aᵀb","Ax = b","AAᵀx̂ = b","x̂ = A⁻¹b"],correctIndex:0,difficulty:"medium",explanation:"Multiply Ax = b by Aᵀ to get AᵀAx̂ = Aᵀb."},{id:"s18-q3",question:"AᵀA is always:",options:["Invertible","Symmetric","Orthogonal","Diagonal"],correctIndex:1,difficulty:"medium",explanation:"(AᵀA)ᵀ = AᵀAᵀᵀ = AᵀA, so it's symmetric."},{id:"s18-q4",question:"In fitting y = C + Dt, the normal equations find:",options:["Any line","The perpendicular line","The line minimizing vertical distances squared","A horizontal line"],correctIndex:2,difficulty:"hard",explanation:"Least squares finds the line minimizing sum of squared vertical errors."},{id:"s18-q5",question:"The residual e = b - Ax̂ in least squares is in:",options:["C(A)","N(A)","Row space of A","N(Aᵀ)"],correctIndex:3,difficulty:"hard",explanation:"The error is orthogonal to C(A), so it's in C(A)⊥ = N(Aᵀ)."}],b=[{id:"s19-q1",question:"Orthonormal vectors satisfy qᵢ · qⱼ = :",options:["1 if i=j, 0 otherwise","1 for all i,j","0 for all i,j","i + j"],correctIndex:0,difficulty:"easy",explanation:"Orthonormal: qᵢ · qⱼ = δᵢⱼ (Kronecker delta)."},{id:"s19-q2",question:"For a matrix Q with orthonormal columns, QᵀQ = :",options:["Q","I","QQᵀ","0"],correctIndex:1,difficulty:"medium",explanation:"Orthonormal columns give QᵀQ = I (identity)."},{id:"s19-q3",question:"The Gram-Schmidt process produces:",options:["Eigenvalues","The inverse matrix","An orthonormal basis","Determinants"],correctIndex:2,difficulty:"easy",explanation:"Gram-Schmidt orthonormalizes a set of vectors."},{id:"s19-q4",question:"A square matrix with orthonormal columns is called:",options:["Symmetric","Diagonal","Positive definite","Orthogonal"],correctIndex:3,difficulty:"medium",explanation:"Square Q with QᵀQ = I is orthogonal (Q⁻¹ = Qᵀ)."},{id:"s19-q5",question:"In QR factorization, R is:",options:["Upper triangular","Lower triangular","Diagonal","Orthogonal"],correctIndex:0,difficulty:"medium",explanation:"A = QR where Q has orthonormal columns and R is upper triangular."}],w=[{id:"ch04-q1",question:"Orthogonal vectors have dot product:",options:["1","0","-1","||u|| ||v||"],correctIndex:1,difficulty:"easy",explanation:"u ⊥ v means u · v = 0."},{id:"ch04-q2",question:"The projection matrix onto column space of A is:",options:["AᵀA","AAᵀ","A(AᵀA)⁻¹Aᵀ","A⁻¹"],correctIndex:2,difficulty:"hard",explanation:"P = A(AᵀA)⁻¹Aᵀ projects onto C(A)."},{id:"ch04-q3",question:"Least squares solution x̂ minimizes:",options:["||x||","||Ax||","||A||","||b - Ax||"],correctIndex:3,difficulty:"medium",explanation:"Least squares minimizes the residual ||b - Ax||."},{id:"ch04-q4",question:"An orthogonal matrix Q satisfies:",options:["Qᵀ = Q⁻¹","Qᵀ = Q","Qᵀ = -Q","Q² = I"],correctIndex:0,difficulty:"medium",explanation:"QᵀQ = I, so Qᵀ = Q⁻¹."},{id:"ch04-q5",question:"QR factorization expresses A as:",options:["Product of eigenvalues","Orthogonal × upper triangular","Lower × upper triangular","Symmetric × antisymmetric"],correctIndex:1,difficulty:"medium",explanation:"A = QR where Q is orthogonal (or has orthonormal columns) and R is upper triangular."},{id:"ch04-q6",question:"The error b - Ax̂ in least squares is in:",options:["C(A)","N(A)","N(Aᵀ)","Row space of A"],correctIndex:2,difficulty:"hard",explanation:"The error is orthogonal to C(A), so it's in C(A)⊥ = N(Aᵀ)."},{id:"ch04-q7",question:"Orthonormal basis makes computing projections:",options:["Impossible","Require matrix inversion","Give zero","Easy (just dot products)"],correctIndex:3,difficulty:"medium",explanation:"With orthonormal q's: projection = Σ(qᵢᵀb)qᵢ, just dot products."},{id:"ch04-q8",question:"For projection P, the complement I - P projects onto:",options:["The orthogonal complement","The same space","The zero vector","The full space"],correctIndex:0,difficulty:"hard",explanation:"P and I - P are complementary projections onto orthogonal subspaces."},{id:"ch04-q9",question:"Orthogonal matrices preserve:",options:["Only angles","Both lengths and angles","Only lengths","Neither"],correctIndex:1,difficulty:"medium",explanation:"||Qx|| = ||x|| and angles are preserved (Q preserves dot products)."},{id:"ch04-q10",question:"Gram-Schmidt produces the R in A = QR by:",options:["Eigenvalue decomposition","Random generation","Recording the coefficients of projections","Matrix inversion"],correctIndex:2,difficulty:"hard",explanation:"The entries of R are the projection coefficients qᵢᵀaⱼ."}],T=[{id:"s20-q1",question:"det(I) equals:",options:["0","n","Undefined","1"],correctIndex:3,difficulty:"easy",explanation:"The identity matrix has determinant 1."},{id:"s20-q2",question:"If two rows of A are equal, det(A) = :",options:["0","1","2","Undefined"],correctIndex:0,difficulty:"easy",explanation:"Equal rows (or columns) make the matrix singular, so det = 0."},{id:"s20-q3",question:"Exchanging two rows of A:",options:["Doubles det(A)","Changes sign of det(A)","Halves det(A)","Doesn't change det(A)"],correctIndex:1,difficulty:"medium",explanation:"Row exchanges multiply the determinant by -1."},{id:"s20-q4",question:"det(AB) equals:",options:["det(A) + det(B)","det(A)/det(B)","det(A) det(B)","det(A + B)"],correctIndex:2,difficulty:"medium",explanation:"The determinant is multiplicative: det(AB) = det(A)det(B)."},{id:"s20-q5",question:"det(cA) for an n×n matrix equals:",options:["c det(A)","c² det(A)","det(A)/c","cⁿ det(A)"],correctIndex:3,difficulty:"hard",explanation:"Scaling all n rows by c multiplies det by cⁿ."}],S=[{id:"s21-q1",question:"The determinant of a triangular matrix equals:",options:["Product of diagonal entries","Sum of diagonal entries","Trace","0"],correctIndex:0,difficulty:"easy",explanation:"det(triangular) = product of diagonal entries (pivots)."},{id:"s21-q2",question:"For a 2×2 matrix [[a,b],[c,d]], det = :",options:["ac - bd","ad - bc","ab - cd","a + d"],correctIndex:1,difficulty:"easy",explanation:"det([[a,b],[c,d]]) = ad - bc."},{id:"s21-q3",question:"The cofactor expansion computes det using:",options:["Only the first row","Only diagonal entries","Any row or column","Only the last row"],correctIndex:2,difficulty:"medium",explanation:"Cofactor expansion works along any row or column."},{id:"s21-q4",question:"Cofactor Cᵢⱼ equals:",options:["det(minor)","aᵢⱼ det(minor)","aᵢⱼ","(-1)^(i+j) det(minor)"],correctIndex:3,difficulty:"hard",explanation:"Cᵢⱼ = (-1)^(i+j) Mᵢⱼ where Mᵢⱼ is the minor (det of submatrix)."}],k=[{id:"s22-q1",question:"A matrix is invertible if and only if det(A):",options:["≠ 0","= 0","= 1","> 0"],correctIndex:0,difficulty:"easy",explanation:"det(A) ≠ 0 ⟺ A is invertible."},{id:"s22-q2",question:"Cramer's rule gives xⱼ = :",options:["det(A)/det(Bⱼ)","det(Bⱼ)/det(A)","det(A) + det(Bⱼ)","det(A·Bⱼ)"],correctIndex:1,difficulty:"medium",explanation:"xⱼ = det(Bⱼ)/det(A) where Bⱼ has column j replaced by b."},{id:"s22-q3",question:"The absolute value of a 2×2 determinant equals:",options:["The perimeter of a parallelogram","The diagonal length","The area of a parallelogram","The sum of sides"],correctIndex:2,difficulty:"medium",explanation:"|det| = area of parallelogram formed by column (or row) vectors."},{id:"s22-q4",question:"A⁻¹ can be computed as:",options:["C/det(A)","det(A)·C","C + det(A)I","Cᵀ/det(A)"],correctIndex:3,difficulty:"hard",explanation:"A⁻¹ = (1/det(A)) Cᵀ where C is the cofactor matrix."}],z=[{id:"ch05-q1",question:"det(A) = 0 means:",options:["A is singular","A is invertible","A is orthogonal","A is symmetric"],correctIndex:0,difficulty:"easy",explanation:"Zero determinant means singular (not invertible)."},{id:"ch05-q2",question:"det(Aᵀ) equals:",options:["1/det(A)","det(A)","-det(A)","(det(A))²"],correctIndex:1,difficulty:"medium",explanation:"Transpose doesn't change the determinant: det(Aᵀ) = det(A)."},{id:"ch05-q3",question:"det(A⁻¹) equals:",options:["det(A)","-det(A)","1/det(A)","(det(A))⁻¹"],correctIndex:2,difficulty:"medium",explanation:"det(A)det(A⁻¹) = det(I) = 1, so det(A⁻¹) = 1/det(A)."},{id:"ch05-q4",question:"Adding a multiple of one row to another:",options:["Changes det","Doubles det","Zeros det","Doesn't change det"],correctIndex:3,difficulty:"medium",explanation:"Row replacement operations preserve the determinant."},{id:"ch05-q5",question:"For orthogonal matrix Q, det(Q) = :",options:["±1","0","n","Any value"],correctIndex:0,difficulty:"hard",explanation:"det(Q)² = det(QᵀQ) = det(I) = 1, so det(Q) = ±1."},{id:"ch05-q6",question:"The volume of a parallelepiped formed by 3 vectors in ℝ³ is:",options:["Sum of determinants","Absolute value of 3×3 determinant","Product of lengths","Trace of matrix"],correctIndex:1,difficulty:"hard",explanation:"|det| gives the volume (area in 2D, volume in 3D)."},{id:"ch05-q7",question:"det(2A) for a 3×3 matrix equals:",options:["2 det(A)","4 det(A)","8 det(A)","6 det(A)"],correctIndex:2,difficulty:"hard",explanation:"det(cA) = c³det(A) for 3×3, so det(2A) = 8det(A)."},{id:"ch05-q8",question:"Cramer's rule is practical for:",options:["Large systems","Singular systems","Overdetermined systems","Small systems or finding one variable"],correctIndex:3,difficulty:"medium",explanation:"Cramer's rule is O(n·n!) - impractical for large n but useful for small systems."},{id:"ch05-q9",question:"If A is 4×4 and det(A) = 3, then det(2A) = :",options:["48","6","12","24"],correctIndex:0,difficulty:"hard",explanation:"det(2A) = 2⁴ det(A) = 16 × 3 = 48."},{id:"ch05-q10",question:"The adjugate matrix is:",options:["The cofactor matrix","The transpose of the cofactor matrix","The inverse matrix","The transpose of A"],correctIndex:1,difficulty:"hard",explanation:"adj(A) = Cᵀ, and A⁻¹ = adj(A)/det(A)."}],Q=[{id:"s23-q1",question:"An eigenvector x satisfies Ax = :",options:["0","x","λx","Aλ"],correctIndex:2,difficulty:"easy",explanation:"Ax = λx defines eigenvector x with eigenvalue λ."},{id:"s23-q2",question:"Eigenvalues are found by solving:",options:["det(A) = 0","A = λI","Ax = 0","det(A - λI) = 0"],correctIndex:3,difficulty:"medium",explanation:"The characteristic equation det(A - λI) = 0 gives eigenvalues."},{id:"s23-q3",question:"The sum of eigenvalues equals:",options:["trace(A)","det(A)","rank(A)","0"],correctIndex:0,difficulty:"medium",explanation:"Sum of eigenvalues = trace = sum of diagonal entries."},{id:"s23-q4",question:"The product of eigenvalues equals:",options:["trace(A)","det(A)","rank(A)","n"],correctIndex:1,difficulty:"medium",explanation:"Product of eigenvalues = determinant."}],C=[{id:"s24-q1",question:"A is diagonalizable if A = :",options:["S + D","D/S","SDS⁻¹","SᵀDS"],correctIndex:2,difficulty:"easy",explanation:"A = SDS⁻¹ where D is diagonal (eigenvalues) and S has eigenvectors."},{id:"s24-q2",question:"The columns of S in A = SDS⁻¹ are:",options:["Eigenvalues","Rows of A","Random vectors","Eigenvectors"],correctIndex:3,difficulty:"easy",explanation:"S has eigenvectors as columns, D has eigenvalues on diagonal."},{id:"s24-q3",question:"A is diagonalizable if it has:",options:["n linearly independent eigenvectors","n eigenvalues (counting multiplicity)","All positive eigenvalues","Integer eigenvalues"],correctIndex:0,difficulty:"medium",explanation:"Need n independent eigenvectors to form invertible S."},{id:"s24-q4",question:"Aᵏ can be computed as:",options:["Sᵏ D Sᵏ","S Dᵏ S⁻¹","kA","A + kI"],correctIndex:1,difficulty:"hard",explanation:"Aᵏ = (SDS⁻¹)ᵏ = SDᵏS⁻¹, and Dᵏ is easy (diagonal)."}],P=[{id:"s25-q1",question:"The solution to du/dt = Au involves:",options:["sin(t)","t²","eᴬᵗ","log(t)"],correctIndex:2,difficulty:"medium",explanation:"u(t) = eᴬᵗu(0), the matrix exponential."},{id:"s25-q2",question:"Stability (u → 0 as t → ∞) requires all eigenvalues to have:",options:["Positive real parts","Zero imaginary parts","Magnitude > 1","Negative real parts"],correctIndex:3,difficulty:"medium",explanation:"Re(λ) < 0 for all λ ensures exponential decay to zero."},{id:"s25-q3",question:"Pure imaginary eigenvalues (λ = ±bi) give:",options:["Oscillations","Exponential growth","Exponential decay","Constant solutions"],correctIndex:0,difficulty:"hard",explanation:"e^(ibt) = cos(bt) + i sin(bt), pure oscillation."},{id:"s25-q4",question:"eᴬᵗ can be computed using:",options:["Just e^λt","S eᴰᵗ S⁻¹","A + t","tA"],correctIndex:1,difficulty:"hard",explanation:"eᴬᵗ = S eᴰᵗ S⁻¹ where eᴰᵗ has e^(λᵢt) on diagonal."}],D=[{id:"s26-q1",question:"A symmetric matrix has:",options:["Complex eigenvalues","Zero eigenvalues","Real eigenvalues","No eigenvalues"],correctIndex:2,difficulty:"easy",explanation:"Symmetric matrices always have real eigenvalues."},{id:"s26-q2",question:"Eigenvectors of a symmetric matrix are:",options:["Parallel","Equal","Complex","Orthogonal"],correctIndex:3,difficulty:"medium",explanation:"Eigenvectors for distinct eigenvalues are orthogonal."},{id:"s26-q3",question:"A symmetric matrix can be written as:",options:["A = QDQᵀ","A = SDS⁻¹","A = LU","A = QR"],correctIndex:0,difficulty:"medium",explanation:"Spectral theorem: A = QDQᵀ with orthogonal Q."},{id:"s26-q4",question:"For symmetric A, the eigenvector matrix S is:",options:["Lower triangular","Orthogonal","Upper triangular","Diagonal"],correctIndex:1,difficulty:"hard",explanation:"S = Q is orthogonal (Qᵀ = Q⁻¹) for symmetric matrices."}],B=[{id:"s27-q1",question:"A positive definite matrix has:",options:["All eigenvalues < 0","Mixed eigenvalues","All eigenvalues > 0","Zero eigenvalues"],correctIndex:2,difficulty:"easy",explanation:"Positive definite ⟺ all eigenvalues strictly positive."},{id:"s27-q2",question:"For positive definite A, xᵀAx is always:",options:["Zero","Negative","Complex","Positive (for x ≠ 0)"],correctIndex:3,difficulty:"easy",explanation:"xᵀAx > 0 for all nonzero x defines positive definite."},{id:"s27-q3",question:"AᵀA is always:",options:["Positive semidefinite","Positive definite","Negative definite","Indefinite"],correctIndex:0,difficulty:"medium",explanation:"xᵀ(AᵀA)x = ||Ax||² ≥ 0, so AᵀA is positive semidefinite."},{id:"s27-q4",question:"A positive definite matrix can be factored as:",options:["A = LU","A = LLᵀ (Cholesky)","A = QR","A = SVD"],correctIndex:1,difficulty:"hard",explanation:"Cholesky: A = LLᵀ with L lower triangular, positive diagonal."}],O=[{id:"ch06-q1",question:"Eigenvalue λ = 0 means:",options:["A is invertible","A is symmetric","A is singular","A is orthogonal"],correctIndex:2,difficulty:"easy",explanation:"λ = 0 ⟺ det(A) = 0 ⟺ A is singular."},{id:"ch06-q2",question:"If A has eigenvalue λ, then A² has eigenvalue:",options:["2λ","λ + 1","√λ","λ²"],correctIndex:3,difficulty:"medium",explanation:"Ax = λx ⟹ A²x = A(λx) = λ(Ax) = λ²x."},{id:"ch06-q3",question:"Similar matrices A and B = P⁻¹AP have:",options:["Same eigenvalues","Same eigenvectors","Same determinant only","Nothing in common"],correctIndex:0,difficulty:"medium",explanation:"Similar matrices share eigenvalues (same characteristic polynomial)."},{id:"ch06-q4",question:"An n×n matrix with n distinct eigenvalues is:",options:["Never diagonalizable","Always diagonalizable","Sometimes diagonalizable","Always singular"],correctIndex:1,difficulty:"hard",explanation:"Distinct eigenvalues guarantee n independent eigenvectors."},{id:"ch06-q5",question:"The spectral theorem applies to:",options:["All matrices","Only diagonal matrices","Symmetric matrices","Only 2×2 matrices"],correctIndex:2,difficulty:"medium",explanation:"Symmetric (or Hermitian) matrices have orthogonal eigenvectors."},{id:"ch06-q6",question:"A matrix with all positive pivots is:",options:["Symmetric","Orthogonal","Singular","Positive definite"],correctIndex:3,difficulty:"hard",explanation:"All positive pivots is one test for positive definiteness."},{id:"ch06-q7",question:"Eigenvalues of A⁻¹ are:",options:["1/λ for each λ of A","Same as A","-λ for each λ of A","λ² for each λ of A"],correctIndex:0,difficulty:"hard",explanation:"Ax = λx ⟹ x = A⁻¹(λx) ⟹ A⁻¹x = (1/λ)x."},{id:"ch06-q8",question:"The trace of a matrix equals:",options:["Product of eigenvalues","Sum of eigenvalues","Determinant","Rank"],correctIndex:1,difficulty:"easy",explanation:"trace(A) = λ₁ + λ₂ + ... + λₙ."},{id:"ch06-q9",question:"For differential equation u' = Au to be stable:",options:["det(A) > 0","All entries positive","trace(A) < 0 and det(A) > 0 (for 2×2)","A symmetric"],correctIndex:2,difficulty:"hard",explanation:"For 2×2: need both eigenvalues negative, which requires trace < 0, det > 0."},{id:"ch06-q10",question:"xᵀAx is called a:",options:["Linear form","Bilinear form","Determinant","Quadratic form"],correctIndex:3,difficulty:"easy",explanation:"xᵀAx is a quadratic form (degree 2 polynomial in components of x)."}],U=[{id:"s28-q1",question:"SVD decomposes A as:",options:["A = UΣVᵀ","A = LU","A = QR","A = SDS⁻¹"],correctIndex:0,difficulty:"easy",explanation:"A = UΣVᵀ where U, V are orthogonal and Σ is diagonal."},{id:"s28-q2",question:"The singular values are:",options:["Eigenvalues of A","Square roots of eigenvalues of AᵀA","Entries of A","Ranks"],correctIndex:1,difficulty:"medium",explanation:"σᵢ = √λᵢ where λᵢ are eigenvalues of AᵀA."},{id:"s28-q3",question:"SVD works for:",options:["Only square matrices","Only symmetric matrices","Any matrix","Only invertible matrices"],correctIndex:2,difficulty:"easy",explanation:"Every matrix has an SVD, rectangular or square."},{id:"s28-q4",question:"The columns of V in A = UΣVᵀ are:",options:["Eigenvectors of A","Left singular vectors","Random orthonormal vectors","Eigenvectors of AᵀA"],correctIndex:3,difficulty:"hard",explanation:"V contains right singular vectors = eigenvectors of AᵀA."}],R=[{id:"s29-q1",question:"The number of nonzero singular values equals:",options:["rank(A)","n","m","det(A)"],correctIndex:0,difficulty:"medium",explanation:"Number of nonzero σᵢ = rank of A."},{id:"s29-q2",question:"Singular values are always:",options:["Complex","Non-negative real","Negative","Zero"],correctIndex:1,difficulty:"easy",explanation:"σᵢ ≥ 0 (square roots of non-negative eigenvalues)."},{id:"s29-q3",question:"U contains eigenvectors of:",options:["A","Aᵀ","AAᵀ","AᵀA"],correctIndex:2,difficulty:"medium",explanation:"U has left singular vectors = eigenvectors of AAᵀ."},{id:"s29-q4",question:"If A is m×n with m > n, then Σ is:",options:["Square n×n","n×m","Always square","m×n with zeros below"],correctIndex:3,difficulty:"hard",explanation:"Σ is m×n, with n singular values and zeros below."}],N=[{id:"s30-q1",question:"PCA finds directions that:",options:["Maximize variance","Minimize variance","Keep all data","Are random"],correctIndex:0,difficulty:"easy",explanation:"Principal components are directions of maximum variance."},{id:"s30-q2",question:"The first principal component is:",options:["The smallest singular vector","The largest singular vector","Any column of V","A random direction"],correctIndex:1,difficulty:"medium",explanation:"First PC = right singular vector for largest singular value."},{id:"s30-q3",question:"Keeping only the top k singular values/vectors is called:",options:["Full SVD","Eigendecomposition","Low-rank approximation","QR factorization"],correctIndex:2,difficulty:"medium",explanation:"Truncated SVD gives the best rank-k approximation."},{id:"s30-q4",question:"The Eckart-Young theorem says the best rank-k approximation minimizes:",options:["||A||₂","rank(A)","trace(A)","||A - Aₖ||"],correctIndex:3,difficulty:"hard",explanation:"Aₖ = UₖΣₖVₖᵀ minimizes ||A - B||₂ among all rank-k matrices B."}],E=[{id:"s31-q1",question:"The pseudoinverse A⁺ is computed from SVD as:",options:["VΣ⁺Uᵀ","UΣVᵀ","VΣ⁻¹Uᵀ","Σ⁻¹"],correctIndex:0,difficulty:"medium",explanation:"A⁺ = VΣ⁺Uᵀ where Σ⁺ inverts nonzero singular values."},{id:"s31-q2",question:"For invertible A, the pseudoinverse equals:",options:["0","A⁻¹","A","Aᵀ"],correctIndex:1,difficulty:"easy",explanation:"When A is invertible, A⁺ = A⁻¹."},{id:"s31-q3",question:"A⁺b gives the least squares solution when:",options:["A is square","A has full column rank","A is any matrix","A is symmetric"],correctIndex:2,difficulty:"medium",explanation:"A⁺b is the minimum-norm least squares solution for any A."},{id:"s31-q4",question:"The polar decomposition writes A as:",options:["A = QR","A = LU","A = UΣ","A = QS where Q orthogonal, S symmetric positive semidefinite"],correctIndex:3,difficulty:"hard",explanation:"A = QS (or A = SQ) with Q orthogonal, S = √(AᵀA)."}],L=[{id:"ch07-q1",question:"In A = UΣVᵀ, U and V are:",options:["Orthogonal","Lower triangular","Upper triangular","Diagonal"],correctIndex:0,difficulty:"easy",explanation:"U and V are orthogonal matrices (orthonormal columns)."},{id:"ch07-q2",question:"||A||₂ (spectral norm) equals:",options:["Sum of singular values","Largest singular value σ₁","Product of singular values","Trace of Σ"],correctIndex:1,difficulty:"medium",explanation:"The 2-norm is the largest singular value."},{id:"ch07-q3",question:"The condition number κ(A) = :",options:["σ₁","σ₁ + σᵣ","σ₁/σᵣ","σ₁ - σᵣ"],correctIndex:2,difficulty:"medium",explanation:"κ = σmax/σmin = σ₁/σᵣ measures sensitivity to errors."},{id:"ch07-q4",question:"SVD reveals the four fundamental subspaces via:",options:["Only U","Only V","Σ entries","U and V columns"],correctIndex:3,difficulty:"hard",explanation:"First r columns of U span C(A), last m-r span N(Aᵀ), etc."},{id:"ch07-q5",question:"For data compression, SVD is used because:",options:["Low-rank approximation is optimal","It's fast","It works only on images","It increases data size"],correctIndex:0,difficulty:"medium",explanation:"Eckart-Young: best low-rank approximation in Frobenius and 2-norm."},{id:"ch07-q6",question:"The Frobenius norm ||A||F equals:",options:["σ₁","√(σ₁² + ... + σᵣ²)","σ₁ + ... + σᵣ","σ₁σ₂...σᵣ"],correctIndex:1,difficulty:"hard",explanation:"||A||F = √(sum of σᵢ²) = √(sum of all aᵢⱼ²)."},{id:"ch07-q7",question:"PCA on mean-centered data X uses SVD of:",options:["Xᵀ","XXᵀ","X","XᵀX"],correctIndex:2,difficulty:"medium",explanation:"SVD of X directly gives principal components in V."},{id:"ch07-q8",question:"The nuclear norm ||A||* is:",options:["σ₁","Product of singular values","Largest eigenvalue","Sum of all singular values"],correctIndex:3,difficulty:"hard",explanation:"||A||* = σ₁ + σ₂ + ... + σᵣ (trace norm)."},{id:"ch07-q9",question:"Image compression via SVD stores:",options:["Uₖ, Σₖ, Vₖ","The full matrix A","Only Σ","Only U"],correctIndex:0,difficulty:"medium",explanation:"Store top k singular values and corresponding u and v vectors."},{id:"ch07-q10",question:"A⁺A is:",options:["Always I","Projection onto row space","Projection onto null space","Always 0"],correctIndex:1,difficulty:"hard",explanation:"A⁺A projects onto the row space of A."}],V=[{id:"s32-q1",question:"A transformation T is linear if:",options:["T(x) = Ax for some A","T(0) = 0","T(x + y) = T(x) + T(y) and T(cx) = cT(x)","All of these are equivalent"],correctIndex:2,difficulty:"easy",explanation:"Linearity means preserving addition and scalar multiplication."},{id:"s32-q2",question:"The matrix of a linear transformation is found by:",options:["Guessing","Taking the inverse","Computing the determinant","Applying T to standard basis vectors"],correctIndex:3,difficulty:"medium",explanation:"Column j of A is T(eⱼ), where eⱼ is the jth standard basis vector."},{id:"s32-q3",question:"The kernel of a linear transformation is:",options:["The null space","The range","The identity","The inverse"],correctIndex:0,difficulty:"easy",explanation:"ker(T) = {x : T(x) = 0} = null space of the matrix."},{id:"s32-q4",question:"rank(T) + nullity(T) = :",options:["0","dim(domain)","dim(range)","1"],correctIndex:1,difficulty:"hard",explanation:"Rank-nullity theorem: dim(range) + dim(kernel) = dim(domain)."}],F=[{id:"s33-q1",question:"Rotation by angle θ in 2D has matrix:",options:["[[sin θ, cos θ], [cos θ, sin θ]]","[[1, θ], [θ, 1]]","[[cos θ, -sin θ], [sin θ, cos θ]]","[[θ, 0], [0, θ]]"],correctIndex:2,difficulty:"medium",explanation:"Rotation: [[cos θ, -sin θ], [sin θ, cos θ]]."},{id:"s33-q2",question:"Projection onto the x-axis has matrix:",options:["[[0, 0], [0, 1]]","[[1, 0], [0, 1]]","[[0, 1], [1, 0]]","[[1, 0], [0, 0]]"],correctIndex:3,difficulty:"easy",explanation:"(x, y) → (x, 0), so matrix is [[1, 0], [0, 0]]."},{id:"s33-q3",question:"The composition ST corresponds to matrix:",options:["ST (matrix product)","S + T","S - T","TS"],correctIndex:0,difficulty:"medium",explanation:"Composition of linear maps = product of matrices."},{id:"s33-q4",question:"Reflection across the line y = x has matrix:",options:["[[1, 0], [0, -1]]","[[0, 1], [1, 0]]","[[-1, 0], [0, 1]]","[[1, 0], [0, 1]]"],correctIndex:1,difficulty:"hard",explanation:"(x, y) → (y, x), so matrix swaps coordinates: [[0, 1], [1, 0]]."}],j=[{id:"s34-q1",question:"If B is a basis and P is the change of basis matrix, then coordinates transform as:",options:["[x]_B = P[x]","[x]_B = P⁻¹[x]","[x] = P[x]_B","Both B and C"],correctIndex:2,difficulty:"medium",explanation:"Standard coords = P × (coords in basis B)."},{id:"s34-q2",question:"Similar matrices represent:",options:["Different transformations","Inverse transformations","Orthogonal transformations","The same transformation in different bases"],correctIndex:3,difficulty:"medium",explanation:"B = P⁻¹AP means A and B are the same transformation, different bases."},{id:"s34-q3",question:"Diagonalization A = SDS⁻¹ is a change of basis to:",options:["Eigenvector basis","Standard basis","Random basis","Orthonormal basis"],correctIndex:0,difficulty:"hard",explanation:"In the eigenvector basis, the transformation is diagonal."},{id:"s34-q4",question:"Similar matrices always have the same:",options:["Entries","Eigenvalues","Eigenvectors","Columns"],correctIndex:1,difficulty:"medium",explanation:"Similar matrices share eigenvalues, determinant, trace, rank."}],X=[{id:"ch08-q1",question:"T(x) = x + b (translation) is:",options:["Linear","Linear only if b = 0","Not linear","Always invertible"],correctIndex:2,difficulty:"easy",explanation:"T(0) = b ≠ 0 (unless b = 0), so not linear."},{id:"ch08-q2",question:"The image (range) of T: ℝ² → ℝ³ is a subspace of:",options:["ℝ²","ℝ⁵","ℝ","ℝ³"],correctIndex:3,difficulty:"easy",explanation:"Range is a subspace of the codomain ℝ³."},{id:"ch08-q3",question:"If T is 1-1 (injective), then:",options:["ker(T) = {0}","T is onto","T is not linear","rank(T) = 0"],correctIndex:0,difficulty:"medium",explanation:"1-1 means only 0 maps to 0, so null space is trivial."},{id:"ch08-q4",question:"Scaling by factor k in all directions has matrix:",options:["I/k","kI","k + I","kᵀ"],correctIndex:1,difficulty:"easy",explanation:"Uniform scaling: x → kx, matrix is kI."},{id:"ch08-q5",question:"The derivative d/dx is a linear transformation on:",options:["ℝ²","ℝ³","The space of polynomials","Only constants"],correctIndex:2,difficulty:"medium",explanation:"d/dx(f + g) = df/dx + dg/dx and d/dx(cf) = c(df/dx)."},{id:"ch08-q6",question:"Integration ∫₀ˣ is linear because:",options:["It gives polynomials","It's the inverse of d/dx","It maps to ℝ","∫(f+g) = ∫f + ∫g and ∫(cf) = c∫f"],correctIndex:3,difficulty:"medium",explanation:"Integration is linear: respects sums and scalar multiples."},{id:"ch08-q7",question:"The matrix of T relative to basis B has columns that are:",options:["T(basis vectors) in coordinates relative to B","The basis vectors","Eigenvalues","Random"],correctIndex:0,difficulty:"hard",explanation:"Column j is [T(vⱼ)]_B, coordinates of T(vⱼ) in basis B."},{id:"ch08-q8",question:"An isomorphism is a linear transformation that is:",options:["Only 1-1","1-1 and onto (bijective)","Only onto","Neither 1-1 nor onto"],correctIndex:1,difficulty:"medium",explanation:"Isomorphism = bijective linear map = invertible."},{id:"ch08-q9",question:"dim(V) = dim(W) and T: V → W is 1-1 implies T is:",options:["Not onto","Not linear","Onto","Zero"],correctIndex:2,difficulty:"hard",explanation:"By rank-nullity: 1-1 means nullity = 0, so rank = dim(V) = dim(W), hence onto."},{id:"ch08-q10",question:"Jordan form is used when a matrix is:",options:["Diagonalizable","Symmetric","Orthogonal","Not diagonalizable"],correctIndex:3,difficulty:"hard",explanation:"Jordan form generalizes diagonalization for non-diagonalizable matrices."}],M=[{id:"s35-q1",question:"The imaginary unit i satisfies:",options:["i² = -1","i = 1","i² = 1","i = -1"],correctIndex:0,difficulty:"easy",explanation:"By definition, i² = -1, so i = √(-1)."},{id:"s35-q2",question:"The complex conjugate of a + bi is:",options:["a + bi","a - bi","-a + bi","-a - bi"],correctIndex:1,difficulty:"easy",explanation:"Conjugate flips the sign of the imaginary part: z̄ = a - bi."},{id:"s35-q3",question:"|a + bi| equals:",options:["a + b","a² + b²","√(a² + b²)","|a| + |b|"],correctIndex:2,difficulty:"medium",explanation:"Modulus: |z| = √(a² + b²), the distance from origin."},{id:"s35-q4",question:"e^(iθ) equals:",options:["cos θ + sin θ","i(cos θ + sin θ)","cos θ - i sin θ","cos θ + i sin θ"],correctIndex:3,difficulty:"hard",explanation:"Euler's formula: e^(iθ) = cos θ + i sin θ."}],W=[{id:"s36-q1",question:"The Hermitian transpose (conjugate transpose) of A is:",options:["Aᴴ = Āᵀ","Aᵀ","Ā","A*"],correctIndex:0,difficulty:"easy",explanation:"Hermitian transpose: Aᴴ = Āᵀ (transpose and conjugate)."},{id:"s36-q2",question:"A Hermitian matrix satisfies:",options:["A = Aᵀ","A = Aᴴ","A = -Aᴴ","A = A⁻¹"],correctIndex:1,difficulty:"medium",explanation:"Hermitian: A = Aᴴ (equals its own conjugate transpose)."},{id:"s36-q3",question:"Eigenvalues of a Hermitian matrix are:",options:["Complex","Purely imaginary","Real","Zero"],correctIndex:2,difficulty:"medium",explanation:"Hermitian matrices always have real eigenvalues."},{id:"s36-q4",question:"A unitary matrix U satisfies:",options:["UᵀU = I","U² = I","U + Uᴴ = I","UᴴU = I"],correctIndex:3,difficulty:"hard",explanation:"Unitary: UᴴU = I, the complex analog of orthogonal."}],H=[{id:"s37-q1",question:"The DFT matrix F has entries:",options:["Fⱼₖ = e^(2πijk/n)","Fⱼₖ = jk","Fⱼₖ = 1/(j+k)","Fⱼₖ = cos(jk)"],correctIndex:0,difficulty:"hard",explanation:"DFT: Fⱼₖ = ωⁿʲᵏ where ωₙ = e^(2πi/n)."},{id:"s37-q2",question:"The FFT reduces DFT complexity from O(n²) to:",options:["O(n)","O(n log n)","O(n³)","O(log n)"],correctIndex:1,difficulty:"medium",explanation:"FFT uses divide-and-conquer to achieve O(n log n)."},{id:"s37-q3",question:"The Fourier matrix F is:",options:["Real symmetric","Lower triangular","Complex unitary (after scaling)","Sparse"],correctIndex:2,difficulty:"hard",explanation:"(1/√n)F is unitary: preserves lengths in complex space."},{id:"s37-q4",question:"FFT is most efficient when n is:",options:["Prime","Odd","Any integer","A power of 2"],correctIndex:3,difficulty:"medium",explanation:"Powers of 2 allow perfect halving at each step."}],Y=[{id:"ch09-q1",question:"z · z̄ equals:",options:["|z|²","0","2a (if z = a + bi)","z²"],correctIndex:0,difficulty:"easy",explanation:"z · z̄ = (a + bi)(a - bi) = a² + b² = |z|²."},{id:"ch09-q2",question:"The complex inner product ⟨u, v⟩ is:",options:["uᵀv","uᴴv","ūᵀv","u · v"],correctIndex:1,difficulty:"medium",explanation:"Complex inner product: ⟨u, v⟩ = uᴴv = Σ ūᵢvᵢ."},{id:"ch09-q3",question:"Skew-Hermitian matrix satisfies:",options:["A = Aᴴ","A = Aᵀ","A = -Aᴴ","A = -A"],correctIndex:2,difficulty:"hard",explanation:"Skew-Hermitian: A = -Aᴴ (eigenvalues are purely imaginary)."},{id:"ch09-q4",question:"The n-th roots of unity are evenly spaced on:",options:["The real line","The imaginary axis","A random curve","The unit circle"],correctIndex:3,difficulty:"medium",explanation:"ωₙᵏ = e^(2πik/n) are n points on the unit circle."},{id:"ch09-q5",question:"Real symmetric is to orthogonal as Hermitian is to:",options:["Unitary","Symmetric","Orthogonal","Diagonal"],correctIndex:0,difficulty:"medium",explanation:"Unitary is the complex analog of orthogonal."},{id:"ch09-q6",question:"Eigenvectors of a Hermitian matrix (for distinct eigenvalues) are:",options:["Parallel","Orthogonal","Real","Zero"],correctIndex:1,difficulty:"hard",explanation:"Just like symmetric matrices, Hermitian has orthogonal eigenvectors."},{id:"ch09-q7",question:"In polar form, z = re^(iθ), r represents:",options:["The argument","The real part","The modulus |z|","The imaginary part"],correctIndex:2,difficulty:"easy",explanation:"r = |z| is the distance from origin (modulus)."},{id:"ch09-q8",question:"The FFT key insight is that ωₙ² = :",options:["ωₙ","ω₂ₙ","1","ωₙ/₂"],correctIndex:3,difficulty:"hard",explanation:"ωₙ² = e^(4πi/n) = e^(2πi/(n/2)) = ωₙ/₂, allowing recursion."},{id:"ch09-q9",question:"For unitary U, |Ux| = :",options:["|x|","0","|U||x|","|x|²"],correctIndex:0,difficulty:"medium",explanation:"Unitary matrices preserve length: |Ux| = |x|."},{id:"ch09-q10",question:"The spectral theorem for Hermitian A says A = :",options:["LU","UDUᴴ with U unitary, D real diagonal","QR","SVD"],correctIndex:1,difficulty:"hard",explanation:"A = UDUᴴ where U is unitary and D has real eigenvalues."}],Z=[{id:"s38-q1",question:"The incidence matrix of a graph has rows for:",options:["Nodes","Paths","Edges","Cycles"],correctIndex:2,difficulty:"easy",explanation:"Each row represents an edge, columns represent nodes."},{id:"s38-q2",question:"The null space of the incidence matrix contains:",options:["Edge flows","Shortest paths","Cycles","Node potentials (up to a constant)"],correctIndex:3,difficulty:"medium",explanation:"Ax = 0 when all nodes have equal potential (constant vector)."},{id:"s38-q3",question:"Kirchhoff's Current Law says currents sum to zero at:",options:["Nodes","Edges","The boundary","Cycles"],correctIndex:0,difficulty:"medium",explanation:"Current in = current out at every node (conservation)."},{id:"s38-q4",question:"The number of independent loops in a connected graph with n nodes and m edges is:",options:["n - 1","m - n + 1","m + n","n"],correctIndex:1,difficulty:"hard",explanation:"Loops = m - n + 1 (Euler's formula for graphs)."}],G=[{id:"s39-q1",question:"A Markov matrix has columns that:",options:["Sum to 0","Are orthogonal","Sum to 1","Are unit vectors"],correctIndex:2,difficulty:"easy",explanation:"Columns are probability distributions: non-negative, sum to 1."},{id:"s39-q2",question:"Every Markov matrix has eigenvalue:",options:["0","-1","2","1"],correctIndex:3,difficulty:"medium",explanation:"λ = 1 always (eigenvector is steady state distribution)."},{id:"s39-q3",question:"The steady state satisfies:",options:["Ax = x","Ax = 0","Ax = 2x","A²x = x"],correctIndex:0,difficulty:"medium",explanation:"Steady state x satisfies Ax = x (eigenvalue 1)."},{id:"s39-q4",question:"For a positive Markov matrix, Aᵏ converges to:",options:["Zero matrix","Rank-1 matrix (steady state × ones)","Identity","Diagonal matrix"],correctIndex:1,difficulty:"hard",explanation:"All columns converge to the steady state eigenvector."}],J=[{id:"s40-q1",question:"Linear programming optimizes a linear objective subject to:",options:["Quadratic constraints","No constraints","Linear inequality constraints","Equality constraints only"],correctIndex:2,difficulty:"easy",explanation:"LP: minimize cᵀx subject to Ax ≤ b, x ≥ 0."},{id:"s40-q2",question:"The feasible region of an LP is:",options:["A sphere","A line","A hyperboloid","A polyhedron (polytope)"],correctIndex:3,difficulty:"medium",explanation:"Linear inequalities create a convex polytope."},{id:"s40-q3",question:"The optimal solution of an LP (if bounded) occurs at:",options:["A vertex (corner)","The center","An edge","Any point"],correctIndex:0,difficulty:"medium",explanation:"Optimum is at a vertex of the feasible polytope."},{id:"s40-q4",question:"The simplex method moves along:",options:["Random directions","Edges of the polytope","The interior","Diagonals"],correctIndex:1,difficulty:"hard",explanation:"Simplex walks along edges from vertex to vertex."}],K=[{id:"s41-q1",question:"Fourier series represents functions as sums of:",options:["Polynomials","Exponentials","Sines and cosines","Step functions"],correctIndex:2,difficulty:"easy",explanation:"f(x) = a₀ + Σ(aₙcos(nx) + bₙsin(nx))."},{id:"s41-q2",question:"The functions sin(nx) and sin(mx) for n ≠ m are:",options:["Parallel","Equal","Opposite","Orthogonal"],correctIndex:3,difficulty:"medium",explanation:"∫sin(nx)sin(mx)dx = 0 for n ≠ m (orthogonality)."},{id:"s41-q3",question:"Fourier coefficients are found using:",options:["Inner products (integrals)","Derivatives","Matrix inversion","Eigenvalues"],correctIndex:0,difficulty:"medium",explanation:"aₙ = (2/π)∫f(x)cos(nx)dx, projection onto basis."},{id:"s41-q4",question:"Parseval's theorem relates energy in time domain to:",options:["Phase","Energy in frequency domain","Amplitude only","Nothing"],correctIndex:1,difficulty:"hard",explanation:"∫|f|² = Σ|coefficients|² (energy conservation)."}],_=[{id:"s42-q1",question:"2D rotation by θ is represented by matrix:",options:["[[θ, 0], [0, θ]]","[[1, θ], [0, 1]]","[[cos θ, -sin θ], [sin θ, cos θ]]","[[θ, 1], [1, θ]]"],correctIndex:2,difficulty:"medium",explanation:"Rotation matrix preserves lengths and rotates by θ."},{id:"s42-q2",question:"Homogeneous coordinates use dimension:",options:["n","n - 1","2n","n + 1"],correctIndex:3,difficulty:"medium",explanation:"(x, y) becomes (x, y, 1) to allow translation as matrix."},{id:"s42-q3",question:"Translation in 2D using 3×3 homogeneous matrix looks like:",options:["[[1,0,tx],[0,1,ty],[0,0,1]]","[[tx,0,0],[0,ty,0],[0,0,1]]","[[1,tx,0],[ty,1,0],[0,0,1]]","[[tx,ty,1],[0,0,0],[0,0,0]]"],correctIndex:0,difficulty:"hard",explanation:"Translation matrix has offset in the last column."},{id:"s42-q4",question:"Composing transformations A then B gives:",options:["A + B","BA","AB","A - B"],correctIndex:1,difficulty:"medium",explanation:"Apply A first, then B: result is BA (right to left)."}],$=[{id:"s43-q1",question:"The Hill cipher encrypts using:",options:["Addition mod 26","XOR","Matrix multiplication mod 26","Substitution"],correctIndex:2,difficulty:"medium",explanation:"Ciphertext = A × plaintext (mod 26)."},{id:"s43-q2",question:"For the Hill cipher, the key matrix must be:",options:["Symmetric","Orthogonal","Diagonal","Invertible mod 26"],correctIndex:3,difficulty:"medium",explanation:"Need A⁻¹ (mod 26) to exist for decryption."},{id:"s43-q3",question:"RSA encryption relies on the difficulty of:",options:["Factoring large numbers","Matrix inversion","Solving linear systems","Computing eigenvalues"],correctIndex:0,difficulty:"easy",explanation:"RSA security: hard to factor n = pq for large primes."},{id:"s43-q4",question:"Public-key cryptography uses:",options:["Same key for encryption and decryption","Different keys for encryption and decryption","No keys","Random keys each time"],correctIndex:1,difficulty:"easy",explanation:"Public key encrypts, private key decrypts (asymmetric)."}],ee=[{id:"s44-q1",question:"A grayscale image can be represented as:",options:["A vector","A single number","A matrix of pixel intensities","A complex number"],correctIndex:2,difficulty:"easy",explanation:"Image = m×n matrix where entries are pixel values."},{id:"s44-q2",question:"Image compression via SVD keeps:",options:["All singular values","Only the smallest singular values","Random singular values","Only the largest k singular values"],correctIndex:3,difficulty:"medium",explanation:"Low-rank approximation: keep top k singular values/vectors."},{id:"s44-q3",question:"Blurring an image is equivalent to:",options:["Convolution with a smoothing kernel","Matrix addition","Taking the inverse","Computing eigenvalues"],correctIndex:0,difficulty:"medium",explanation:"Blur = convolution with averaging or Gaussian kernel."},{id:"s44-q4",question:"Edge detection uses kernels that approximate:",options:["Integration","Derivatives (gradients)","Fourier transform","Identity"],correctIndex:1,difficulty:"hard",explanation:"Edges = high gradient; Sobel/Laplacian detect derivatives."}],ie=[{id:"ch10-q1",question:"In a graph, the left null space of incidence matrix A represents:",options:["Spanning trees","Node degrees","Cycle flows","Shortest paths"],correctIndex:2,difficulty:"hard",explanation:"yᵀA = 0 gives flows around cycles (Kirchhoff's Voltage Law)."},{id:"ch10-q2",question:"All eigenvalues of a Markov matrix satisfy:",options:["|λ| = 1","λ > 1","λ < 0","|λ| ≤ 1"],correctIndex:3,difficulty:"medium",explanation:"Markov eigenvalues have |λ| ≤ 1; λ = 1 is always present."},{id:"ch10-q3",question:"The dual of a linear program relates to:",options:["A different LP with transposed constraints","A quadratic program","An integer program","A nonlinear program"],correctIndex:0,difficulty:"hard",explanation:"Primal and dual LPs have related optimal values."},{id:"ch10-q4",question:"Fourier series uses which type of basis?",options:["Polynomial","Orthogonal (trigonometric)","Standard","Random"],correctIndex:1,difficulty:"easy",explanation:"Sines and cosines form an orthogonal basis for functions."},{id:"ch10-q5",question:"Scaling by factor s in 2D has matrix:",options:["[[1, s], [s, 1]]","[[s, s], [0, 0]]","[[s, 0], [0, s]]","[[0, s], [s, 0]]"],correctIndex:2,difficulty:"easy",explanation:"Uniform scaling: sI = [[s, 0], [0, s]]."},{id:"ch10-q6",question:"In Hill cipher, gcd(det(A), 26) must equal:",options:["0","2","26","1"],correctIndex:3,difficulty:"hard",explanation:"det(A) must be coprime to 26 for A⁻¹ (mod 26) to exist."},{id:"ch10-q7",question:"SVD image compression stores O(k(m+n)) instead of:",options:["O(mn)","O(m+n)","O(k)","O(1)"],correctIndex:0,difficulty:"medium",explanation:"Full image: O(mn). Rank-k SVD: O(k(m+n)), huge savings."},{id:"ch10-q8",question:"The Google PageRank uses:",options:["Symmetric matrices","Markov matrices","Orthogonal matrices","Diagonal matrices"],correctIndex:1,difficulty:"medium",explanation:"PageRank is the steady state of a Markov chain."},{id:"ch10-q9",question:"Shearing transformation has matrix:",options:["[[k, 0], [0, k]]","[[0, 1], [1, 0]]","[[1, k], [0, 1]]","[[k, k], [k, k]]"],correctIndex:2,difficulty:"medium",explanation:"Shear: [[1, k], [0, 1]] shifts x by ky, keeps y."},{id:"ch10-q10",question:"The Gibbs phenomenon in Fourier series refers to:",options:["Perfect convergence","Undershoot everywhere","Zero coefficients","Overshoot near discontinuities"],correctIndex:3,difficulty:"hard",explanation:"Fourier series overshoots by ~9% at jump discontinuities."}],ne=[{id:"s45-q1",question:"Partial pivoting chooses the pivot with:",options:["Largest absolute value in column","Smallest absolute value","Random selection","First nonzero entry"],correctIndex:0,difficulty:"medium",explanation:"Partial pivoting: choose largest |entry| in column below pivot."},{id:"s45-q2",question:"Floating-point errors accumulate due to:",options:["Exact arithmetic","Rounding at each operation","Matrix size","Determinant value"],correctIndex:1,difficulty:"easy",explanation:"Finite precision means each operation introduces small errors."},{id:"s45-q3",question:"An ill-conditioned matrix has condition number that is:",options:["Close to 1","Negative","Very large","Zero"],correctIndex:2,difficulty:"medium",explanation:"Large κ(A) = σmax/σmin means high sensitivity to errors."},{id:"s45-q4",question:"Complete pivoting searches for the largest entry in:",options:["Current column only","Current row only","Diagonal only","Entire remaining submatrix"],correctIndex:3,difficulty:"hard",explanation:"Complete pivoting: max over all remaining entries (costly)."}],te=[{id:"s46-q1",question:"The Jacobi method updates xₙ₊₁ using:",options:["Only xₙ values","Mix of xₙ and xₙ₊₁","Only xₙ₊₁ values","Random values"],correctIndex:0,difficulty:"medium",explanation:"Jacobi uses all old values xₙ to compute new xₙ₊₁."},{id:"s46-q2",question:"Gauss-Seidel differs from Jacobi by:",options:["Being slower","Using updated values immediately","Never converging","Using random order"],correctIndex:1,difficulty:"medium",explanation:"Gauss-Seidel uses new values as soon as computed."},{id:"s46-q3",question:"Iterative methods converge when the spectral radius ρ(M) is:",options:["> 1","= 1","< 1","Complex"],correctIndex:2,difficulty:"hard",explanation:"Convergence requires ρ(M) < 1 for iteration matrix M."},{id:"s46-q4",question:"Conjugate gradient method is designed for:",options:["Any matrix","Singular matrices","Complex matrices","Symmetric positive definite matrices"],correctIndex:3,difficulty:"hard",explanation:"CG exploits symmetry and positive definiteness for efficiency."}],oe=[{id:"s47-q1",question:"Power iteration finds:",options:["The largest eigenvalue (in magnitude)","All eigenvalues","The smallest eigenvalue","The median eigenvalue"],correctIndex:0,difficulty:"medium",explanation:"Power method: Aᵏv converges to dominant eigenvector."},{id:"s47-q2",question:"Inverse iteration finds eigenvalues near:",options:["Zero","A chosen shift μ","Infinity","One"],correctIndex:1,difficulty:"hard",explanation:"(A - μI)⁻¹ has largest eigenvalue 1/(λ - μ) for λ nearest μ."},{id:"s47-q3",question:"The QR algorithm computes eigenvalues by:",options:["Direct formula","Random sampling","Repeated QR factorizations","Interpolation"],correctIndex:2,difficulty:"medium",explanation:"A = QR, then A' = RQ, repeat: converges to eigenvalue form."},{id:"s47-q4",question:"Shifting in QR algorithm (A - μI) accelerates convergence near:",options:["All eigenvalues","Zero","Infinity","The eigenvalue closest to μ"],correctIndex:3,difficulty:"hard",explanation:"Shift μ speeds up convergence for eigenvalue near μ."}],ae=[{id:"ch11-q1",question:"Why is pivoting important in Gaussian elimination?",options:["It prevents division by zero and reduces error","It makes the algorithm faster","It computes eigenvalues","It is not important"],correctIndex:0,difficulty:"easy",explanation:"Pivoting avoids zero pivots and small divisors (stability)."},{id:"ch11-q2",question:"The relative error in solving Ax = b is bounded by:",options:["det(A)","κ(A) × relative error in b","rank(A)","1/det(A)"],correctIndex:1,difficulty:"hard",explanation:"Condition number κ(A) amplifies input errors."},{id:"ch11-q3",question:"For a diagonally dominant matrix, Jacobi and Gauss-Seidel:",options:["Never converge","Sometimes converge","Always converge","Are identical"],correctIndex:2,difficulty:"medium",explanation:"Diagonal dominance guarantees convergence of both methods."},{id:"ch11-q4",question:"Successive Over-Relaxation (SOR) uses parameter ω to:",options:["Slow down convergence","Guarantee divergence","Compute eigenvalues","Speed up convergence"],correctIndex:3,difficulty:"hard",explanation:"Optimal ω (1 < ω < 2) can greatly accelerate Gauss-Seidel."},{id:"ch11-q5",question:"Power iteration requires the matrix to have:",options:["One dominant eigenvalue","All equal eigenvalues","Complex eigenvalues","Zero determinant"],correctIndex:0,difficulty:"medium",explanation:"Need |λ₁| > |λ₂| for convergence to λ₁ eigenvector."},{id:"ch11-q6",question:"The rate of convergence of power iteration depends on:",options:["Matrix size","|λ₂/λ₁|","The determinant","The trace"],correctIndex:1,difficulty:"hard",explanation:"Smaller |λ₂/λ₁| means faster convergence."},{id:"ch11-q7",question:"Hessenberg form (for QR algorithm) has zeros:",options:["Everywhere","Above the diagonal","Below the subdiagonal","On the diagonal"],correctIndex:2,difficulty:"hard",explanation:"Upper Hessenberg: aᵢⱼ = 0 for i > j + 1."},{id:"ch11-q8",question:"Sparse matrices benefit from iterative methods because:",options:["Direct methods fail","They are always faster","They give exact answers","They preserve sparsity"],correctIndex:3,difficulty:"medium",explanation:"Direct methods create fill-in; iterative methods preserve sparsity."},{id:"ch11-q9",question:"The Rayleigh quotient R(x) = xᵀAx/xᵀx approximates:",options:["An eigenvalue","The determinant","The trace","The norm"],correctIndex:0,difficulty:"medium",explanation:"R(x) equals an eigenvalue when x is an eigenvector."},{id:"ch11-q10",question:"Krylov subspace methods build solutions from:",options:["Random vectors","Span of {b, Ab, A²b, ...}","Eigenvectors only","Null space vectors"],correctIndex:1,difficulty:"hard",explanation:"Krylov space Kₖ = span{b, Ab, ..., Aᵏ⁻¹b}."}],se=[{id:"s48-q1",question:"The mean of a random variable X is also called:",options:["Variance","Standard deviation","Expected value E[X]","Mode"],correctIndex:2,difficulty:"easy",explanation:"Mean = expected value = μ = E[X]."},{id:"s48-q2",question:"Variance Var(X) measures:",options:["The center","The maximum","The minimum","The spread (average squared deviation)"],correctIndex:3,difficulty:"easy",explanation:"Var(X) = E[(X - μ)²] = E[X²] - (E[X])²."},{id:"s48-q3",question:"For independent X and Y, Var(X + Y) = :",options:["Var(X) + Var(Y)","Var(X) - Var(Y)","Var(X) × Var(Y)","Var(X)/Var(Y)"],correctIndex:0,difficulty:"medium",explanation:"Variances add for independent random variables."},{id:"s48-q4",question:"The covariance Cov(X,Y) is positive when:",options:["X and Y move in opposite directions","X and Y move together","X and Y are independent","X = Y"],correctIndex:1,difficulty:"medium",explanation:"Positive covariance: X high when Y high (and vice versa)."}],re=[{id:"s49-q1",question:"The covariance matrix Σ is always:",options:["Diagonal","Orthogonal","Symmetric positive semidefinite","Upper triangular"],correctIndex:2,difficulty:"medium",explanation:"Σ = E[(X - μ)(X - μ)ᵀ] is symmetric and PSD."},{id:"s49-q2",question:"The diagonal entries of the covariance matrix are:",options:["Covariances","Means","Correlations","Variances"],correctIndex:3,difficulty:"easy",explanation:"Σᵢᵢ = Var(Xᵢ), diagonal = individual variances."},{id:"s49-q3",question:"Correlation coefficient ρ is between:",options:["-1 and +1","0 and 1","-∞ and +∞","0 and +∞"],correctIndex:0,difficulty:"medium",explanation:"ρ = Cov(X,Y)/(σₓσᵧ), always -1 ≤ ρ ≤ 1."},{id:"s49-q4",question:"For a linear transformation Y = AX, the covariance of Y is:",options:["AΣ","AΣAᵀ","ΣA","AᵀΣA"],correctIndex:1,difficulty:"hard",explanation:"Cov(AX) = A Cov(X) Aᵀ = AΣAᵀ."}],ce=[{id:"s50-q1",question:"The multivariate normal is characterized by:",options:["Mean only","Variance only","Mean vector μ and covariance matrix Σ","Just the dimension"],correctIndex:2,difficulty:"easy",explanation:"N(μ, Σ): mean μ and covariance Σ fully describe it."},{id:"s50-q2",question:"In linear regression, the normal equations are:",options:["Ax = b","β = y/X","Xβ = y exactly","XᵀXβ = Xᵀy"],correctIndex:3,difficulty:"medium",explanation:"Least squares: XᵀXβ̂ = Xᵀy minimizes ||y - Xβ||²."},{id:"s50-q3",question:"Principal components are eigenvectors of:",options:["The covariance matrix","The data matrix","The mean vector","The identity"],correctIndex:0,difficulty:"medium",explanation:"PCA: eigenvectors of Σ give principal directions."},{id:"s50-q4",question:"The first principal component captures:",options:["Minimum variance","Maximum variance direction","Zero variance","Mean"],correctIndex:1,difficulty:"medium",explanation:"PC1 = direction of largest variance (top eigenvector)."},{id:"s50-q5",question:"Machine learning feature extraction often uses:",options:["Only raw features","Random projection","PCA or SVD for dimensionality reduction","No preprocessing"],correctIndex:2,difficulty:"hard",explanation:"SVD/PCA reduce dimensions while preserving variance."}],le=[{id:"ch12-q1",question:"The sample covariance matrix from data matrix X (centered) is:",options:["XXᵀ","X + Xᵀ","X⁻¹","XᵀX/(n-1)"],correctIndex:3,difficulty:"medium",explanation:"S = XᵀX/(n-1) for centered data (n samples)."},{id:"ch12-q2",question:"If Σ is singular, the distribution is:",options:["Degenerate (lower-dimensional)","Non-existent","Uniform","Discrete"],correctIndex:0,difficulty:"hard",explanation:"Singular Σ means data lies in a lower-dimensional subspace."},{id:"ch12-q3",question:"The variance explained by top k principal components equals:",options:["k","Sum of top k eigenvalues / sum of all eigenvalues","Product of eigenvalues","k/n"],correctIndex:1,difficulty:"hard",explanation:"(λ₁ + ... + λₖ)/(λ₁ + ... + λₙ) = fraction of variance."},{id:"ch12-q4",question:"In ridge regression, we minimize ||y - Xβ||² + :",options:["||β||","||X||","λ||β||²","0"],correctIndex:2,difficulty:"hard",explanation:"Ridge: add λ||β||² penalty for regularization."},{id:"ch12-q5",question:"For uncorrelated variables, the covariance matrix is:",options:["Identity","Zero","Full rank","Diagonal"],correctIndex:3,difficulty:"medium",explanation:"Uncorrelated: Cov(Xᵢ, Xⱼ) = 0 for i ≠ j, so Σ is diagonal."},{id:"ch12-q6",question:"The Mahalanobis distance uses:",options:["Covariance matrix inverse Σ⁻¹","Identity matrix","Mean vector","Standard deviation"],correctIndex:0,difficulty:"hard",explanation:"d² = (x-μ)ᵀΣ⁻¹(x-μ) accounts for correlation."},{id:"ch12-q7",question:"Linear discriminant analysis (LDA) finds directions that:",options:["Maximize within-class variance","Maximize between-class / within-class variance","Minimize all variance","Random directions"],correctIndex:1,difficulty:"hard",explanation:"LDA: maximize separation between classes."},{id:"ch12-q8",question:"The best linear predictor of Y given X is:",options:["E[Y]","Cov(X,Y)","E[Y] + Cov(Y,X)Var(X)⁻¹(X - E[X])","Var(Y)"],correctIndex:2,difficulty:"hard",explanation:"Linear regression formula using covariance."},{id:"ch12-q9",question:"Whitening transforms data to have covariance:",options:["Zero matrix","Diagonal matrix","Original Σ","Identity matrix"],correctIndex:3,difficulty:"medium",explanation:"Whitening: W = Σ^(-1/2) gives WXWᵀ = I."},{id:"ch12-q10",question:"Factor analysis assumes data X = :",options:["Lf + noise (low-rank + noise)","μ + noise","Random","Exactly low-rank"],correctIndex:0,difficulty:"hard",explanation:"X = μ + Lf + ε: latent factors f, loadings L, noise ε."}],de={"01":n,"02":t,"03":o,"04":s,"05":r,"06":c,"07":l,"08":d,"09":u,10:m,11:f,12:h,13:x,14:y,15:q,16:v,17:g,18:I,19:b,20:T,21:S,22:k,23:Q,24:C,25:P,26:D,27:B,28:U,29:R,30:N,31:E,32:V,33:F,34:j,35:M,36:W,37:H,38:Z,39:G,40:J,41:K,42:_,43:$,44:ee,45:ne,46:te,47:oe,48:se,49:re,50:ce},ue={"01":a,"02":p,"03":A,"04":w,"05":z,"06":O,"07":L,"08":X,"09":Y,10:ie,11:ae,12:le};function me(e){const i=e.toString().padStart(2,"0");return de[i]}function pe(e){const i=e.toString().padStart(2,"0");return ue[i]}export{me as a,pe as g};
