import{j as s}from"./vendor-animation-6GFvN5rC.js";import{L as t,D as e,T as r,E as l}from"./ContentBlocks-ByGNY4hF.js";import{I as a,M as i}from"./MathBlock-BTK9YHZB.js";import"./vendor-react-ByzHzWFU.js";import"./index-CIaFkRw9.js";import"./vendor-math-ClxlXyPc.js";import"./vendor-firebase-core-DIJkQv9Q.js";import"./index-Djvuv9mj.js";function j(){return s.jsxs(t,{sectionId:48,children:[s.jsxs("p",{children:[s.jsx("strong",{children:"Statistics"})," is built on linear algebra. Mean, variance, covariance, regressionâ€”all are matrix operations. Understanding this connection reveals the geometry behind statistical methods."]}),s.jsx("h2",{children:"Random Vectors"}),s.jsxs(e,{title:"Mean Vector",className:"my-6",children:[s.jsxs("p",{children:["For a random vector ",s.jsx(a,{children:"\\mathbf{X} = (X_1, \\ldots, X_n)^T"}),":"]}),s.jsx(i,{children:"\\boldsymbol{\\mu} = E[\\mathbf{X}] = \\begin{bmatrix} E[X_1] \\\\ \\vdots \\\\ E[X_n] \\end{bmatrix}"})]}),s.jsxs(e,{title:"Covariance Matrix",className:"my-6",children:[s.jsxs("p",{children:["The ",s.jsx("strong",{children:"covariance matrix"})," captures all pairwise relationships:"]}),s.jsx(i,{children:"\\Sigma = E[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T]"}),s.jsx(i,{children:"\\Sigma_{ij} = \\text{Cov}(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)]"}),s.jsx("p",{className:"mt-2",children:"Diagonal: variances. Off-diagonal: covariances."})]}),s.jsxs(r,{title:"Properties of Covariance Matrix",className:"my-6",proof:s.jsxs(s.Fragment,{children:[s.jsxs("p",{children:[s.jsx("strong",{children:"Symmetric:"})," ",s.jsx(a,{children:"\\Sigma_{ij} = \\text{Cov}(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)]"}),"."]}),s.jsxs("p",{className:"mt-2",children:["Since the product is commutative: ",s.jsx(a,{children:"\\Sigma_{ij} = \\Sigma_{ji}"}),", so ",s.jsx(a,{children:"\\Sigma = \\Sigma^T"}),"."]}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Positive semidefinite:"})," For any vector ",s.jsx(a,{children:"\\mathbf{a}"}),":"]}),s.jsx(i,{children:"\\mathbf{a}^T\\Sigma\\mathbf{a} = \\mathbf{a}^T E[(\\mathbf{X}-\\boldsymbol{\\mu})(\\mathbf{X}-\\boldsymbol{\\mu})^T]\\mathbf{a} = E[\\mathbf{a}^T(\\mathbf{X}-\\boldsymbol{\\mu})(\\mathbf{X}-\\boldsymbol{\\mu})^T\\mathbf{a}]"}),s.jsxs("p",{className:"mt-2",children:["Let ",s.jsx(a,{children:"Y = \\mathbf{a}^T(\\mathbf{X}-\\boldsymbol{\\mu})"}),". Then ",s.jsx(a,{children:"\\mathbf{a}^T\\Sigma\\mathbf{a} = E[Y^2] = \\text{Var}(Y) \\geq 0"}),"."]})]}),children:[s.jsxs("p",{children:["The covariance matrix ",s.jsx(a,{children:"\\Sigma"})," is:"]}),s.jsxs("ul",{className:"list-disc list-inside space-y-1",children:[s.jsxs("li",{children:[s.jsx("strong",{children:"Symmetric:"})," ",s.jsx(a,{children:"\\Sigma = \\Sigma^T"})]}),s.jsxs("li",{children:[s.jsx("strong",{children:"Positive semidefinite:"})," ",s.jsx(a,{children:"\\mathbf{a}^T\\Sigma\\mathbf{a} \\geq 0"})," for all ",s.jsx(a,{children:"\\mathbf{a}"})]})]})]}),s.jsx("h2",{children:"Linear Transformations of Random Vectors"}),s.jsxs(r,{title:"Transformation of Mean and Covariance",className:"my-6",proof:s.jsxs(s.Fragment,{children:[s.jsxs("p",{children:[s.jsx("strong",{children:"Mean:"})," Using linearity of expectation:"]}),s.jsx(i,{children:"E[\\mathbf{Y}] = E[A\\mathbf{X} + \\mathbf{b}] = AE[\\mathbf{X}] + \\mathbf{b}"}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Covariance:"})," Let ",s.jsx(a,{children:"\\boldsymbol{\\mu}_Y = E[\\mathbf{Y}]"}),". Then ",s.jsx(a,{children:"\\mathbf{Y} - \\boldsymbol{\\mu}_Y = A(\\mathbf{X} - \\boldsymbol{\\mu}_X)"}),"."]}),s.jsx(i,{children:"\\text{Cov}(\\mathbf{Y}) = E[(\\mathbf{Y} - \\boldsymbol{\\mu}_Y)(\\mathbf{Y} - \\boldsymbol{\\mu}_Y)^T]"}),s.jsx(i,{children:"= E[A(\\mathbf{X} - \\boldsymbol{\\mu}_X)(\\mathbf{X} - \\boldsymbol{\\mu}_X)^T A^T] = A\\Sigma_X A^T"})]}),children:[s.jsxs("p",{children:["If ",s.jsx(a,{children:"\\mathbf{Y} = A\\mathbf{X} + \\mathbf{b}"}),":"]}),s.jsx(i,{children:"E[\\mathbf{Y}] = AE[\\mathbf{X}] + \\mathbf{b}"}),s.jsx(i,{children:"\\text{Cov}(\\mathbf{Y}) = A\\Sigma_X A^T"})]}),s.jsxs(l,{title:"Portfolio Variance",className:"my-6",children:[s.jsxs("p",{children:["For portfolio return ",s.jsx(a,{children:"R = w_1R_1 + w_2R_2 = \\mathbf{w}^T\\mathbf{R}"}),":"]}),s.jsx(i,{children:"\\text{Var}(R) = \\mathbf{w}^T\\Sigma\\mathbf{w}"}),s.jsx("p",{className:"mt-2",children:"This is a quadratic form! Minimizing portfolio variance is a linear algebra problem."})]}),s.jsx("h2",{children:"The Multivariate Normal Distribution"}),s.jsxs(e,{title:"Multivariate Normal",className:"my-6",children:[s.jsxs("p",{children:[s.jsx(a,{children:"\\mathbf{X} \\sim N(\\boldsymbol{\\mu}, \\Sigma)"})," has density:"]}),s.jsx(i,{children:"f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)"}),s.jsxs("p",{className:"mt-2",children:["The exponent is a quadratic form with matrix ",s.jsx(a,{children:"\\Sigma^{-1}"}),"."]})]}),s.jsxs(r,{title:"Geometry of Multivariate Normal",className:"my-6",proof:s.jsxs(s.Fragment,{children:[s.jsxs("p",{children:[s.jsx("strong",{children:"Constant density contours:"})," The density is constant when the exponent ",s.jsx(a,{children:"(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) = c^2"})," is constant."]}),s.jsxs("p",{className:"mt-2",children:["This is the equation of an ellipsoid centered at ",s.jsx(a,{children:"\\boldsymbol{\\mu}"}),"."]}),s.jsxs("p",{className:"mt-2",children:[s.jsx("strong",{children:"Principal axes:"})," Diagonalize ",s.jsx(a,{children:"\\Sigma = Q\\Lambda Q^T"}),". In transformed coordinates ",s.jsx(a,{children:"\\mathbf{y} = Q^T(\\mathbf{x} - \\boldsymbol{\\mu})"}),":"]}),s.jsx(i,{children:"\\mathbf{y}^T\\Lambda^{-1}\\mathbf{y} = \\sum_i \\frac{y_i^2}{\\lambda_i} = c^2"}),s.jsxs("p",{className:"mt-2",children:["This is an axis-aligned ellipsoid with semi-axes ",s.jsx(a,{children:"c\\sqrt{\\lambda_i}"}),". The eigenvectors of ",s.jsx(a,{children:"\\Sigma"})," give the principal directions."]})]}),children:[s.jsxs("p",{children:["Contours of constant density are ellipsoids. The eigendecomposition ",s.jsx(a,{children:"\\Sigma = Q\\Lambda Q^T"}),":"]}),s.jsxs("ul",{className:"list-disc list-inside space-y-1",children:[s.jsxs("li",{children:[s.jsxs("strong",{children:["Eigenvectors ",s.jsx(a,{children:"Q"}),":"]})," principal axes of the ellipsoid"]}),s.jsxs("li",{children:[s.jsxs("strong",{children:["Eigenvalues ",s.jsx(a,{children:"\\Lambda"}),":"]})," squared semi-axis lengths"]})]})]}),s.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 my-6 border border-primary-500/20",children:[s.jsx("p",{className:"font-semibold text-primary-400 mb-2",children:"Mahalanobis Distance"}),s.jsxs("p",{className:"text-dark-300 text-sm",children:["The ",s.jsx("strong",{children:"Mahalanobis distance"})," measures how far ",s.jsx(a,{children:"\\mathbf{x}"})," is from ",s.jsx(a,{children:"\\boldsymbol{\\mu}"}),", accounting for correlation:"]}),s.jsx(i,{children:"d^2 = (\\mathbf{x} - \\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})"})]}),s.jsx("h2",{children:"Sample Statistics"}),s.jsxs(e,{title:"Sample Mean and Covariance",className:"my-6",children:[s.jsxs("p",{children:["Given data ",s.jsx(a,{children:"\\mathbf{x}_1, \\ldots, \\mathbf{x}_n"}),":"]}),s.jsx(i,{children:"\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i"}),s.jsx(i,{children:"S = \\frac{1}{n-1}\\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^T = \\frac{1}{n-1}X_c^TX_c"}),s.jsxs("p",{className:"mt-2",children:["where ",s.jsx(a,{children:"X_c"})," is the centered data matrix."]})]}),s.jsx("h2",{children:"Key Ideas"}),s.jsx("div",{className:"bg-gradient-to-br from-primary-500/10 to-dark-800/50 rounded-xl p-6 my-6 border border-primary-500/20",children:s.jsxs("ul",{className:"space-y-3 text-dark-200",children:[s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"1."}),s.jsxs("span",{children:["Covariance matrix ",s.jsx(a,{children:"\\Sigma"})," is symmetric positive semidefinite."]})]}),s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"2."}),s.jsxs("span",{children:["Linear transform: ",s.jsx(a,{children:"\\text{Cov}(A\\mathbf{X}) = A\\Sigma A^T"}),"."]})]}),s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"3."}),s.jsx("span",{children:"Multivariate normal: ellipsoid contours, axes from eigenvectors."})]}),s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"4."}),s.jsxs("span",{children:["Mahalanobis distance uses ",s.jsx(a,{children:"\\Sigma^{-1}"})," to account for correlation."]})]}),s.jsxs("li",{className:"flex items-start gap-3",children:[s.jsx("span",{className:"text-primary-400 font-bold",children:"5."}),s.jsxs("span",{children:["Sample covariance: ",s.jsx(a,{children:"S = \\frac{1}{n-1}X_c^TX_c"}),"."]})]})]})})]})}export{j as default};
