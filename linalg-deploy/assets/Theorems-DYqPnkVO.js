import{j as e,A as g,m as y}from"./vendor-animation-8XabJWu9.js";import{r as c,L as d}from"./vendor-react-dnKiRYoA.js";import{H as v,S as x}from"./Sidebar-ByRcBpap.js";import{C as A}from"./index-D4aEnt1z.js";import"./vendor-math-ClxlXyPc.js";import{M as T}from"./MathBlock-ByDekCQ-.js";import"./vendor-firebase-core-DIJkQv9Q.js";const r=[{id:"def-vector",title:"Definition: Vector",statement:"A vector $\\mathbf{v}$ in $\\mathbb{R}^n$ is an ordered list of $n$ real numbers: $\\mathbf{v} = (v_1, v_2, \\ldots, v_n)$.",sectionId:1,sectionTitle:"Vectors and Linear Combinations",category:"Introduction to Vectors",type:"definition"},{id:"def-linear-combination",title:"Definition: Linear Combination",statement:"A linear combination of vectors $\\mathbf{v}_1, \\ldots, \\mathbf{v}_k$ is any sum $c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k$ where $c_1, \\ldots, c_k$ are scalars.",sectionId:1,sectionTitle:"Vectors and Linear Combinations",category:"Introduction to Vectors",type:"definition"},{id:"def-dot-product",title:"Definition: Dot Product",statement:"The dot product of vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$ is $\\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n$.",sectionId:2,sectionTitle:"Lengths and Dot Products",category:"Introduction to Vectors",type:"definition"},{id:"thm-cauchy-schwarz",title:"Cauchy-Schwarz Inequality",statement:"For any vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n$, $|\\mathbf{u} \\cdot \\mathbf{v}| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|$, with equality iff $\\mathbf{u}$ and $\\mathbf{v}$ are parallel.",sectionId:2,sectionTitle:"Lengths and Dot Products",category:"Introduction to Vectors",type:"theorem",hasProof:!0,proof:`
      Consider $\\|\\mathbf{u} - t\\mathbf{v}\\|^2 \\geq 0$ for all $t \\in \\mathbb{R}$.
      Expanding: $\\|\\mathbf{u}\\|^2 - 2t(\\mathbf{u} \\cdot \\mathbf{v}) + t^2\\|\\mathbf{v}\\|^2 \\geq 0$.
      This quadratic in $t$ has at most one root, so its discriminant $\\leq 0$:
      $4(\\mathbf{u} \\cdot \\mathbf{v})^2 - 4\\|\\mathbf{u}\\|^2\\|\\mathbf{v}\\|^2 \\leq 0$.
      Thus $|\\mathbf{u} \\cdot \\mathbf{v}| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|$.
    `},{id:"thm-triangle-inequality",title:"Triangle Inequality",statement:"For any vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n$, $\\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|$.",sectionId:2,sectionTitle:"Lengths and Dot Products",category:"Introduction to Vectors",type:"theorem",hasProof:!0,proof:`
      $\\|\\mathbf{u} + \\mathbf{v}\\|^2 = (\\mathbf{u} + \\mathbf{v}) \\cdot (\\mathbf{u} + \\mathbf{v})$
      $= \\|\\mathbf{u}\\|^2 + 2(\\mathbf{u} \\cdot \\mathbf{v}) + \\|\\mathbf{v}\\|^2$
      $\\leq \\|\\mathbf{u}\\|^2 + 2\\|\\mathbf{u}\\|\\|\\mathbf{v}\\| + \\|\\mathbf{v}\\|^2$ (by Cauchy-Schwarz)
      $= (\\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|)^2$.
      Taking square roots gives the result.
    `},{id:"def-matrix",title:"Definition: Matrix",statement:"An $m \\times n$ matrix $A$ is a rectangular array of numbers with $m$ rows and $n$ columns: $A = (a_{ij})$ where $1 \\leq i \\leq m$ and $1 \\leq j \\leq n$.",sectionId:3,sectionTitle:"Matrices",category:"Introduction to Vectors",type:"definition"},{id:"thm-matrix-vector-mult",title:"Matrix-Vector Multiplication",statement:"The product $A\\mathbf{x}$ of an $m \\times n$ matrix $A$ and vector $\\mathbf{x} \\in \\mathbb{R}^n$ is a linear combination of the columns of $A$ with coefficients from $\\mathbf{x}$.",sectionId:3,sectionTitle:"Matrices",category:"Introduction to Vectors",type:"theorem"},{id:"def-linear-system",title:"Definition: System of Linear Equations",statement:"A system of linear equations is a collection of equations $a_{11}x_1 + \\cdots + a_{1n}x_n = b_1, \\ldots, a_{m1}x_1 + \\cdots + a_{mn}x_n = b_m$, written as $A\\mathbf{x} = \\mathbf{b}$.",sectionId:4,sectionTitle:"Vectors and Linear Equations",category:"Solving Linear Equations",type:"definition"},{id:"thm-row-column-picture",title:"Row and Column Pictures",statement:"The system $A\\mathbf{x} = \\mathbf{b}$ can be viewed as: (1) Row picture: intersection of hyperplanes; (2) Column picture: linear combination of column vectors equaling $\\mathbf{b}$.",sectionId:4,sectionTitle:"Vectors and Linear Equations",category:"Solving Linear Equations",type:"theorem"},{id:"thm-gaussian-elimination",title:"Gaussian Elimination",statement:"Any matrix can be reduced to row echelon form by a sequence of elementary row operations: (1) swapping rows, (2) multiplying a row by a nonzero scalar, (3) adding a multiple of one row to another.",sectionId:5,sectionTitle:"The Idea of Elimination",category:"Solving Linear Equations",type:"theorem",hasProof:!0,proof:`
      Work column by column from left to right:
      1. Find a pivot (nonzero entry) in the current column
      2. If necessary, swap rows to move the pivot up
      3. Use the pivot to eliminate all entries below it
      4. Move to the next column and repeat
      The process terminates with an upper triangular (echelon) form.
    `},{id:"def-pivot",title:"Definition: Pivot",statement:"A pivot is the first nonzero entry in a row of a matrix in echelon form. The pivot positions determine the structure of the solution space.",sectionId:5,sectionTitle:"The Idea of Elimination",category:"Solving Linear Equations",type:"definition"},{id:"def-elementary-matrix",title:"Definition: Elementary Matrix",statement:"An elementary matrix $E$ is the result of performing a single elementary row operation on the identity matrix $I$. Multiplying $EA$ performs that operation on $A$.",sectionId:6,sectionTitle:"Elimination Using Matrices",category:"Solving Linear Equations",type:"definition"},{id:"thm-matrix-mult-associative",title:"Matrix Multiplication is Associative",statement:"For matrices $A$, $B$, $C$ of compatible sizes, $(AB)C = A(BC)$.",sectionId:7,sectionTitle:"Rules for Matrix Operations",category:"Solving Linear Equations",type:"theorem"},{id:"thm-matrix-mult-not-commutative",title:"Matrix Multiplication is Not Commutative",statement:"In general, $AB \\neq BA$ even when both products are defined.",sectionId:7,sectionTitle:"Rules for Matrix Operations",category:"Solving Linear Equations",type:"theorem"},{id:"def-inverse-matrix",title:"Definition: Inverse Matrix",statement:"A square matrix $A$ is invertible if there exists a matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$.",sectionId:8,sectionTitle:"Inverse Matrices",category:"Solving Linear Equations",type:"definition"},{id:"thm-inverse-unique",title:"Uniqueness of Inverse",statement:"If $A$ is invertible, its inverse is unique.",sectionId:8,sectionTitle:"Inverse Matrices",category:"Solving Linear Equations",type:"theorem",hasProof:!0,proof:`
      Suppose $B$ and $C$ are both inverses of $A$.
      Then $B = BI = B(AC) = (BA)C = IC = C$.
    `},{id:"thm-inverse-product",title:"Inverse of a Product",statement:"If $A$ and $B$ are invertible matrices of the same size, then $(AB)^{-1} = B^{-1}A^{-1}$.",sectionId:8,sectionTitle:"Inverse Matrices",category:"Solving Linear Equations",type:"theorem",hasProof:!0,proof:`
      $(AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AIA^{-1} = AA^{-1} = I$.
      Similarly $(B^{-1}A^{-1})(AB) = I$.
    `},{id:"thm-lu-factorization",title:"LU Factorization",statement:"If elimination on $A$ requires no row exchanges, then $A = LU$ where $L$ is lower triangular with 1s on the diagonal (unit lower triangular) and $U$ is upper triangular.",sectionId:9,sectionTitle:"Elimination = Factorization: A = LU",category:"Solving Linear Equations",type:"theorem"},{id:"thm-plu-factorization",title:"PA = LU Factorization",statement:"For any invertible matrix $A$, there exists a permutation matrix $P$ such that $PA = LU$.",sectionId:9,sectionTitle:"Elimination = Factorization: A = LU",category:"Solving Linear Equations",type:"theorem"},{id:"def-transpose",title:"Definition: Transpose",statement:"The transpose of $A$, denoted $A^T$, is the matrix obtained by exchanging rows and columns: $(A^T)_{ij} = A_{ji}$.",sectionId:10,sectionTitle:"Transposes and Permutations",category:"Solving Linear Equations",type:"definition"},{id:"thm-transpose-properties",title:"Properties of Transpose",statement:"(1) $(A^T)^T = A$; (2) $(A + B)^T = A^T + B^T$; (3) $(cA)^T = cA^T$; (4) $(AB)^T = B^T A^T$.",sectionId:10,sectionTitle:"Transposes and Permutations",category:"Solving Linear Equations",type:"theorem"},{id:"def-symmetric",title:"Definition: Symmetric Matrix",statement:"A matrix $A$ is symmetric if $A^T = A$, meaning $a_{ij} = a_{ji}$ for all $i, j$.",sectionId:10,sectionTitle:"Transposes and Permutations",category:"Solving Linear Equations",type:"definition"},{id:"def-vector-space",title:"Definition: Vector Space",statement:"A vector space $V$ over $\\mathbb{R}$ is a set with operations of addition and scalar multiplication satisfying: closure, commutativity, associativity, existence of zero vector, existence of additive inverses, and distributive properties.",sectionId:11,sectionTitle:"Spaces of Vectors",category:"Vector Spaces",type:"definition"},{id:"def-subspace",title:"Definition: Subspace",statement:"A subspace of a vector space $V$ is a nonempty subset $S \\subseteq V$ that is closed under addition and scalar multiplication.",sectionId:11,sectionTitle:"Spaces of Vectors",category:"Vector Spaces",type:"definition"},{id:"thm-subspace-test",title:"Subspace Test",statement:"A subset $S$ of a vector space $V$ is a subspace iff: (1) $\\mathbf{0} \\in S$, (2) $\\mathbf{u} + \\mathbf{v} \\in S$ for all $\\mathbf{u}, \\mathbf{v} \\in S$, (3) $c\\mathbf{u} \\in S$ for all $\\mathbf{u} \\in S$ and scalars $c$.",sectionId:11,sectionTitle:"Spaces of Vectors",category:"Vector Spaces",type:"theorem"},{id:"def-column-space",title:"Definition: Column Space",statement:"The column space $C(A)$ of matrix $A$ is the set of all linear combinations of the columns of $A$. It is the range of the transformation $\\mathbf{x} \\mapsto A\\mathbf{x}$.",sectionId:11,sectionTitle:"Spaces of Vectors",category:"Vector Spaces",type:"definition"},{id:"def-nullspace",title:"Definition: Nullspace",statement:"The nullspace $N(A)$ of matrix $A$ is the set of all solutions to $A\\mathbf{x} = \\mathbf{0}$: $N(A) = \\{\\mathbf{x} : A\\mathbf{x} = \\mathbf{0}\\}$.",sectionId:12,sectionTitle:"The Nullspace of A",category:"Vector Spaces",type:"definition"},{id:"thm-nullspace-subspace",title:"Nullspace is a Subspace",statement:"The nullspace $N(A)$ is a subspace of $\\mathbb{R}^n$ (where $A$ is $m \\times n$).",sectionId:12,sectionTitle:"The Nullspace of A",category:"Vector Spaces",type:"theorem",hasProof:!0,proof:`
      1. $\\mathbf{0}$ is in $N(A)$ since $A\\mathbf{0} = \\mathbf{0}$.
      2. If $\\mathbf{u}, \\mathbf{v} \\in N(A)$, then $A(\\mathbf{u} + \\mathbf{v}) = A\\mathbf{u} + A\\mathbf{v} = \\mathbf{0}$.
      3. If $\\mathbf{u} \\in N(A)$, then $A(c\\mathbf{u}) = cA\\mathbf{u} = c\\mathbf{0} = \\mathbf{0}$.
    `},{id:"thm-complete-solution",title:"Complete Solution to Ax = b",statement:"The complete solution to $A\\mathbf{x} = \\mathbf{b}$ is $\\mathbf{x} = \\mathbf{x}_p + \\mathbf{x}_n$ where $\\mathbf{x}_p$ is any particular solution and $\\mathbf{x}_n$ ranges over all vectors in $N(A)$.",sectionId:13,sectionTitle:"The Complete Solution to Ax = b",category:"Vector Spaces",type:"theorem"},{id:"def-linear-independence",title:"Definition: Linear Independence",statement:"Vectors $\\mathbf{v}_1, \\ldots, \\mathbf{v}_k$ are linearly independent if $c_1\\mathbf{v}_1 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}$ implies all $c_i = 0$.",sectionId:14,sectionTitle:"Independence, Basis and Dimension",category:"Vector Spaces",type:"definition"},{id:"def-span",title:"Definition: Span",statement:"The span of vectors $\\mathbf{v}_1, \\ldots, \\mathbf{v}_k$ is the set of all linear combinations: $\\text{span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\} = \\{c_1\\mathbf{v}_1 + \\cdots + c_k\\mathbf{v}_k : c_i \\in \\mathbb{R}\\}$.",sectionId:14,sectionTitle:"Independence, Basis and Dimension",category:"Vector Spaces",type:"definition"},{id:"def-basis",title:"Definition: Basis",statement:"A basis for a vector space $V$ is a set of vectors that is both linearly independent and spans $V$.",sectionId:14,sectionTitle:"Independence, Basis and Dimension",category:"Vector Spaces",type:"definition"},{id:"thm-basis-same-size",title:"All Bases Have Same Size",statement:"Any two bases for a finite-dimensional vector space $V$ have the same number of vectors.",sectionId:14,sectionTitle:"Independence, Basis and Dimension",category:"Vector Spaces",type:"theorem"},{id:"def-dimension",title:"Definition: Dimension",statement:"The dimension of a vector space $V$, written $\\dim(V)$, is the number of vectors in any basis for $V$.",sectionId:14,sectionTitle:"Independence, Basis and Dimension",category:"Vector Spaces",type:"definition"},{id:"def-rank",title:"Definition: Rank",statement:"The rank of a matrix $A$, written $\\text{rank}(A)$ or $r$, is the dimension of its column space $C(A)$, which equals the number of pivot columns.",sectionId:15,sectionTitle:"Dimensions of the Four Subspaces",category:"Vector Spaces",type:"definition"},{id:"thm-rank-nullity",title:"Rank-Nullity Theorem (Dimension Theorem)",statement:"For an $m \\times n$ matrix $A$ with rank $r$: $\\dim(C(A)) + \\dim(N(A)) = n$, or equivalently, $r + (n - r) = n$.",sectionId:15,sectionTitle:"Dimensions of the Four Subspaces",category:"Vector Spaces",type:"theorem",hasProof:!0,proof:`
      The rank $r$ equals the number of pivot columns.
      The nullity (dimension of $N(A)$) equals the number of free variables, which is $n - r$.
      Thus $r + (n - r) = n$.
    `},{id:"thm-four-subspaces",title:"The Four Fundamental Subspaces",statement:"For an $m \\times n$ matrix $A$ with rank $r$: (1) $C(A) \\subseteq \\mathbb{R}^m$ has dim $r$; (2) $N(A) \\subseteq \\mathbb{R}^n$ has dim $n-r$; (3) $C(A^T) \\subseteq \\mathbb{R}^n$ has dim $r$; (4) $N(A^T) \\subseteq \\mathbb{R}^m$ has dim $m-r$.",sectionId:15,sectionTitle:"Dimensions of the Four Subspaces",category:"Vector Spaces",type:"theorem"},{id:"def-orthogonal-vectors",title:"Definition: Orthogonal Vectors",statement:"Vectors $\\mathbf{u}$ and $\\mathbf{v}$ are orthogonal (perpendicular) if $\\mathbf{u} \\cdot \\mathbf{v} = 0$.",sectionId:16,sectionTitle:"Orthogonality of the Four Subspaces",category:"Orthogonality",type:"definition"},{id:"def-orthogonal-complement",title:"Definition: Orthogonal Complement",statement:"The orthogonal complement $V^\\perp$ of subspace $V$ is the set of all vectors orthogonal to every vector in $V$: $V^\\perp = \\{\\mathbf{w} : \\mathbf{w} \\cdot \\mathbf{v} = 0 \\text{ for all } \\mathbf{v} \\in V\\}$.",sectionId:16,sectionTitle:"Orthogonality of the Four Subspaces",category:"Orthogonality",type:"definition"},{id:"thm-fundamental-subspaces-orthogonal",title:"Orthogonality of Fundamental Subspaces",statement:"The row space $C(A^T)$ and nullspace $N(A)$ are orthogonal complements in $\\mathbb{R}^n$. The column space $C(A)$ and left nullspace $N(A^T)$ are orthogonal complements in $\\mathbb{R}^m$.",sectionId:16,sectionTitle:"Orthogonality of the Four Subspaces",category:"Orthogonality",type:"theorem"},{id:"thm-projection-formula",title:"Projection onto a Line",statement:"The projection of $\\mathbf{b}$ onto the line through $\\mathbf{a}$ is $\\mathbf{p} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{a} \\cdot \\mathbf{a}}\\mathbf{a} = \\frac{\\mathbf{a}\\mathbf{a}^T}{\\mathbf{a}^T\\mathbf{a}}\\mathbf{b}$.",sectionId:17,sectionTitle:"Projections",category:"Orthogonality",type:"theorem"},{id:"thm-projection-matrix",title:"Projection Matrix",statement:"The projection onto the column space of $A$ is $P = A(A^TA)^{-1}A^T$. The projection matrix satisfies $P^2 = P$ (idempotent) and $P^T = P$ (symmetric).",sectionId:17,sectionTitle:"Projections",category:"Orthogonality",type:"theorem"},{id:"thm-least-squares",title:"Least Squares Solution",statement:"The least squares solution to $A\\mathbf{x} = \\mathbf{b}$ minimizes $\\|A\\mathbf{x} - \\mathbf{b}\\|^2$ and satisfies the normal equations $A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}$.",sectionId:18,sectionTitle:"Least Squares Approximations",category:"Orthogonality",type:"theorem",hasProof:!0,proof:`
      The error $\\mathbf{e} = \\mathbf{b} - A\\hat{\\mathbf{x}}$ is minimized when $\\mathbf{e}$ is perpendicular to $C(A)$.
      This means $A^T\\mathbf{e} = \\mathbf{0}$, so $A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = \\mathbf{0}$.
      Rearranging: $A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}$.
    `},{id:"def-orthonormal",title:"Definition: Orthonormal Vectors",statement:"Vectors $\\mathbf{q}_1, \\ldots, \\mathbf{q}_n$ are orthonormal if they are pairwise orthogonal and each has unit length: $\\mathbf{q}_i \\cdot \\mathbf{q}_j = \\delta_{ij}$ (1 if $i = j$, 0 otherwise).",sectionId:19,sectionTitle:"Orthonormal Bases and Gram-Schmidt",category:"Orthogonality",type:"definition"},{id:"def-orthogonal-matrix",title:"Definition: Orthogonal Matrix",statement:"A square matrix $Q$ is orthogonal if its columns are orthonormal vectors, which is equivalent to $Q^TQ = QQ^T = I$, or $Q^{-1} = Q^T$.",sectionId:19,sectionTitle:"Orthonormal Bases and Gram-Schmidt",category:"Orthogonality",type:"definition"},{id:"thm-gram-schmidt",title:"Gram-Schmidt Process",statement:"Given linearly independent vectors $\\mathbf{a}_1, \\ldots, \\mathbf{a}_n$, the Gram-Schmidt process produces orthonormal vectors $\\mathbf{q}_1, \\ldots, \\mathbf{q}_n$ spanning the same subspace.",sectionId:19,sectionTitle:"Orthonormal Bases and Gram-Schmidt",category:"Orthogonality",type:"theorem"},{id:"thm-qr-factorization",title:"QR Factorization",statement:"If $A$ has linearly independent columns, then $A = QR$ where $Q$ has orthonormal columns and $R$ is upper triangular with positive diagonal entries.",sectionId:19,sectionTitle:"Orthonormal Bases and Gram-Schmidt",category:"Orthogonality",type:"theorem"},{id:"def-determinant",title:"Definition: Determinant",statement:"The determinant $\\det(A)$ or $|A|$ is a scalar associated with a square matrix, uniquely defined by: (1) $\\det(I) = 1$, (2) row exchange reverses sign, (3) determinant is linear in each row separately.",sectionId:20,sectionTitle:"The Properties of Determinants",category:"Determinants",type:"definition"},{id:"thm-det-properties",title:"Properties of Determinants",statement:"Key properties: (1) $\\det(A^T) = \\det(A)$; (2) $\\det(AB) = \\det(A)\\det(B)$; (3) $\\det(A^{-1}) = 1/\\det(A)$; (4) $\\det(cA) = c^n\\det(A)$ for $n \\times n$ matrix.",sectionId:20,sectionTitle:"The Properties of Determinants",category:"Determinants",type:"theorem"},{id:"thm-invertible-det",title:"Invertibility and Determinant",statement:"A square matrix $A$ is invertible if and only if $\\det(A) \\neq 0$.",sectionId:20,sectionTitle:"The Properties of Determinants",category:"Determinants",type:"theorem",hasProof:!0,proof:`
      If $A$ is invertible: $\\det(A)\\det(A^{-1}) = \\det(AA^{-1}) = \\det(I) = 1$, so $\\det(A) \\neq 0$.
      If $\\det(A) \\neq 0$: Elimination produces no zero pivots, so $A$ is invertible.
    `},{id:"def-cofactor",title:"Definition: Cofactor",statement:"The cofactor $C_{ij}$ of entry $a_{ij}$ is $(-1)^{i+j}$ times the determinant of the $(n-1) \\times (n-1)$ matrix obtained by deleting row $i$ and column $j$.",sectionId:21,sectionTitle:"Permutations and Cofactors",category:"Determinants",type:"definition"},{id:"thm-cofactor-expansion",title:"Cofactor Expansion",statement:"The determinant can be computed by cofactor expansion along any row $i$: $\\det(A) = a_{i1}C_{i1} + a_{i2}C_{i2} + \\cdots + a_{in}C_{in}$, or along any column.",sectionId:21,sectionTitle:"Permutations and Cofactors",category:"Determinants",type:"theorem"},{id:"thm-cramers-rule",title:"Cramer's Rule",statement:"If $\\det(A) \\neq 0$, the solution to $A\\mathbf{x} = \\mathbf{b}$ is $x_i = \\det(B_i)/\\det(A)$ where $B_i$ is $A$ with column $i$ replaced by $\\mathbf{b}$.",sectionId:22,sectionTitle:"Cramer's Rule, Inverses, and Volumes",category:"Determinants",type:"theorem"},{id:"thm-inverse-adjugate",title:"Inverse via Cofactors",statement:"$A^{-1} = \\frac{1}{\\det(A)}C^T$ where $C$ is the matrix of cofactors (the adjugate of $A$).",sectionId:22,sectionTitle:"Cramer's Rule, Inverses, and Volumes",category:"Determinants",type:"theorem"},{id:"thm-det-volume",title:"Determinant as Volume",statement:"$|\\det(A)|$ equals the volume of the parallelepiped formed by the rows (or columns) of $A$.",sectionId:22,sectionTitle:"Cramer's Rule, Inverses, and Volumes",category:"Determinants",type:"theorem"},{id:"def-eigenvalue-eigenvector",title:"Definition: Eigenvalue and Eigenvector",statement:"A scalar $\\lambda$ is an eigenvalue of $A$ if there exists a nonzero vector $\\mathbf{x}$ such that $A\\mathbf{x} = \\lambda\\mathbf{x}$. The vector $\\mathbf{x}$ is called an eigenvector.",sectionId:23,sectionTitle:"Introduction to Eigenvalues",category:"Eigenvalues and Eigenvectors",type:"definition"},{id:"thm-characteristic-polynomial",title:"Characteristic Polynomial",statement:"The eigenvalues of $A$ are the roots of the characteristic polynomial $\\det(A - \\lambda I) = 0$.",sectionId:23,sectionTitle:"Introduction to Eigenvalues",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-trace-det-eigenvalues",title:"Trace and Determinant from Eigenvalues",statement:"For an $n \\times n$ matrix: (1) $\\text{trace}(A) = \\sum \\lambda_i$ (sum of eigenvalues), (2) $\\det(A) = \\prod \\lambda_i$ (product of eigenvalues).",sectionId:23,sectionTitle:"Introduction to Eigenvalues",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"def-diagonalizable",title:"Definition: Diagonalizable",statement:"A matrix $A$ is diagonalizable if there exists an invertible matrix $S$ such that $S^{-1}AS = D$ where $D$ is diagonal. Equivalently, $A = SDS^{-1}$.",sectionId:24,sectionTitle:"Diagonalizing a Matrix",category:"Eigenvalues and Eigenvectors",type:"definition"},{id:"thm-diagonalization",title:"Diagonalization Theorem",statement:"An $n \\times n$ matrix $A$ is diagonalizable iff $A$ has $n$ linearly independent eigenvectors. The columns of $S$ are the eigenvectors; $D$ has the eigenvalues on the diagonal.",sectionId:24,sectionTitle:"Diagonalizing a Matrix",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-distinct-eigenvalues",title:"Distinct Eigenvalues",statement:"If $A$ has $n$ distinct eigenvalues, then $A$ is diagonalizable.",sectionId:24,sectionTitle:"Diagonalizing a Matrix",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-matrix-power",title:"Powers via Diagonalization",statement:"If $A = SDS^{-1}$, then $A^k = SD^kS^{-1}$. This makes computing high powers efficient since $D^k$ is trivial.",sectionId:24,sectionTitle:"Diagonalizing a Matrix",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-diff-eq-solution",title:"Solution to du/dt = Au",statement:"The solution to $d\\mathbf{u}/dt = A\\mathbf{u}$ with $\\mathbf{u}(0) = \\mathbf{u}_0$ is $\\mathbf{u}(t) = e^{At}\\mathbf{u}_0 = \\sum c_i e^{\\lambda_i t}\\mathbf{x}_i$ where $\\lambda_i, \\mathbf{x}_i$ are eigenvalue-eigenvector pairs.",sectionId:25,sectionTitle:"Systems of Differential Equations",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-spectral-theorem",title:"Spectral Theorem for Symmetric Matrices",statement:"A real symmetric matrix $A$ has all real eigenvalues, and eigenvectors from different eigenspaces are orthogonal. $A$ can be diagonalized as $A = Q\\Lambda Q^T$ with orthogonal $Q$.",sectionId:26,sectionTitle:"Symmetric Matrices",category:"Eigenvalues and Eigenvectors",type:"theorem",hasProof:!0,proof:`
      Real eigenvalues: For $A\\mathbf{x} = \\lambda\\mathbf{x}$ with $A = A^T$,
      $\\bar{\\mathbf{x}}^TA\\mathbf{x} = \\lambda\\|\\mathbf{x}\\|^2$ and
      $\\bar{\\mathbf{x}}^TA^T\\mathbf{x} = \\bar{\\mathbf{x}}^TA\\mathbf{x} = \\bar{\\lambda}\\|\\mathbf{x}\\|^2$.
      Thus $\\lambda = \\bar{\\lambda}$, so $\\lambda$ is real.

      Orthogonal eigenvectors: If $A\\mathbf{x} = \\lambda\\mathbf{x}$ and $A\\mathbf{y} = \\mu\\mathbf{y}$ with $\\lambda \\neq \\mu$,
      then $\\lambda(\\mathbf{x}\\cdot\\mathbf{y}) = (A\\mathbf{x})\\cdot\\mathbf{y} = \\mathbf{x}\\cdot(A\\mathbf{y}) = \\mu(\\mathbf{x}\\cdot\\mathbf{y})$.
      Since $\\lambda \\neq \\mu$, we have $\\mathbf{x}\\cdot\\mathbf{y} = 0$.
    `},{id:"def-positive-definite",title:"Definition: Positive Definite Matrix",statement:"A symmetric matrix $A$ is positive definite if $\\mathbf{x}^TA\\mathbf{x} > 0$ for all nonzero vectors $\\mathbf{x}$.",sectionId:27,sectionTitle:"Positive Definite Matrices",category:"Eigenvalues and Eigenvectors",type:"definition"},{id:"thm-positive-definite-tests",title:"Tests for Positive Definiteness",statement:"A symmetric matrix $A$ is positive definite iff: (1) all eigenvalues are positive, (2) all upper-left determinants are positive, (3) all pivots are positive, (4) $A = R^TR$ for some matrix $R$ with independent columns.",sectionId:27,sectionTitle:"Positive Definite Matrices",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-svd",title:"Singular Value Decomposition (SVD)",statement:"Every $m \\times n$ matrix $A$ can be factored as $A = U\\Sigma V^T$ where $U$ is $m \\times m$ orthogonal, $V$ is $n \\times n$ orthogonal, and $\\Sigma$ is $m \\times n$ diagonal with nonnegative entries $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq 0$.",sectionId:29,sectionTitle:"Bases and Matrices in the SVD",category:"SVD",type:"theorem"},{id:"def-singular-values",title:"Definition: Singular Values",statement:"The singular values $\\sigma_1, \\ldots, \\sigma_r$ of $A$ are the square roots of the nonzero eigenvalues of $A^TA$ (or equivalently, of $AA^T$).",sectionId:29,sectionTitle:"Bases and Matrices in the SVD",category:"SVD",type:"definition"},{id:"thm-svd-geometry",title:"Geometric Interpretation of SVD",statement:"The SVD shows that every linear transformation is a rotation ($V^T$), followed by a scaling ($\\Sigma$), followed by another rotation ($U$).",sectionId:31,sectionTitle:"The Geometry of the SVD",category:"SVD",type:"theorem"},{id:"thm-low-rank-approximation",title:"Best Low-Rank Approximation",statement:"The best rank-$k$ approximation to $A$ (minimizing $\\|A - B\\|$ over all rank-$k$ matrices $B$) is $A_k = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$.",sectionId:30,sectionTitle:"Principal Component Analysis (PCA)",category:"SVD",type:"theorem"},{id:"def-pca",title:"Definition: Principal Component Analysis",statement:"PCA finds the directions of maximum variance in data. The first principal component is $\\mathbf{v}_1$ from the SVD of the centered data matrix; the $k$th component is $\\mathbf{v}_k$.",sectionId:30,sectionTitle:"Principal Component Analysis (PCA)",category:"SVD",type:"definition"},{id:"def-linear-transformation",title:"Definition: Linear Transformation",statement:"A transformation $T: V \\to W$ is linear if $T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$ and $T(c\\mathbf{v}) = cT(\\mathbf{v})$ for all vectors and scalars.",sectionId:32,sectionTitle:"The Idea of a Linear Transformation",category:"Linear Transformations",type:"definition"},{id:"thm-transformation-matrix",title:"Matrix of a Linear Transformation",statement:"Every linear transformation $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ can be represented by an $m \\times n$ matrix $A$. The $j$th column of $A$ is $T(\\mathbf{e}_j)$.",sectionId:33,sectionTitle:"The Matrix of a Linear Transformation",category:"Linear Transformations",type:"theorem"},{id:"def-kernel-range",title:"Definition: Kernel and Range",statement:"The kernel (nullspace) of $T$ is $\\ker(T) = \\{\\mathbf{v} : T(\\mathbf{v}) = \\mathbf{0}\\}$. The range (image) of $T$ is $\\text{range}(T) = \\{T(\\mathbf{v}) : \\mathbf{v} \\in V\\}$.",sectionId:32,sectionTitle:"The Idea of a Linear Transformation",category:"Linear Transformations",type:"definition"},{id:"def-similar-matrices",title:"Definition: Similar Matrices",statement:"Matrices $A$ and $B$ are similar if $B = M^{-1}AM$ for some invertible matrix $M$. Similar matrices represent the same transformation in different bases.",sectionId:34,sectionTitle:"The Search for a Good Basis",category:"Linear Transformations",type:"definition"},{id:"thm-similar-eigenvalues",title:"Similar Matrices Have Same Eigenvalues",statement:"Similar matrices have the same eigenvalues (with the same algebraic multiplicities).",sectionId:34,sectionTitle:"The Search for a Good Basis",category:"Linear Transformations",type:"theorem"},{id:"def-complex-inner-product",title:"Definition: Complex Inner Product",statement:"For complex vectors, the inner product is $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\bar{\\mathbf{u}}^T \\mathbf{v} = \\sum \\bar{u}_i v_i$. Note the conjugate on the first argument.",sectionId:35,sectionTitle:"Complex Numbers",category:"Complex Vectors",type:"definition"},{id:"def-hermitian",title:"Definition: Hermitian Matrix",statement:"A complex matrix $A$ is Hermitian if $A^H = A$, where $A^H = \\bar{A}^T$ is the conjugate transpose. Hermitian matrices are the complex analogue of symmetric matrices.",sectionId:36,sectionTitle:"Hermitian and Unitary Matrices",category:"Complex Vectors",type:"definition"},{id:"def-unitary",title:"Definition: Unitary Matrix",statement:"A complex matrix $U$ is unitary if $U^HU = UU^H = I$, i.e., $U^{-1} = U^H$. Unitary matrices are the complex analogue of orthogonal matrices.",sectionId:36,sectionTitle:"Hermitian and Unitary Matrices",category:"Complex Vectors",type:"definition"},{id:"thm-hermitian-eigenvalues",title:"Eigenvalues of Hermitian Matrices",statement:"A Hermitian matrix has all real eigenvalues and can be diagonalized by a unitary matrix: $A = U\\Lambda U^H$.",sectionId:36,sectionTitle:"Hermitian and Unitary Matrices",category:"Complex Vectors",type:"theorem"},{id:"def-fourier-matrix",title:"Definition: Fourier Matrix",statement:"The $n \\times n$ Fourier matrix $F_n$ has entries $(F_n)_{jk} = \\omega^{jk}$ where $\\omega = e^{2\\pi i/n}$. It satisfies $F_n^HF_n = nI$, so $F_n/\\sqrt{n}$ is unitary.",sectionId:37,sectionTitle:"The Fast Fourier Transform",category:"Complex Vectors",type:"definition"},{id:"thm-fft",title:"Fast Fourier Transform",statement:"The FFT computes $F_n\\mathbf{x}$ in $O(n \\log n)$ operations instead of $O(n^2)$ by recursively factoring $F_n$ using $F_{n/2}$.",sectionId:37,sectionTitle:"The Fast Fourier Transform",category:"Complex Vectors",type:"theorem"},{id:"def-incidence-matrix",title:"Definition: Incidence Matrix",statement:"For a graph with $n$ nodes and $m$ edges, the incidence matrix $A$ is $m \\times n$ with $A_{ij} = -1$ if edge $i$ leaves node $j$, $+1$ if it enters, and $0$ otherwise.",sectionId:38,sectionTitle:"Graphs and Networks",category:"Applications",type:"definition"},{id:"thm-kirchhoff",title:"Kirchhoff's Laws",statement:"Current law: $A^T\\mathbf{y} = \\mathbf{0}$ (currents balance at each node). Voltage law: $A\\mathbf{x} = \\mathbf{e}$ (potential differences around loops).",sectionId:38,sectionTitle:"Graphs and Networks",category:"Applications",type:"theorem"},{id:"def-markov-matrix",title:"Definition: Markov Matrix",statement:"A Markov (stochastic) matrix has nonnegative entries with each column summing to 1. It represents transition probabilities between states.",sectionId:40,sectionTitle:"Markov Matrices, Population, and Economics",category:"Applications",type:"definition"},{id:"thm-markov-steady-state",title:"Markov Steady State",statement:"A Markov matrix $A$ has eigenvalue $\\lambda = 1$. If $A$ is positive (all entries $> 0$), then $A^k\\mathbf{u}_0 \\to \\mathbf{u}_{\\infty}$ where $A\\mathbf{u}_{\\infty} = \\mathbf{u}_{\\infty}$ (steady state).",sectionId:40,sectionTitle:"Markov Matrices, Population, and Economics",category:"Applications",type:"theorem"},{id:"thm-fourier-series",title:"Fourier Series Expansion",statement:"A periodic function $f(x)$ can be written as $f(x) = a_0 + \\sum (a_n \\cos nx + b_n \\sin nx)$. The sines and cosines form an orthogonal basis in function space.",sectionId:42,sectionTitle:"Fourier Series: Linear Algebra for Functions",category:"Applications",type:"theorem"},{id:"thm-graphics-transformations",title:"Graphics Transformations",statement:"Rotation by angle $\\theta$: $\\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}$. Reflection, scaling, and shearing are also linear and represented by matrices.",sectionId:43,sectionTitle:"Computer Graphics",category:"Applications",type:"theorem"},{id:"def-condition-number",title:"Definition: Condition Number",statement:"The condition number $\\kappa(A) = \\|A\\| \\|A^{-1}\\| = \\sigma_{\\max}/\\sigma_{\\min}$ measures sensitivity to errors. Large $\\kappa$ means $A$ is ill-conditioned.",sectionId:46,sectionTitle:"Norms and Condition Numbers",category:"Numerical Linear Algebra",type:"definition"},{id:"thm-condition-error",title:"Error Bound from Condition Number",statement:"For $A\\mathbf{x} = \\mathbf{b}$ with perturbation $\\delta\\mathbf{b}$, the relative error satisfies $\\frac{\\|\\delta\\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\leq \\kappa(A) \\frac{\\|\\delta\\mathbf{b}\\|}{\\|\\mathbf{b}\\|}$.",sectionId:46,sectionTitle:"Norms and Condition Numbers",category:"Numerical Linear Algebra",type:"theorem"},{id:"thm-iterative-convergence",title:"Iterative Method Convergence",statement:"The iteration $\\mathbf{x}_{k+1} = M\\mathbf{x}_k + \\mathbf{c}$ converges iff all eigenvalues of $M$ satisfy $|\\lambda| < 1$ (spectral radius $\\rho(M) < 1$).",sectionId:47,sectionTitle:"Iterative Methods and Preconditioners",category:"Numerical Linear Algebra",type:"theorem"},{id:"def-covariance-matrix",title:"Definition: Covariance Matrix",statement:"For random vector $\\mathbf{X}$, the covariance matrix $\\Sigma$ has entries $\\Sigma_{ij} = \\text{Cov}(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)]$. It is symmetric and positive semidefinite.",sectionId:49,sectionTitle:"Covariance Matrices and Joint Probabilities",category:"Probability and Statistics",type:"definition"},{id:"thm-sample-covariance",title:"Sample Covariance Matrix",statement:"For data matrix $X$ (samples in rows, centered), the sample covariance matrix is $S = \\frac{1}{n-1}X^TX$.",sectionId:49,sectionTitle:"Covariance Matrices and Joint Probabilities",category:"Probability and Statistics",type:"theorem"},{id:"thm-multivariate-gaussian",title:"Multivariate Gaussian Distribution",statement:"The multivariate Gaussian with mean $\\boldsymbol{\\mu}$ and covariance $\\Sigma$ has density $f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)$.",sectionId:50,sectionTitle:"Multivariate Gaussian and Weighted Least Squares",category:"Probability and Statistics",type:"theorem"},{id:"thm-weighted-least-squares",title:"Weighted Least Squares",statement:"When errors have covariance $\\Sigma$, the weighted least squares solution minimizes $(A\\mathbf{x} - \\mathbf{b})^T\\Sigma^{-1}(A\\mathbf{x} - \\mathbf{b})$ and satisfies $(A^T\\Sigma^{-1}A)\\hat{\\mathbf{x}} = A^T\\Sigma^{-1}\\mathbf{b}$.",sectionId:50,sectionTitle:"Multivariate Gaussian and Weighted Least Squares",category:"Probability and Statistics",type:"theorem"}];function I(){return[...new Set(r.map(a=>a.category).filter(a=>!!a))]}function S(a){const o=a.toLowerCase();return r.filter(n=>n.title.toLowerCase().includes(o)||n.statement.toLowerCase().includes(o))}function j(){const[a,o]=c.useState(!1),[n,h]=c.useState(""),[m,$]=c.useState(null),l=n?S(n):r,f=n?[{category:"Search Results",items:l}]:I().map(i=>({category:i,items:r.filter(s=>s.category===i)})),b=i=>{$(m===i?null:i)},u={theorem:"text-amber-400 bg-amber-500/10",definition:"text-blue-400 bg-blue-500/10",lemma:"text-purple-400 bg-purple-500/10",corollary:"text-green-400 bg-green-500/10",proposition:"text-cyan-400 bg-cyan-500/10"};return e.jsxs("div",{className:"min-h-screen bg-dark-950",children:[e.jsx(v,{onToggleSidebar:()=>o(!a),sidebarOpen:a}),e.jsx(x,{isOpen:a,onClose:()=>o(!1)}),e.jsx("main",{className:"pt-20 pb-12 px-4 lg:pl-80 lg:pr-8",children:e.jsxs("div",{className:"max-w-4xl mx-auto",children:[e.jsx("h1",{className:"text-3xl font-bold text-dark-100 mb-2",children:"Theorems & Definitions"}),e.jsxs("p",{className:"text-dark-400 mb-8",children:["Quick reference for all theorems and definitions in ",A]}),e.jsx("div",{className:"mb-8",children:e.jsx("input",{type:"text",placeholder:"Search theorems and definitions...",value:n,onChange:i=>h(i.target.value),className:"w-full px-4 py-3 rounded-xl bg-dark-800 border border-dark-700 text-dark-100 placeholder-dark-500 focus:outline-none focus:border-primary-500 transition-colors"})}),e.jsx("div",{className:"bg-gradient-to-br from-amber-500/10 to-dark-800/50 border border-amber-500/20 rounded-2xl p-4 mb-8",children:e.jsxs("p",{className:"text-amber-300 text-sm",children:[e.jsx("span",{className:"font-semibold",children:"Tip:"})," Click any theorem to go to its section. Many include expandable proofs with LaTeX!"]})}),e.jsx("div",{className:"space-y-8",children:f.map(({category:i,items:s})=>e.jsxs("div",{className:"space-y-3",children:[e.jsxs("h2",{className:"text-xl font-bold text-dark-200 border-b border-dark-700/50 pb-2 flex items-center gap-3",children:[e.jsx("span",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-amber-500/20 to-orange-500/10 flex items-center justify-center text-sm font-bold text-amber-400",children:s.length}),i]}),e.jsx("div",{className:"space-y-2",children:s.map(t=>e.jsx(d,{to:`/section/${t.sectionId}`,className:"block group",children:e.jsxs("div",{className:"relative overflow-hidden rounded-xl bg-dark-800/40 border border-dark-700/50 p-4 transition-all duration-200 hover:border-amber-500/30 hover:bg-dark-800/60",children:[e.jsx("div",{className:"absolute left-0 top-0 bottom-0 w-1 bg-gradient-to-b from-amber-500 to-orange-600 opacity-40 group-hover:opacity-100 transition-opacity"}),e.jsxs("div",{className:"pl-3",children:[e.jsxs("div",{className:"flex items-center gap-2 mb-2 flex-wrap",children:[t.type&&e.jsx("span",{className:`text-[10px] font-semibold uppercase tracking-wider px-2 py-0.5 rounded ${u[t.type]||"text-dark-400 bg-dark-700"}`,children:t.type}),e.jsxs("span",{className:"text-[10px] text-dark-500",children:["Section ",t.sectionId]}),t.sectionTitle&&e.jsx("span",{className:"text-[10px] text-dark-600",children:t.sectionTitle}),t.hasProof&&t.proof&&e.jsx("span",{onClick:p=>{p.preventDefault(),b(t.id)},className:"ml-auto text-[10px] text-amber-500/70 font-medium cursor-pointer hover:text-amber-400 transition-colors",children:m===t.id?"Hide Proof":"View Proof"})]}),e.jsx("h3",{className:"font-semibold text-amber-400 group-hover:text-amber-300 transition-colors",children:t.title}),e.jsx("p",{className:"text-sm text-dark-400 mt-1",children:t.statement}),t.hasProof&&t.proof&&e.jsx("div",{className:"mt-3",children:e.jsx(g,{children:m===t.id?e.jsx(y.div,{initial:{height:0,opacity:0},animate:{height:"auto",opacity:1},exit:{height:0,opacity:0},transition:{duration:.3,ease:"easeInOut"},className:"overflow-hidden",children:e.jsxs("div",{className:"pt-4 border-t border-dark-700/50",children:[e.jsx("h4",{className:"text-sm font-semibold text-amber-400 mb-2",children:"Proof:"}),e.jsx(T,{children:t.proof})]})}):null})})]})]})},t.id))})]},i))}),l.length===0&&e.jsx("div",{className:"text-center py-12 text-dark-400",children:"No theorems found matching your search."}),e.jsx("div",{className:"mt-12 pt-8 border-t border-dark-700/50 flex justify-between items-center",children:e.jsxs(d,{to:"/interactive",className:"text-primary-400 hover:text-primary-300 transition-colors flex items-center gap-2",children:[e.jsx("svg",{className:"w-4 h-4",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:e.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M12 19l9 2-9-5-9-5 5-2-9 0-9 5 5 9 12 19 19 2 12 0 0 0 9 5-2-9-5-9 0-9 5-9-5 5-2 9 0 9-5"})}),"Interactive Modules"]})})]})})]})}export{j as default};
