import{j as e}from"./vendor-animation-CmGrHCVd.js";import{L as t,D as r,T as a,E as l}from"./ContentBlocks-CmxP2e2i.js";import{I as s,M as i}from"./MathBlock-DHbtlfU5.js";import"./vendor-react-C1UuhE6f.js";import"./index-BWj-jU2X.js";import"./index-Djvuv9mj.js";import"./vendor-math-ClxlXyPc.js";function j(){return e.jsxs(t,{sectionId:46,children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Iterative methods"})," solve large systems ",e.jsx(s,{children:"Ax = b"})," by repeatedly improving an approximation. They're essential when ",e.jsx(s,{children:"A"})," is too large to factor directly."]}),e.jsx("h2",{children:"Basic Iteration"}),e.jsxs(r,{title:"Splitting Methods",className:"my-6",children:[e.jsxs("p",{children:["Write ",e.jsx(s,{children:"A = M - N"})," and iterate:"]}),e.jsx(i,{children:"M\\mathbf{x}_{k+1} = N\\mathbf{x}_k + \\mathbf{b}"}),e.jsxs("p",{className:"mt-2",children:["Or equivalently: ",e.jsx(s,{children:"\\mathbf{x}_{k+1} = M^{-1}N\\mathbf{x}_k + M^{-1}\\mathbf{b}"})]})]}),e.jsxs(a,{title:"Convergence",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Error recurrence:"})," Let ",e.jsx(s,{children:"\\mathbf{e}_k = \\mathbf{x}_k - \\mathbf{x}^*"})," be the error. Since ",e.jsx(s,{children:"\\mathbf{x}^*"})," is the fixed point:"]}),e.jsx(i,{children:"\\mathbf{e}_{k+1} = M^{-1}N\\mathbf{e}_k = (M^{-1}N)^k \\mathbf{e}_0"}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Power of matrix:"})," For ",e.jsx(s,{children:"B = M^{-1}N"}),", ",e.jsx(s,{children:"B^k \\to 0"})," as ",e.jsx(s,{children:"k \\to \\infty"})," if and only if ",e.jsx(s,{children:"\\rho(B) < 1"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Why:"})," If ",e.jsx(s,{children:"\\lambda"})," is an eigenvalue of ",e.jsx(s,{children:"B"})," with ",e.jsx(s,{children:"|\\lambda| \\geq 1"}),", then ",e.jsx(s,{children:"B^k\\mathbf{v} = \\lambda^k \\mathbf{v}"})," does not converge to zero."]}),e.jsxs("p",{className:"mt-2",children:["Conversely, if ",e.jsx(s,{children:"\\\\rho(B) < 1"}),", all eigenvalue components decay, and ",e.jsx(s,{children:"\\\\|B^k\\\\| \\\\to 0"}),"."]})]}),children:[e.jsx("p",{children:"The iteration converges if and only if:"}),e.jsx(i,{children:"\\rho(M^{-1}N) < 1"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"\\rho"})," is the ",e.jsx("strong",{children:"spectral radius"})," (largest ",e.jsx(s,{children:"|\\lambda|"}),")."]})]}),e.jsx("h2",{children:"Classical Methods"}),e.jsxs(r,{title:"Jacobi Iteration",className:"my-6",children:[e.jsxs("p",{children:["Split ",e.jsx(s,{children:"A = D - L - U"})," (diagonal, lower, upper):"]}),e.jsx(i,{children:"x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j \\neq i} a_{ij}x_j^{(k)}\\right)"}),e.jsx("p",{className:"mt-2",children:"Update all components simultaneously using old values."})]}),e.jsxs(r,{title:"Gauss-Seidel Iteration",className:"my-6",children:[e.jsx("p",{children:"Use new values as soon as they're computed:"}),e.jsx(i,{children:"x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j < i} a_{ij}x_j^{(k+1)} - \\sum_{j > i} a_{ij}x_j^{(k)}\\right)"}),e.jsx("p",{className:"mt-2",children:"Often converges faster than Jacobi."})]}),e.jsxs(l,{title:"Comparing Jacobi and Gauss-Seidel",className:"my-6",children:[e.jsx("p",{children:"For diagonally dominant systems, both converge, but Gauss-Seidel is typically 2Ã— faster."}),e.jsxs("p",{className:"mt-2",children:["For ",e.jsx(s,{children:"A = \\begin{bmatrix} 4 & 1 \\\\\\\\ 1 & 3 \\end{bmatrix}"}),":"]}),e.jsxs("ul",{className:"list-disc list-inside mt-2",children:[e.jsxs("li",{children:["Jacobi: ",e.jsx(s,{children:"\\rho \\approx 0.25"})]}),e.jsxs("li",{children:["Gauss-Seidel: ",e.jsx(s,{children:"\\rho \\approx 0.0625"})]})]})]}),e.jsx("h2",{children:"Krylov Subspace Methods"}),e.jsxs(r,{title:"Krylov Subspace",className:"my-6",children:[e.jsxs("p",{children:["The ",e.jsx(s,{children:"k"}),"th ",e.jsx("strong",{children:"Krylov subspace"})," is:"]}),e.jsx(i,{children:"\\mathcal{K}_k(A, \\mathbf{b}) = \\text{span}\\{\\mathbf{b}, A\\mathbf{b}, A^2\\mathbf{b}, \\ldots, A^{k-1}\\mathbf{b}\\}"}),e.jsx("p",{className:"mt-2",children:"Find the best approximate solution in this subspace."})]}),e.jsxs(a,{title:"Conjugate Gradient Method",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Objective:"})," The minimum of ",e.jsx(s,{children:"f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^TA\\mathbf{x} - \\mathbf{b}^T\\mathbf{x}"})," occurs where ",e.jsx(s,{children:"\\nabla f = A\\mathbf{x} - \\mathbf{b} = \\mathbf{0}"}),", i.e., at the solution of ",e.jsx(s,{children:"A\\mathbf{x} = \\mathbf{b}"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Finite termination:"})," CG generates search directions ",e.jsx(s,{children:"\\mathbf{p}_0, \\mathbf{p}_1, \\ldots"})," that are ",e.jsx(s,{children:"A"}),"-conjugate: ",e.jsx(s,{children:"\\mathbf{p}_i^T A \\mathbf{p}_j = 0"})," for ",e.jsx(s,{children:"i \\neq j"}),"."]}),e.jsxs("p",{className:"mt-2",children:["These form a basis for the Krylov subspace ",e.jsx(s,{children:"\\mathcal{K}_k"}),". Since ",e.jsx(s,{children:"\\dim \\mathcal{K}_n = n"}),", exact solution is found in at most ",e.jsx(s,{children:"n"})," steps."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Error bound:"})," Using Chebyshev polynomials:"]}),e.jsx(i,{children:"\\|\\mathbf{e}_k\\|_A \\leq 2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k \\|\\mathbf{e}_0\\|_A"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"\\|\\mathbf{e}\\|_A = \\sqrt{\\mathbf{e}^T A \\mathbf{e}}"})," is the energy norm."]})]}),children:[e.jsxs("p",{children:["For symmetric positive definite ",e.jsx(s,{children:"A"}),", ",e.jsx("strong",{children:"conjugate gradients (CG)"}),"minimizes:"]}),e.jsx(i,{children:"f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^TA\\mathbf{x} - \\mathbf{b}^T\\mathbf{x}"}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[e.jsxs("li",{children:["Converges in at most ",e.jsx(s,{children:"n"})," steps (exact arithmetic)"]}),e.jsxs("li",{children:["In practice, convergence rate depends on ",e.jsx(s,{children:"\\kappa(A)"})]}),e.jsxs("li",{children:["Error decreases by factor ",e.jsx(s,{children:"\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}"})," per step"]})]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 my-6 border border-primary-500/20",children:[e.jsx("p",{className:"font-semibold text-primary-400 mb-2",children:"GMRES"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["For non-symmetric ",e.jsx(s,{children:"A"}),", ",e.jsx("strong",{children:"GMRES"})," (Generalized Minimal Residual) minimizes",e.jsx(s,{children:"\\|A\\mathbf{x} - \\mathbf{b}\\|"})," over the Krylov subspace. It's the workhorse of modern iterative solvers."]})]}),e.jsx("h2",{children:"Preconditioning"}),e.jsxs(r,{title:"Preconditioning",className:"my-6",children:[e.jsxs("p",{children:["Solve ",e.jsx(s,{children:"M^{-1}Ax = M^{-1}b"})," instead of ",e.jsx(s,{children:"Ax = b"}),":"]}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[e.jsxs("li",{children:["Choose ",e.jsx(s,{children:"M \\approx A"})," but easy to invert"]}),e.jsxs("li",{children:["Goal: ",e.jsx(s,{children:"\\kappa(M^{-1}A) \\ll \\kappa(A)"})]}),e.jsx("li",{children:"Common: incomplete LU, Jacobi, multigrid"})]})]}),e.jsx("h2",{children:"Key Ideas"}),e.jsx("div",{className:"bg-gradient-to-br from-primary-500/10 to-dark-800/50 rounded-xl p-6 my-6 border border-primary-500/20",children:e.jsxs("ul",{className:"space-y-3 text-dark-200",children:[e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"1."}),e.jsxs("span",{children:["Iterative methods: ",e.jsx(s,{children:"\\mathbf{x}_{k+1} = B\\mathbf{x}_k + \\mathbf{c}"}),". Converge if ",e.jsx(s,{children:"\\rho(B) < 1"}),"."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"2."}),e.jsx("span",{children:"Jacobi: update all simultaneously. Gauss-Seidel: use new values immediately."})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"3."}),e.jsxs("span",{children:["Krylov methods (CG, GMRES): search in ",e.jsx(s,{children:"\\text{span}\\{b, Ab, A^2b, \\\\ldots}\\"}),"."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"4."}),e.jsx("span",{children:"CG for SPD matrices; GMRES for general matrices."})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"5."}),e.jsx("span",{children:"Preconditioning reduces condition number and speeds convergence."})]})]})})]})}export{j as default};
