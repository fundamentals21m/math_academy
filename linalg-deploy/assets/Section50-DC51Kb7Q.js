import{j as e}from"./vendor-animation-6GFvN5rC.js";import{L as r,T as n,D as a,E as t}from"./ContentBlocks-_MgE7-JA.js";import{M as i,I as s}from"./MathBlock-BTK9YHZB.js";import"./vendor-react-ByzHzWFU.js";import"./index-BZQLok7r.js";import"./vendor-math-ClxlXyPc.js";import"./vendor-firebase-core-DIJkQv9Q.js";import"./index-Djvuv9mj.js";function j(){return e.jsxs(r,{sectionId:50,children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Machine learning"})," is applied linear algebra. Neural networks, support vector machines, and dimensionality reduction all rely on matrix operations. This final section connects the course to modern data science."]}),e.jsx("h2",{children:"Dimensionality Reduction"}),e.jsxs(n,{title:"PCA Revisited",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Goal:"})," Find directions of maximum variance. The variance along direction ",e.jsx(s,{children:"\\mathbf{v}"})," (unit vector) is ",e.jsx(s,{children:"\\mathbf{v}^T \\Sigma \\mathbf{v}"})," where ",e.jsx(s,{children:"\\Sigma"})," is the covariance matrix."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Solution:"})," Maximize ",e.jsx(s,{children:"\\mathbf{v}^T \\Sigma \\mathbf{v}"})," subject to ",e.jsx(s,{children:"\\|\\mathbf{v}\\| = 1"}),". By Lagrange multipliers, ",e.jsx(s,{children:"\\Sigma \\mathbf{v} = \\lambda \\mathbf{v}"}),"."]}),e.jsxs("p",{className:"mt-2",children:["The principal components are eigenvectors of ",e.jsx(s,{children:"\\Sigma"}),". The variance along each is its eigenvalue."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"SVD connection:"})," For centered data ",e.jsx(s,{children:"X"}),", ",e.jsx(s,{children:"\\Sigma \\propto X^TX"}),". The SVD ",e.jsx(s,{children:"X = U\\Sigma V^T"})," gives the same principal directions in ",e.jsx(s,{children:"V"}),"."]})]}),children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Principal Component Analysis"})," projects high-dimensional data to lower dimensions:"]}),e.jsx(i,{children:"X = U\\Sigma V^T \\quad \\Rightarrow \\quad X_k = U_k\\Sigma_k V_k^T"}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[e.jsxs("li",{children:["Keep only ",e.jsx(s,{children:"k"})," largest singular values"]}),e.jsxs("li",{children:["Minimizes reconstruction error ",e.jsx(s,{children:"\\|X - X_k\\|_F"})]}),e.jsxs("li",{children:["New coordinates: ",e.jsx(s,{children:"Z = XV_k = U_k\\Sigma_k"})]})]})]}),e.jsx("h2",{children:"Linear Classifiers"}),e.jsxs(a,{title:"Linear Classification",className:"my-6",children:[e.jsxs("p",{children:["Find a hyperplane ",e.jsx(s,{children:"\\mathbf{w}^T\\mathbf{x} + b = 0"})," that separates two classes:"]}),e.jsx(i,{children:"\\text{Predict class } +1 \\text{ if } \\mathbf{w}^T\\mathbf{x} + b > 0"}),e.jsxs("p",{className:"mt-2",children:["The weight vector ",e.jsx(s,{children:"\\mathbf{w}"})," is normal to the separating hyperplane."]})]}),e.jsxs(n,{title:"Support Vector Machines",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Margin calculation:"})," For a separating hyperplane ",e.jsx(s,{children:"\\mathbf{w}^T\\mathbf{x} + b = 0"}),", the distance from point ",e.jsx(s,{children:"\\mathbf{x}_i"})," to the hyperplane is ",e.jsx(s,{children:"|\\mathbf{w}^T\\mathbf{x}_i + b|/\\|\\mathbf{w}\\|"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Constraint normalization:"})," Scale ",e.jsx(s,{children:"\\mathbf{w}, b"})," so the closest points satisfy ",e.jsx(s,{children:"|\\mathbf{w}^T\\mathbf{x}_i + b| = 1"}),". Then margin = ",e.jsx(s,{children:"1/\\|\\mathbf{w}\\|"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Maximize margin:"})," Maximize ",e.jsx(s,{children:"1/\\|\\mathbf{w}\\|"})," ⟺ minimize ",e.jsx(s,{children:"\\|\\mathbf{w}\\|^2"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Support vectors:"})," The constraints ",e.jsx(s,{children:"y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1"})," are active (equality) only for points on the margin boundaries. These are the support vectors; they determine the solution."]})]}),children:[e.jsx("p",{children:"SVM finds the maximum-margin hyperplane:"}),e.jsx(i,{children:"\\min \\|\\mathbf{w}\\|^2 \\quad \\text{subject to } y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1"}),e.jsxs("p",{className:"mt-2",children:["The margin is ",e.jsx(s,{children:"2/\\|\\mathbf{w}\\|"}),". Support vectors lie on the margin boundaries."]})]}),e.jsx("h2",{children:"Neural Networks"}),e.jsxs(a,{title:"Neural Network as Matrix Operations",className:"my-6",children:[e.jsx("p",{children:"A fully-connected layer computes:"}),e.jsx(i,{children:"\\mathbf{h} = \\sigma(W\\mathbf{x} + \\mathbf{b})"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"\\sigma"})," is a nonlinear activation (ReLU, sigmoid, etc.).",e.jsx("br",{}),"Without nonlinearity, deep networks would collapse to a single matrix!"]})]}),e.jsxs(t,{title:"Matrix View of Deep Learning",className:"my-6",children:[e.jsx("p",{children:"A 3-layer network:"}),e.jsx(i,{children:"\\mathbf{y} = W_3 \\sigma(W_2 \\sigma(W_1 \\mathbf{x}))"}),e.jsxs("p",{className:"mt-2",children:["Training = finding ",e.jsx(s,{children:"W_1, W_2, W_3"})," to minimize loss.",e.jsx("br",{}),"Gradients computed via backpropagation (chain rule + matrix calculus)."]})]}),e.jsx("h2",{children:"Gradient Descent"}),e.jsxs(a,{title:"Gradient Descent",className:"my-6",children:[e.jsxs("p",{children:["Minimize ",e.jsx(s,{children:"f(\\mathbf{w})"})," by iterating:"]}),e.jsx(i,{children:"\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha \\nabla f(\\mathbf{w}_k)"}),e.jsxs("p",{className:"mt-2",children:["For least squares: ",e.jsx(s,{children:"\\nabla f = X^T(X\\mathbf{w} - y)"})]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 my-6 border border-primary-500/20",children:[e.jsx("p",{className:"font-semibold text-primary-400 mb-2",children:"Condition Number in Optimization"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Convergence rate of gradient descent depends on ",e.jsx(s,{children:"\\kappa(X^TX)"}),".",e.jsx("br",{}),"Ill-conditioned problems → slow convergence → need preconditioning or adaptive methods (Adam, etc.)."]})]}),e.jsx("h2",{children:"Matrix Factorization"}),e.jsxs(a,{title:"Low-Rank Matrix Factorization",className:"my-6",children:[e.jsxs("p",{children:["Approximate matrix ",e.jsx(s,{children:"R"})," (e.g., user-item ratings) as:"]}),e.jsx(i,{children:"R \\approx UV^T"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"U"})," (users) and ",e.jsx(s,{children:"V"})," (items) have ",e.jsx(s,{children:"k"})," columns (latent factors). This is the basis of recommender systems!"]})]}),e.jsx("h2",{children:"Regularization"}),e.jsxs(n,{title:"Ridge Regression",className:"my-6",proof:e.jsxs(e.Fragment,{children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Objective:"})," Minimize ",e.jsx(s,{children:"f(\\boldsymbol{\\beta}) = \\|X\\boldsymbol{\\beta} - \\mathbf{y}\\|^2 + \\lambda\\|\\boldsymbol{\\beta}\\|^2"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Derivative:"})," ",e.jsx(s,{children:"\\frac{\\partial f}{\\partial \\boldsymbol{\\beta}} = 2X^T(X\\boldsymbol{\\beta} - \\mathbf{y}) + 2\\lambda\\boldsymbol{\\beta} = 0"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Solve:"})," ",e.jsx(s,{children:"(X^TX + \\lambda I)\\boldsymbol{\\beta} = X^T\\mathbf{y}"}),", giving ",e.jsx(s,{children:"\\hat{\\boldsymbol{\\beta}} = (X^TX + \\lambda I)^{-1}X^T\\mathbf{y}"}),"."]}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Why it works:"})," Adding ",e.jsx(s,{children:"\\lambda I"})," shifts all eigenvalues of ",e.jsx(s,{children:"X^TX"})," up by ",e.jsx(s,{children:"\\lambda"}),". This:"]}),e.jsxs("ul",{className:"list-disc list-inside ml-4",children:[e.jsx("li",{children:"Ensures invertibility (no zero eigenvalues)"}),e.jsxs("li",{children:["Reduces condition number: ",e.jsx(s,{children:"\\kappa = (\\sigma_{\\max}^2 + \\lambda)/(\\sigma_{\\min}^2 + \\lambda)"})]}),e.jsx("li",{children:"Shrinks coefficients toward zero (bias-variance tradeoff)"})]})]}),children:[e.jsx("p",{children:"Add penalty to prevent overfitting:"}),e.jsx(i,{children:"\\min \\|X\\boldsymbol{\\beta} - y\\|^2 + \\lambda\\|\\boldsymbol{\\beta}\\|^2"}),e.jsx("p",{className:"mt-2",children:"Solution:"}),e.jsx(i,{children:"\\hat{\\boldsymbol{\\beta}} = (X^TX + \\lambda I)^{-1}X^Ty"}),e.jsxs("p",{className:"mt-2 text-primary-400",children:["Adding ",e.jsx(s,{children:"\\lambda I"})," improves condition number and shrinks coefficients."]})]}),e.jsx("h2",{children:"Key Ideas"}),e.jsx("div",{className:"bg-gradient-to-br from-primary-500/10 to-dark-800/50 rounded-xl p-6 my-6 border border-primary-500/20",children:e.jsxs("ul",{className:"space-y-3 text-dark-200",children:[e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"1."}),e.jsxs("span",{children:["PCA/SVD for dimensionality reduction; keep top ",e.jsx(s,{children:"k"})," singular values."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"2."}),e.jsxs("span",{children:["SVM: find max-margin hyperplane; ",e.jsx(s,{children:"\\mathbf{w}"})," is normal vector."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"3."}),e.jsx("span",{children:"Neural networks: layers of matrix multiply + nonlinearity."})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"4."}),e.jsxs("span",{children:["Gradient descent: ",e.jsx(s,{children:"\\mathbf{w} \\to \\mathbf{w} - \\alpha\\nabla f"}),"; condition number affects speed."]})]}),e.jsxs("li",{className:"flex items-start gap-3",children:[e.jsx("span",{className:"text-primary-400 font-bold",children:"5."}),e.jsxs("span",{children:["Ridge regression: ",e.jsx(s,{children:"(X^TX + \\lambda I)^{-1}X^Ty"})," for regularization."]})]})]})}),e.jsxs("div",{className:"bg-gradient-to-br from-green-500/10 to-dark-800/50 rounded-xl p-6 my-8 border border-green-500/20",children:[e.jsx("h3",{className:"text-xl font-bold text-green-400 mb-4",children:"Course Complete!"}),e.jsx("p",{className:"text-dark-200",children:"Congratulations on completing Introduction to Linear Algebra! You now have the foundation to understand and apply linear algebra in:"}),e.jsxs("ul",{className:"list-disc list-inside mt-3 text-dark-300 space-y-1",children:[e.jsx("li",{children:"Machine Learning and Data Science"}),e.jsx("li",{children:"Computer Graphics and Vision"}),e.jsx("li",{children:"Signal Processing and Communications"}),e.jsx("li",{children:"Scientific Computing and Engineering"}),e.jsx("li",{children:"Cryptography and Coding Theory"})]}),e.jsx("p",{className:"mt-4 text-dark-200",children:"Linear algebra is the language of modern applied mathematics. Keep exploring!"})]})]})}export{j as default};
