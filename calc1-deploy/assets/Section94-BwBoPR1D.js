import{j as e}from"./vendor-animation-0o8UKZ_1.js";import{L as n,C as s}from"./Callout-C5cp0Vl2.js";import{I as i,M as t}from"./MathBlock-_bBfq7Jh.js";import"./vendor-react-Drj8qL0h.js";import"./index-Cvr528Rw.js";import"./vendor-math-p018AHG0.js";import"./vendor-firebase-core-BXWtuYvb.js";import"./quizMap-B5sveRg_.js";function m(){return e.jsxs(n,{sectionId:94,children:[e.jsx("h1",{children:"Best Approximation"}),e.jsxs("p",{children:["When a vector doesn't lie in a subspace, we seek the ",e.jsx("strong",{children:"best approximation"}),"—the closest point in the subspace. This leads to the powerful method of ",e.jsx("strong",{children:"least squares"}),", with applications ranging from data fitting to solving overdetermined systems."]}),e.jsx("h2",{children:"The Approximation Problem"}),e.jsxs(s,{type:"definition",title:"Best Approximation",children:[e.jsxs("p",{children:["Given a vector ",e.jsx(i,{children:"\\mathbf{b}"})," and a subspace ",e.jsx(i,{children:"W"})," of an inner product space ",e.jsx(i,{children:"V"}),", the ",e.jsx("strong",{children:"best approximation"})," to ",e.jsx(i,{children:"\\mathbf{b}"})," in ",e.jsx(i,{children:"W"})," is the vector ",e.jsx(i,{children:"\\hat{\\mathbf{b}} \\in W"})," that minimizes:"]}),e.jsx(t,{children:"\\|\\mathbf{b} - \\hat{\\mathbf{b}}\\|"})]}),e.jsxs(s,{type:"theorem",title:"Best Approximation Theorem",children:[e.jsxs("p",{children:["Let ",e.jsx(i,{children:"W"})," be a finite-dimensional subspace of an inner product space ",e.jsx(i,{children:"V"}),". For any ",e.jsx(i,{children:"\\mathbf{b} \\in V"}),":"]}),e.jsxs("ol",{className:"list-decimal pl-6 space-y-2",children:[e.jsxs("li",{children:["There exists a unique best approximation ",e.jsx(i,{children:"\\hat{\\mathbf{b}} \\in W"})]}),e.jsxs("li",{children:[e.jsx(i,{children:"\\hat{\\mathbf{b}} = \\text{proj}_W(\\mathbf{b})"})," (the orthogonal projection)"]}),e.jsxs("li",{children:["The error ",e.jsx(i,{children:"\\mathbf{b} - \\hat{\\mathbf{b}}"})," is orthogonal to ",e.jsx(i,{children:"W"})]})]})]}),e.jsxs(s,{type:"info",title:"Why Projection is Best",children:[e.jsxs("p",{children:["For any ",e.jsx(i,{children:"\\mathbf{w} \\in W"}),", by the Pythagorean theorem:"]}),e.jsx(t,{children:"\\|\\mathbf{b} - \\mathbf{w}\\|^2 = \\|\\mathbf{b} - \\hat{\\mathbf{b}}\\|^2 + \\|\\hat{\\mathbf{b}} - \\mathbf{w}\\|^2 \\geq \\|\\mathbf{b} - \\hat{\\mathbf{b}}\\|^2"}),e.jsxs("p",{children:["Equality holds only when ",e.jsx(i,{children:"\\mathbf{w} = \\hat{\\mathbf{b}}"}),"."]})]}),e.jsx("h2",{children:"Projection Formula"}),e.jsxs(s,{type:"theorem",title:"Projection with Orthonormal Basis",children:[e.jsxs("p",{children:["If ",e.jsx(i,{children:"\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}"})," is an orthonormal basis for ",e.jsx(i,{children:"W"}),", then:"]}),e.jsx(t,{children:"\\hat{\\mathbf{b}} = \\text{proj}_W(\\mathbf{b}) = \\sum_{i=1}^{k} \\langle \\mathbf{b}, \\mathbf{u}_i \\rangle \\mathbf{u}_i"})]}),e.jsxs(s,{type:"example",title:"Best Approximation in ℝ³",children:[e.jsxs("p",{children:["Find the best approximation to ",e.jsx(i,{children:"\\mathbf{b} = (1, 2, 3)"})," in the plane ",e.jsx(i,{children:"W = \\text{span}\\{(1, 0, 0), (0, 1, 0)\\}"}),"."]}),e.jsxs("p",{children:[e.jsx("strong",{children:"Solution:"})," The standard basis vectors are already orthonormal."]}),e.jsx(t,{children:"\\hat{\\mathbf{b}} = \\langle \\mathbf{b}, \\mathbf{e}_1 \\rangle \\mathbf{e}_1 + \\langle \\mathbf{b}, \\mathbf{e}_2 \\rangle \\mathbf{e}_2 = 1 \\cdot (1, 0, 0) + 2 \\cdot (0, 1, 0) = (1, 2, 0)"}),e.jsxs("p",{children:["Error: ",e.jsx(i,{children:"\\mathbf{b} - \\hat{\\mathbf{b}} = (0, 0, 3)"}),", which is orthogonal to ",e.jsx(i,{children:"W"}),". ✓"]})]}),e.jsx("h2",{children:"Least Squares Problems"}),e.jsxs(s,{type:"definition",title:"Least Squares Problem",children:[e.jsxs("p",{children:["Given a system ",e.jsx(i,{children:"A\\mathbf{x} = \\mathbf{b}"})," that may have no solution (when ",e.jsx(i,{children:"\\mathbf{b} \\notin \\text{Col}(A)"}),"), the ",e.jsx("strong",{children:"least squares solution"})," is the vector ",e.jsx(i,{children:"\\hat{\\mathbf{x}}"})," that minimizes:"]}),e.jsx(t,{children:"\\|A\\mathbf{x} - \\mathbf{b}\\|^2"})]}),e.jsxs(s,{type:"info",title:"Geometric Interpretation",children:[e.jsxs("p",{children:["We seek ",e.jsx(i,{children:"\\hat{\\mathbf{x}}"})," such that ",e.jsx(i,{children:"A\\hat{\\mathbf{x}}"})," is the best approximation to ",e.jsx(i,{children:"\\mathbf{b}"})," in ",e.jsx(i,{children:"\\text{Col}(A)"}),"."]}),e.jsx(t,{children:"A\\hat{\\mathbf{x}} = \\text{proj}_{\\text{Col}(A)}(\\mathbf{b})"})]}),e.jsxs(s,{type:"theorem",title:"Normal Equations",children:[e.jsxs("p",{children:["The least squares solution ",e.jsx(i,{children:"\\hat{\\mathbf{x}}"})," satisfies the ",e.jsx("strong",{children:"normal equations"}),":"]}),e.jsx(t,{children:"A^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b}"}),e.jsxs("p",{children:["If ",e.jsx(i,{children:"A"})," has linearly independent columns, then ",e.jsx(i,{children:"A^T A"})," is invertible and:"]}),e.jsx(t,{children:"\\hat{\\mathbf{x}} = (A^T A)^{-1} A^T \\mathbf{b}"})]}),e.jsxs(s,{type:"info",title:"Derivation",children:[e.jsxs("p",{children:["The error ",e.jsx(i,{children:"\\mathbf{b} - A\\hat{\\mathbf{x}}"})," must be orthogonal to ",e.jsx(i,{children:"\\text{Col}(A)"}),"."]}),e.jsxs("p",{children:["This means ",e.jsx(i,{children:"\\mathbf{b} - A\\hat{\\mathbf{x}} \\perp"})," every column of ",e.jsx(i,{children:"A"}),":"]}),e.jsx(t,{children:"A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = \\mathbf{0}"}),e.jsx("p",{children:"Rearranging gives the normal equations."})]}),e.jsx("h2",{children:"Linear Regression"}),e.jsxs(s,{type:"example",title:"Fitting a Line",children:[e.jsxs("p",{children:["Find the best-fit line ",e.jsx(i,{children:"y = c_0 + c_1 x"})," through the points ",e.jsx(i,{children:"(0, 1), (1, 2), (2, 4)"}),"."]}),e.jsxs("p",{children:[e.jsx("strong",{children:"Solution:"})," Set up the system ",e.jsx(i,{children:"A\\mathbf{c} = \\mathbf{b}"}),":"]}),e.jsx(t,{children:"\\begin{pmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} c_0 \\ c_1 \\end{pmatrix} = \\begin{pmatrix} 1 \\ 2 \\ 4 \\end{pmatrix}"}),e.jsxs("p",{children:["Compute ",e.jsx(i,{children:"A^T A"})," and ",e.jsx(i,{children:"A^T \\mathbf{b}"}),":"]}),e.jsx(t,{children:"A^T A = \\begin{pmatrix} 3 & 3 \\ 3 & 5 \\end{pmatrix}, \\quad A^T \\mathbf{b} = \\begin{pmatrix} 7 \\ 10 \\end{pmatrix}"}),e.jsx("p",{children:"Solve the normal equations:"}),e.jsx(t,{children:"\\begin{pmatrix} 3 & 3 \\ 3 & 5 \\end{pmatrix} \\begin{pmatrix} c_0 \\ c_1 \\end{pmatrix} = \\begin{pmatrix} 7 \\ 10 \\end{pmatrix}"}),e.jsx(t,{children:"c_0 = \\frac{5}{6}, \\quad c_1 = \\frac{3}{2}"}),e.jsxs("p",{children:["Best-fit line: ",e.jsx(i,{children:"y = \\frac{5}{6} + \\frac{3}{2}x"})]})]}),e.jsxs(s,{type:"example",title:"Fitting a Parabola",children:[e.jsxs("p",{children:["For a quadratic fit ",e.jsx(i,{children:"y = c_0 + c_1 x + c_2 x^2"}),", the design matrix becomes:"]}),e.jsx(t,{children:"A = \\begin{pmatrix} 1 & x_1 & x_1^2 \\ 1 & x_2 & x_2^2 \\ \\vdots & \\vdots & \\vdots \\ 1 & x_n & x_n^2 \\end{pmatrix}"}),e.jsx("p",{children:"The same normal equations method applies."})]}),e.jsx("h2",{children:"Least Squares via QR"}),e.jsxs(s,{type:"theorem",title:"QR Solution",children:[e.jsxs("p",{children:["If ",e.jsx(i,{children:"A = QR"})," (QR factorization with ",e.jsx(i,{children:"Q"})," orthonormal, ",e.jsx(i,{children:"R"})," upper triangular), then:"]}),e.jsx(t,{children:"R\\hat{\\mathbf{x}} = Q^T \\mathbf{b}"}),e.jsx("p",{children:"This is more numerically stable than using the normal equations directly."})]}),e.jsxs(s,{type:"info",title:"Why QR is Better",children:[e.jsxs("p",{children:["Computing ",e.jsx(i,{children:"A^T A"})," can amplify numerical errors (condition number squares). The QR approach avoids this by working with ",e.jsx(i,{children:"Q"})," and ",e.jsx(i,{children:"R"})," directly:"]}),e.jsx(t,{children:"A^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b} \\implies R^T R \\hat{\\mathbf{x}} = R^T Q^T \\mathbf{b} \\implies R\\hat{\\mathbf{x}} = Q^T \\mathbf{b}"})]}),e.jsx("h2",{children:"Function Approximation"}),e.jsxs(s,{type:"example",title:"Best Polynomial Approximation",children:[e.jsxs("p",{children:["Find the best linear approximation to ",e.jsx(i,{children:"f(x) = e^x"})," on ",e.jsx(i,{children:"[0, 1]"})," in the inner product space with:"]}),e.jsx(t,{children:"\\langle f, g \\rangle = \\int_0^1 f(x) g(x) \\, dx"}),e.jsxs("p",{children:[e.jsx("strong",{children:"Solution:"})," Seek ",e.jsx(i,{children:"p(x) = c_0 + c_1 x"})," minimizing ",e.jsx(i,{children:"\\|e^x - p(x)\\|"}),"."]}),e.jsxs("p",{children:["Apply Gram-Schmidt to ",e.jsx(i,{children:"\\{1, x\\}"}),":"]}),e.jsx(t,{children:"\\|1\\|^2 = \\int_0^1 1 \\, dx = 1, \\quad \\mathbf{u}_1 = 1"}),e.jsx(t,{children:"\\langle x, 1 \\rangle = \\int_0^1 x \\, dx = \\frac{1}{2}"}),e.jsx(t,{children:"\\mathbf{w}_2 = x - \\frac{1}{2}, \\quad \\|\\mathbf{w}_2\\|^2 = \\int_0^1 \\left(x - \\frac{1}{2}\\right)^2 dx = \\frac{1}{12}"}),e.jsx(t,{children:"\\mathbf{u}_2 = \\sqrt{12}\\left(x - \\frac{1}{2}\\right) = 2\\sqrt{3}\\left(x - \\frac{1}{2}\\right)"}),e.jsxs("p",{children:["Project ",e.jsx(i,{children:"e^x"}),":"]}),e.jsx(t,{children:"\\langle e^x, 1 \\rangle = e - 1, \\quad \\langle e^x, \\mathbf{u}_2 \\rangle = 2\\sqrt{3}(1) = 2\\sqrt{3}"}),e.jsxs("p",{children:["The best approximation is ",e.jsx(i,{children:"p(x) \\approx 0.718 + 1.69x"}),"."]})]}),e.jsx("h2",{children:"The Pseudoinverse"}),e.jsxs(s,{type:"definition",title:"Moore-Penrose Pseudoinverse",children:[e.jsxs("p",{children:["For any matrix ",e.jsx(i,{children:"A"}),", the ",e.jsx("strong",{children:"pseudoinverse"})," ",e.jsx(i,{children:"A^+"})," gives the least squares solution:"]}),e.jsx(t,{children:"\\hat{\\mathbf{x}} = A^+ \\mathbf{b}"}),e.jsxs("p",{children:["When ",e.jsx(i,{children:"A"})," has independent columns: ",e.jsx(i,{children:"A^+ = (A^T A)^{-1} A^T"})]})]}),e.jsx(s,{type:"theorem",title:"Properties of Pseudoinverse",children:e.jsxs("ol",{className:"list-decimal pl-6 space-y-2",children:[e.jsx("li",{children:e.jsx(i,{children:"A A^+ A = A"})}),e.jsx("li",{children:e.jsx(i,{children:"A^+ A A^+ = A^+"})}),e.jsxs("li",{children:[e.jsx(i,{children:"(A A^+)^T = A A^+"})," (projection onto ",e.jsx(i,{children:"\\text{Col}(A)"}),")"]}),e.jsxs("li",{children:[e.jsx(i,{children:"(A^+ A)^T = A^+ A"})," (projection onto ",e.jsx(i,{children:"\\text{Row}(A)"}),")"]})]})}),e.jsx("h2",{children:"Residuals and Error Analysis"}),e.jsxs(s,{type:"definition",title:"Residual",children:[e.jsxs("p",{children:["The ",e.jsx("strong",{children:"residual"})," is the error vector:"]}),e.jsx(t,{children:"\\mathbf{r} = \\mathbf{b} - A\\hat{\\mathbf{x}}"}),e.jsxs("p",{children:["The least squares minimizes ",e.jsx(i,{children:"\\|\\mathbf{r}\\|^2 = \\sum r_i^2"}),"."]})]}),e.jsx(s,{type:"theorem",title:"Residual Properties",children:e.jsxs("ol",{className:"list-decimal pl-6 space-y-2",children:[e.jsx("li",{children:e.jsx(i,{children:"\\mathbf{r} \\perp \\text{Col}(A)"})}),e.jsx("li",{children:e.jsx(i,{children:"A^T \\mathbf{r} = \\mathbf{0}"})}),e.jsxs("li",{children:[e.jsx(i,{children:"\\|\\mathbf{b}\\|^2 = \\|A\\hat{\\mathbf{x}}\\|^2 + \\|\\mathbf{r}\\|^2"})," (Pythagorean theorem)"]})]})}),e.jsxs(s,{type:"example",title:"Computing Residual",children:[e.jsxs("p",{children:["For the line-fitting example with ",e.jsx(i,{children:"\\hat{\\mathbf{x}} = (5/6, 3/2)^T"}),":"]}),e.jsx(t,{children:"A\\hat{\\mathbf{x}} = \\begin{pmatrix} 5/6 \\ 7/3 \\ 23/6 \\end{pmatrix}, \\quad \\mathbf{r} = \\mathbf{b} - A\\hat{\\mathbf{x}} = \\begin{pmatrix} 1/6 \\ -1/3 \\ 1/6 \\end{pmatrix}"}),e.jsx(t,{children:"\\|\\mathbf{r}\\|^2 = \\frac{1}{36} + \\frac{1}{9} + \\frac{1}{36} = \\frac{1}{6}"})]}),e.jsx("h2",{children:"Applications"}),e.jsx(s,{type:"info",title:"Common Applications",children:e.jsxs("ul",{className:"list-disc pl-6 space-y-2",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"Linear regression:"})," Fitting lines/curves to data"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Signal processing:"})," Denoising, filtering"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Computer graphics:"})," Surface fitting"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Control systems:"})," System identification"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Machine learning:"})," Linear models, feature extraction"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Statistics:"})," Parameter estimation"]})]})}),e.jsx("h2",{children:"Summary"}),e.jsxs(s,{type:"info",title:"Key Concepts",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Best approximation:"})," ",e.jsx(i,{children:"\\hat{\\mathbf{b}} = \\text{proj}_W(\\mathbf{b})"})," minimizes ",e.jsx(i,{children:"\\|\\mathbf{b} - \\hat{\\mathbf{b}}\\|"})]}),e.jsxs("p",{className:"mt-3",children:[e.jsx("strong",{children:"Least squares problem:"})," Minimize ",e.jsx(i,{children:"\\|A\\mathbf{x} - \\mathbf{b}\\|^2"})]}),e.jsx("p",{className:"mt-3",children:e.jsx("strong",{children:"Normal equations:"})}),e.jsx(t,{children:"A^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b}"}),e.jsx("p",{className:"mt-3",children:e.jsx("strong",{children:"Solution (when columns independent):"})}),e.jsx(t,{children:"\\hat{\\mathbf{x}} = (A^T A)^{-1} A^T \\mathbf{b}"}),e.jsxs("p",{className:"mt-3",children:[e.jsx("strong",{children:"QR method:"})," ",e.jsx(i,{children:"R\\hat{\\mathbf{x}} = Q^T \\mathbf{b}"})," (more stable)"]}),e.jsx("p",{className:"mt-3",children:e.jsx("strong",{children:"Key properties:"})}),e.jsxs("ul",{className:"list-disc pl-6 space-y-1",children:[e.jsxs("li",{children:["Residual ",e.jsx(i,{children:"\\mathbf{r} = \\mathbf{b} - A\\hat{\\mathbf{x}}"})," is orthogonal to ",e.jsx(i,{children:"\\text{Col}(A)"})]}),e.jsx("li",{children:e.jsx(i,{children:"\\|\\mathbf{b}\\|^2 = \\|A\\hat{\\mathbf{x}}\\|^2 + \\|\\mathbf{r}\\|^2"})})]})]})]})}export{m as default};
