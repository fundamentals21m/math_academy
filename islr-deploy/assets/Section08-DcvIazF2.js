import{j as e}from"./index-D-anHz-v.js";import{L as n,C as a}from"./Callout-S0Ir82k9.js";import{D as s,R as r,E as l}from"./ContentBlocks-DqG_oog1.js";import{M as t,a as i}from"./MathBlock-D_BNZVWb.js";function m(){return e.jsxs(n,{sectionId:8,children:[e.jsx("h2",{children:"Other Considerations in the Regression Model"}),e.jsxs("p",{children:["So far we have assumed that the predictors are quantitative. But often some predictors are ",e.jsx("em",{children:"qualitative"}),". We also need to consider extensions like interactions and potential problems that can arise in regression."]}),e.jsx("h2",{children:"Qualitative Predictors"}),e.jsxs("p",{children:["Qualitative variables, also called ",e.jsx("em",{children:"categorical variables"})," or ",e.jsx("em",{children:"factors"}),", take on discrete values. Examples include gender, region, or brand."]}),e.jsx("h3",{children:"Two-Level Qualitative Variables"}),e.jsxs("p",{children:["For a qualitative variable with two levels (e.g., male/female), we create a",e.jsx("em",{children:"dummy variable"})," (or ",e.jsx("em",{children:"indicator variable"}),"):"]}),e.jsxs(s,{title:"Dummy Variable",children:[e.jsx(t,{children:"x_i = \\begin{cases} 1 & \\text{if } i\\text{th person is female} \\\\ 0 & \\text{if } i\\text{th person is male} \\end{cases}"}),e.jsx("p",{className:"mt-2",children:"The resulting model is:"}),e.jsx(t,{children:"y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i = \\begin{cases} \\beta_0 + \\beta_1 + \\epsilon_i & \\text{if female} \\\\ \\beta_0 + \\epsilon_i & \\text{if male} \\end{cases}"}),e.jsxs("p",{className:"mt-2",children:["Here ",e.jsx(i,{children:"\\beta_0"})," is the average value for males (the ",e.jsx("em",{children:"baseline"}),"), and ",e.jsx(i,{children:"\\beta_1"})," is the difference between females and males."]})]}),e.jsx("h3",{children:"Qualitative Variables with More Than Two Levels"}),e.jsx("p",{children:"When a qualitative predictor has more than two levels, we create multiple dummy variables. For example, for ethnicity with three levels (Asian, Caucasian, African American):"}),e.jsx(t,{children:"x_{i1} = \\begin{cases} 1 & \\text{if Asian} \\\\ 0 & \\text{otherwise} \\end{cases} \\quad x_{i2} = \\begin{cases} 1 & \\text{if Caucasian} \\\\ 0 & \\text{otherwise} \\end{cases}"}),e.jsx("p",{children:"The model becomes:"}),e.jsx(t,{children:"y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i"}),e.jsxs(a,{type:"info",children:[e.jsx("strong",{children:"Baseline Category:"})," With ",e.jsx(i,{children:"k"})," levels, we create ",e.jsx(i,{children:"k-1"})," dummy variables. The level without a dummy variable is the ",e.jsx("em",{children:"baseline"}),". The coefficients represent differences from this baseline."]}),e.jsx(r,{title:"Qualitative Predictors in R",output:`Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        5.7066     0.5314  10.739  < 2e-16 ***
ShelveLoc Good     4.8487     0.1528  31.724  < 2e-16 ***
ShelveLoc Medium   1.9533     0.1258  15.531  < 2e-16 ***`,children:`# R automatically creates dummy variables for factors
lm.fit <- lm(Sales ~ ShelveLoc, data = Carseats)
summary(lm.fit)

# ShelveLoc has 3 levels: Bad, Good, Medium
# Bad is the baseline (intercept)
# Coefficients show difference from Bad`}),e.jsx("h2",{children:"Interaction Terms"}),e.jsxs("p",{children:["The standard linear model assumes that the effect of one predictor on the response is",e.jsx("em",{children:"additive"}),"—it doesn't depend on the values of other predictors. But this is often unrealistic."]}),e.jsxs(s,{title:"Interaction Effect",children:[e.jsxs("p",{children:["An ",e.jsx("strong",{children:"interaction"})," occurs when the effect of one predictor on the response depends on the value of another predictor. We include an interaction term:"]}),e.jsx(t,{children:"Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon"}),e.jsx("p",{className:"mt-2",children:"This can be rewritten as:"}),e.jsx(t,{children:"Y = \\beta_0 + (\\beta_1 + \\beta_3 X_2) X_1 + \\beta_2 X_2 + \\epsilon"}),e.jsxs("p",{className:"mt-2",children:["The effect of ",e.jsx(i,{children:"X_1"})," on ",e.jsx(i,{children:"Y"})," is now ",e.jsx(i,{children:"\\beta_1 + \\beta_3 X_2"}),", which depends on ",e.jsx(i,{children:"X_2"}),"."]})]}),e.jsxs(l,{title:"TV and Radio Interaction",children:[e.jsxs("p",{children:["In the advertising data, there may be ",e.jsx("em",{children:"synergy"})," between TV and radio advertising. Spending on radio might be more effective when TV spending is also high."]}),e.jsx(t,{children:"\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{TV} \\times \\text{radio}) + \\epsilon"}),e.jsxs("p",{className:"mt-2",children:["The coefficient ",e.jsx(i,{children:"\\beta_3"})," for the interaction term is positive and highly significant, confirming that TV and radio advertising are more effective together than the sum of their individual effects."]})]}),e.jsx(r,{title:"Interaction in R",output:`Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) 6.750e+00  2.479e-01  27.233  < 2e-16 ***
TV          1.910e-02  1.504e-03  12.699  < 2e-16 ***
radio       2.886e-02  8.905e-03   3.241  0.00142 ** 
TV:radio    1.086e-03  5.242e-05  20.727  < 2e-16 ***

Multiple R-squared:  0.9678`,children:`# Include interaction term
lm.fit <- lm(sales ~ TV * radio, data = Advertising)
summary(lm.fit)

# TV * radio is shorthand for TV + radio + TV:radio
# The interaction is highly significant!`}),e.jsxs(a,{type:"warning",children:[e.jsx("strong",{children:"Hierarchical Principle:"})," If we include an interaction in a model, we should also include the main effects, even if the p-values for the main effects are not significant. The interaction term only makes sense in the context of the main effects."]}),e.jsx("h2",{children:"Non-linear Relationships"}),e.jsx("p",{children:"Linear regression can model some non-linear relationships by including transformed versions of the predictors. For example, we can include polynomial terms."}),e.jsxs(s,{title:"Polynomial Regression",children:[e.jsx(t,{children:"y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\cdots + \\beta_d x_i^d + \\epsilon_i"}),e.jsxs("p",{className:"mt-2",children:["This is still a ",e.jsx("em",{children:"linear"})," model because it is linear in the coefficients",e.jsx(i,{children:"\\beta_0, \\beta_1, \\ldots, \\beta_d"}),". The predictors are ",e.jsx(i,{children:"x, x^2, \\ldots, x^d"}),"."]})]}),e.jsx(r,{title:"Polynomial Regression in R",output:`Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.42e+01   2.37e+00  -5.986 3.28e-09 ***
horsepower   1.98e-01   1.18e-02  16.737  < 2e-16 ***
I(horsepower^2)  -7.69e-04   1.38e-05 -55.658  < 2e-16 ***

Multiple R-squared:  0.6876`,children:`# Quadratic fit
lm.fit2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)
summary(lm.fit2)

# The squared term is highly significant
# This indicates a non-linear relationship`}),e.jsx("h2",{children:"Potential Problems"}),e.jsx("p",{children:"When fitting a linear regression model, several problems may arise. Here are the most common ones and how to detect them."}),e.jsx("h3",{children:"1. Non-linearity of the Data"}),e.jsxs("p",{children:["If the true relationship is non-linear, the linear model will systematically over- or under-predict. Use ",e.jsx("em",{children:"residual plots"})," to detect this."]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700 my-4",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Detection"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Plot residuals ",e.jsx(i,{children:"e_i = y_i - \\hat y_i"})," against fitted values ",e.jsx(i,{children:"\\hat y_i"}),". Look for patterns—a curved pattern suggests non-linearity."]})]}),e.jsx("h3",{children:"2. Correlation of Error Terms"}),e.jsx("p",{children:"An important assumption is that the error terms are uncorrelated. If there is correlation, the estimated standard errors will be too small, leading to overly narrow confidence intervals."}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700 my-4",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Detection"}),e.jsx("p",{className:"text-dark-300 text-sm",children:"Common with time series data. Plot residuals against time or observation order and look for patterns. The Durbin-Watson test can formally test for autocorrelation."})]}),e.jsx("h3",{children:"3. Non-constant Variance (Heteroscedasticity)"}),e.jsxs("p",{children:["We assume ",e.jsx(i,{children:"\\text{Var}(\\epsilon_i) = \\sigma^2"})," is constant. If the variance of errors increases with the response, we have ",e.jsx("em",{children:"heteroscedasticity"}),"."]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700 my-4",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Detection & Solution"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Look for a funnel shape in the residual plot. Common solutions include transforming the response using ",e.jsx(i,{children:"\\log(Y)"})," or ",e.jsxs(i,{children:["\\sqrt",Y]}),"."]})]}),e.jsx("h3",{children:"4. Outliers"}),e.jsxs("p",{children:["An ",e.jsx("em",{children:"outlier"})," is a point for which ",e.jsx(i,{children:"y_i"})," is far from the value predicted by the model. Outliers can have large effects on RSE and ",e.jsx(i,{children:"R^2"}),"."]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700 my-4",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Detection"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Examine ",e.jsx("em",{children:"studentized residuals"}),". Observations with studentized residuals greater than 3 in absolute value are potential outliers."]})]}),e.jsx("h3",{children:"5. High-Leverage Points"}),e.jsxs("p",{children:["Observations with unusual predictor values have ",e.jsx("em",{children:"high leverage"}),". They can have a large impact on the fitted regression line."]}),e.jsxs(s,{title:"Leverage Statistic",children:[e.jsxs("p",{children:["The leverage statistic ",e.jsx(i,{children:"h_i"})," quantifies the leverage of observation ",e.jsx(i,{children:"i"}),". It always lies between ",e.jsx(i,{children:"1/n"})," and 1, and the average leverage is ",e.jsx(i,{children:"(p+1)/n"}),"."]}),e.jsxs("p",{className:"mt-2",children:["Observations with ",e.jsx(i,{children:"h_i"})," greatly exceeding ",e.jsx(i,{children:"(p+1)/n"})," are high-leverage points."]})]}),e.jsx("h3",{children:"6. Collinearity"}),e.jsxs("p",{children:[e.jsx("em",{children:"Collinearity"})," refers to the situation in which two or more predictor variables are closely related to one another. It can make it difficult to determine the individual effects of collinear variables."]}),e.jsxs(s,{title:"Variance Inflation Factor (VIF)",children:[e.jsx("p",{children:"The VIF measures how much the variance of a coefficient is inflated due to collinearity:"}),e.jsx(t,{children:"\\text{VIF}(\\hat{\\beta}_j) = \\frac{1}{1 - R^2_{X_j | X_{-j}}}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(i,{children:"R^2_{X_j | X_{-j}}"})," is the ",e.jsx(i,{children:"R^2"})," from regressing ",e.jsx(i,{children:"X_j"})," onto all other predictors. A VIF of 1 indicates no collinearity; values exceeding 5 or 10 indicate problematic collinearity."]})]}),e.jsx(r,{title:"Diagnostic Plots in R",children:`# Fit model
lm.fit <- lm(sales ~ TV + radio + newspaper, data = Advertising)

# Diagnostic plots
par(mfrow = c(2, 2))
plot(lm.fit)

# Calculate VIF (requires car package)
library(car)
vif(lm.fit)

# Studentized residuals
rstudent(lm.fit)

# Leverage values
hatvalues(lm.fit)`}),e.jsx("h2",{children:"Summary"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsx("li",{children:"Qualitative predictors are included using dummy variables"}),e.jsx("li",{children:"Interaction terms model non-additive effects between predictors"}),e.jsx("li",{children:"Polynomial terms can capture non-linear relationships"}),e.jsx("li",{children:"Always check residual plots for violations of assumptions"}),e.jsx("li",{children:"Use VIF to detect collinearity among predictors"})]}),e.jsxs(a,{type:"success",children:[e.jsx("strong",{children:"Coming Up:"})," Next we'll examine a case study applying these concepts to a marketing problem, followed by a comparison of linear regression with K-nearest neighbors."]})]})}export{m as default};
