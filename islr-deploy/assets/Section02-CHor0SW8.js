import{j as e}from"./vendor-animation-0o8UKZ_1.js";import{L as n,C as s,S as o}from"./Callout-B_HVXLUb.js";import{D as r,E as a,T as l}from"./ContentBlocks-BGX5GcBB.js";import{I as i,M as t}from"./MathBlock-_bBfq7Jh.js";import"./vendor-react-Drj8qL0h.js";import"./index-Df8Jh5wn.js";import"./vendor-math-p018AHG0.js";import"./vendor-firebase-core-BXWtuYvb.js";const d=[{id:"s02-e01",type:"multiple-choice",question:"What type of error can potentially be reduced by using a better statistical model?",options:["Irreducible error","Reducible error","Random error","Measurement error"],correctIndex:1,difficulty:"easy",explanation:"Reducible error arises because $\\hat{f}$ is not a perfect estimate for $f$. We can reduce this by using more appropriate methods or more data."},{id:"s02-e02",type:"multiple-choice",question:"Parametric methods require us to:",options:["Have very large datasets","Avoid making any assumptions","Assume a functional form for $f$","Use only non-linear models"],correctIndex:2,difficulty:"easy",explanation:"Parametric methods first assume a functional form (like linear), then estimate the parameters of that form from data."},{id:"s02-e03",type:"multiple-choice",question:"In the linear model $f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$, how many parameters need to be estimated?",options:["1","2","4","3"],correctIndex:3,difficulty:"easy",explanation:"We need to estimate $\\beta_0$ (intercept), $\\beta_1$, and $\\beta_2$ - three parameters total."},{id:"s02-e04",type:"text",question:'What type of methods make no explicit assumptions about the functional form of $f$? (one word, starts with "non")',correctAnswer:"non-parametric",difficulty:"easy",explanation:"Non-parametric methods do not assume a specific form for $f$, instead seeking an estimate that fits the data flexibly."},{id:"s02-e05",type:"multiple-choice",question:"Even with a perfect estimate of $f$, we cannot predict $Y$ perfectly because of:",options:["Irreducible error","Reducible error","Training error","Model complexity"],correctIndex:0,difficulty:"easy",explanation:"Irreducible error from $\\epsilon$ cannot be reduced regardless of how well we estimate $f$, as it represents inherent randomness."},{id:"s02-m01",type:"multiple-choice",question:"Which is an advantage of parametric over non-parametric methods?",options:["They can fit any functional form","They require fewer observations to estimate $f$","They never make incorrect assumptions","They always have lower bias"],correctIndex:1,difficulty:"medium",explanation:"Parametric methods reduce the problem to estimating a fixed number of parameters, requiring less data than non-parametric methods that must estimate the entire function."},{id:"s02-m02",type:"multiple-choice",question:"A very flexible model typically has:",options:["High bias, low variance","High bias, high variance","Low bias, high variance","Low bias, low variance"],correctIndex:2,difficulty:"medium",explanation:"Flexible models can closely fit training data (low bias) but are sensitive to the specific data used (high variance)."},{id:"s02-m03",type:"multiple-choice",question:"Linear regression is less flexible than thin-plate splines. Which method is more interpretable?",options:["Thin-plate splines","They are equally interpretable","It depends on the data","Linear regression"],correctIndex:3,difficulty:"medium",explanation:"There is a trade-off between flexibility and interpretability. Linear regression, being less flexible, is more interpretable."},{id:"s02-m04",type:"multiple-choice",question:"The expected prediction error can be decomposed into:",options:["Bias squared + variance + irreducible error","Training error + test error","Bias + variance","Reducible error only"],correctIndex:0,difficulty:"medium",explanation:"$E[(Y - \\hat{Y})^2] = [\\text{Bias}(\\hat{f})]^2 + \\text{Var}(\\hat{f}) + \\text{Var}(\\epsilon)$"},{id:"s02-m05",type:"multiple-choice",question:"If we primarily need to understand which predictors affect the response, we should prefer:",options:["More flexible methods for better fit","Less flexible methods for better interpretability","Deep neural networks","Non-parametric methods"],correctIndex:1,difficulty:"medium",explanation:"For inference (understanding relationships), interpretability is crucial. Less flexible methods like linear regression make it easier to understand predictor effects."},{id:"s02-h01",type:"multiple-choice",question:"In parametric regression, assuming $f$ is linear when the true relationship is quadratic will result in:",options:["Increased variance","Decreased irreducible error","Increased bias","Better predictions"],correctIndex:2,difficulty:"hard",explanation:"Using an incorrect functional form (linear when true is quadratic) introduces bias - a systematic error that cannot be reduced by collecting more data."},{id:"s02-h02",type:"multiple-choice",question:"Why might a less flexible method outperform a more flexible method even when the true $f$ is non-linear?",options:["Less flexible methods always have lower error","Non-linear relationships cannot be modeled","Flexible methods always overfit","The reduction in variance may outweigh the increase in bias"],correctIndex:3,difficulty:"hard",explanation:"The bias-variance trade-off means that sometimes accepting some bias (from a simpler model) is worth the reduction in variance, especially with limited data."},{id:"s02-h03",type:"multiple-choice",question:"A non-parametric method requires more data than a parametric method primarily because:",options:["It must estimate the function at every point rather than just a few parameters","It must estimate many more parameters","Non-parametric methods are always more complex","Parametric methods use regularization"],correctIndex:0,difficulty:"hard",explanation:"Non-parametric methods effectively estimate $f$ at many points (or use local information), requiring more data than estimating a small fixed set of parameters."}];function j(){return e.jsxs(n,{sectionId:2,children:[e.jsx("h2",{children:"The Statistical Learning Framework"}),e.jsxs("p",{children:["Suppose we observe a quantitative response ",e.jsx(i,{children:"Y"})," and ",e.jsx(i,{children:"p"})," different predictors, ",e.jsx(i,{children:"X_1, X_2, \\ldots, X_p"}),". We assume that there is some relationship between ",e.jsx(i,{children:"Y"})," and ",e.jsx(i,{children:"X = (X_1, X_2, \\ldots, X_p)"}),", which can be written in the very general form:"]}),e.jsx(t,{children:"Y = f(X) + \\epsilon"}),e.jsxs(r,{title:"The Statistical Learning Model",children:[e.jsxs("p",{children:["Here ",e.jsx(i,{children:"f"})," is some fixed but unknown function of ",e.jsx(i,{children:"X_1, \\ldots, X_p"}),", and ",e.jsx(i,{children:"\\epsilon"})," is a random ",e.jsx("em",{children:"error term"}),", which is independent of ",e.jsx(i,{children:"X"})," and has mean zero."]}),e.jsxs("p",{className:"mt-2",children:["In this formulation, ",e.jsx(i,{children:"f"})," represents the ",e.jsx("em",{children:"systematic"})," information that ",e.jsx(i,{children:"X"})," provides about ",e.jsx(i,{children:"Y"}),"."]})]}),e.jsx("h2",{children:"Example: Income and Education"}),e.jsxs("p",{children:["Consider a study examining the relationship between years of education, seniority, and income. The response variable is ",e.jsx(i,{children:"Y = \\text{income}"}),", and the predictors are ",e.jsx(i,{children:"X_1 = \\text{years of education}"})," and ",e.jsx(i,{children:"X_2 = \\text{seniority}"}),"."]}),e.jsxs(a,{title:"Income Prediction",children:[e.jsx("p",{children:"Suppose we have collected data on 30 individuals. We might model income as:"}),e.jsx(t,{children:"\\text{Income} = f(\\text{Education}, \\text{Seniority}) + \\epsilon"}),e.jsxs("p",{className:"mt-2",children:["The function ",e.jsx(i,{children:"f"})," might be a surface in 3D space, where different combinations of education and seniority map to different expected incomes."]})]}),e.jsxs("h2",{children:["Why Estimate ",e.jsx(i,{children:"f"}),"?"]}),e.jsxs("p",{children:["There are two main reasons we may wish to estimate ",e.jsx(i,{children:"f"}),": ",e.jsx("strong",{children:"prediction"})," and ",e.jsx("strong",{children:"inference"}),". Depending on whether our ultimate goal is prediction, inference, or some combination of the two, different methods for estimating ",e.jsx(i,{children:"f"})," may be appropriate."]}),e.jsx("h3",{children:"Prediction"}),e.jsxs("p",{children:["In many situations, a set of inputs ",e.jsx(i,{children:"X"})," are readily available, but the output ",e.jsx(i,{children:"Y"})," cannot be easily obtained. In this setting, since the error term averages to zero, we can predict ",e.jsx(i,{children:"Y"})," using:"]}),e.jsx(t,{children:"\\hat{Y} = \\hat{f}(X)"}),e.jsxs("p",{children:["where ",e.jsx(i,{children:"\\hat f"})," represents our estimate for ",e.jsx(i,{children:"f"}),", and ",e.jsx(i,{children:"\\hat Y"})," represents the resulting prediction for ",e.jsx(i,{children:"Y"}),"."]}),e.jsxs(r,{title:"Reducible and Irreducible Error",children:[e.jsxs("p",{children:["The accuracy of ",e.jsx(i,{children:"\\hat Y"})," as a prediction for ",e.jsx(i,{children:"Y"})," depends on two quantities:"]}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"Reducible error:"})," The error that arises because ",e.jsx(i,{children:"\\hat f"})," is not a perfect estimate for ",e.jsx(i,{children:"f"}),". This can potentially be reduced by using a more appropriate statistical learning technique."]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Irreducible error:"})," The error that arises from ",e.jsx(i,{children:"\\epsilon"}),". Even if we could perfectly estimate ",e.jsx(i,{children:"f"}),", we could not perfectly predict ",e.jsx(i,{children:"Y"})," because ",e.jsx(i,{children:"\\epsilon"})," cannot be predicted using ",e.jsx(i,{children:"X"}),"."]})]})]}),e.jsxs("p",{children:["Why is the irreducible error larger than zero? The quantity ",e.jsx(i,{children:"\\epsilon"})," may contain unmeasured variables that are useful in predicting ",e.jsx(i,{children:"Y"}),": since we don't measure them, ",e.jsx(i,{children:"f"})," cannot use them for its prediction. It may also contain unmeasurable variation."]}),e.jsxs(l,{title:"Decomposition of Expected Prediction Error",children:[e.jsxs("p",{children:["Consider a given estimate ",e.jsx(i,{children:"\\hat f"})," and a set of predictors ",e.jsx(i,{children:"X"}),", which yields the prediction ",e.jsx(i,{children:"\\hat Y = \\hat f(X)"}),". Assuming ",e.jsx(i,{children:"\\hat f"})," and ",e.jsx(i,{children:"X"})," are fixed, we can show that:"]}),e.jsx(t,{children:"E[(Y - \\hat{Y})^2] = E[(f(X) + \\epsilon - \\hat{f}(X))^2]"}),e.jsx(t,{children:"= \\underbrace{[f(X) - \\hat{f}(X)]^2}_{\\text{Reducible}} + \\underbrace{\\text{Var}(\\epsilon)}_{\\text{Irreducible}}"})]}),e.jsxs(s,{type:"warning",children:[e.jsx("strong",{children:"Key Insight:"})," The irreducible error provides an upper bound on the accuracy of our prediction for ",e.jsx(i,{children:"Y"}),". This bound is almost always unknown in practice, but it reminds us that prediction will never be perfect."]}),e.jsx("h3",{children:"Inference"}),e.jsxs("p",{children:["We are often interested in understanding the ",e.jsx("em",{children:"relationship"})," between ",e.jsx(i,{children:"X"})," and ",e.jsx(i,{children:"Y"}),", rather than just predicting ",e.jsx(i,{children:"Y"}),". In this situation we wish to estimate ",e.jsx(i,{children:"f"}),", but our goal is not necessarily to make predictions."]}),e.jsx("p",{children:"We instead want to understand:"}),e.jsxs("div",{className:"space-y-3 my-6",children:[e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-dark-700",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Which predictors are associated with the response?"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["It is often the case that only a small fraction of the available predictors are substantially associated with ",e.jsx(i,{children:"Y"}),". Identifying these predictors is often valuable."]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-dark-700",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-2",children:"What is the relationship between the response and each predictor?"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Some predictors may have a positive relationship with ",e.jsx(i,{children:"Y"}),", while others have a negative relationship. The relationship may also depend on the values of other predictors."]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-dark-700",children:[e.jsx("h4",{className:"text-amber-400 font-semibold mb-2",children:"Can the relationship be summarized using a linear equation?"}),e.jsx("p",{className:"text-dark-300 text-sm",children:"Or is the relationship more complicated? Linear models are easier to interpret, but may not capture the true relationship accurately."})]})]}),e.jsxs("h2",{children:["How Do We Estimate ",e.jsx(i,{children:"f"}),"?"]}),e.jsxs("p",{children:["Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function ",e.jsx(i,{children:"f"}),". In broad terms, most statistical learning methods can be characterized as either ",e.jsx("em",{children:"parametric"})," or ",e.jsx("em",{children:"non-parametric"}),"."]}),e.jsx("h3",{children:"Parametric Methods"}),e.jsx("p",{children:"Parametric methods involve a two-step model-based approach:"}),e.jsxs("div",{className:"my-6 p-5 bg-gradient-to-br from-blue-500/10 to-blue-600/5 rounded-xl border border-blue-500/20",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-3",children:"Two-Step Parametric Approach"}),e.jsxs("ol",{className:"list-decimal list-inside space-y-3 text-dark-300",children:[e.jsxs("li",{children:[e.jsxs("strong",{children:["Make an assumption about the functional form of ",e.jsx(i,{children:"f"}),"."]}),e.jsxs("p",{className:"ml-6 mt-1 text-dark-400 text-sm",children:["For example, we might assume that ",e.jsx(i,{children:"f"})," is linear in ",e.jsx(i,{children:"X"}),":"]}),e.jsx("div",{className:"ml-6 mt-2",children:e.jsx(t,{children:"f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p"})})]}),e.jsxs("li",{children:[e.jsxs("strong",{children:["Use the training data to ",e.jsx("em",{children:"fit"})," or ",e.jsx("em",{children:"train"})," the model."]}),e.jsxs("p",{className:"ml-6 mt-1 text-dark-400 text-sm",children:["Estimate the parameters ",e.jsx(i,{children:"\\beta_0, \\beta_1, \\ldots, \\beta_p"})," such that:"]}),e.jsx("div",{className:"ml-6 mt-2",children:e.jsx(t,{children:"Y \\approx \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p"})})]})]})]}),e.jsxs(s,{type:"info",children:[e.jsx("strong",{children:"Advantage:"})," Estimating a set of parameters is much simpler than fitting an entirely arbitrary function ",e.jsx(i,{children:"f"}),".",e.jsx("br",{}),e.jsx("br",{}),e.jsx("strong",{children:"Disadvantage:"})," The model we choose will usually not match the true unknown form of ",e.jsx(i,{children:"f"}),". If the chosen model is too far from the true ",e.jsx(i,{children:"f"}),", our estimate will be poor."]}),e.jsx("h3",{children:"Non-Parametric Methods"}),e.jsxs("p",{children:["Non-parametric methods do not make explicit assumptions about the functional form of ",e.jsx(i,{children:"f"}),". Instead, they seek an estimate of ",e.jsx(i,{children:"f"})," that gets as close to the data points as possible without being too rough or wiggly."]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4 my-6",children:[e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-emerald-500/30",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Advantages"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsxs("li",{children:["Can accurately fit a wider range of shapes for ",e.jsx(i,{children:"f"})]}),e.jsx("li",{children:"No assumptions about functional form needed"}),e.jsx("li",{children:"More flexible than parametric approaches"})]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-amber-500/30",children:[e.jsx("h4",{className:"text-amber-400 font-semibold mb-2",children:"Disadvantages"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsx("li",{children:"Requires a very large number of observations"}),e.jsx("li",{children:"Can overfit if not carefully tuned"}),e.jsx("li",{children:"Harder to interpret than parametric models"})]})]})]}),e.jsx("h2",{children:"The Trade-Off Between Flexibility and Interpretability"}),e.jsxs("p",{children:["There is often a trade-off between ",e.jsx("em",{children:"flexibility"})," and ",e.jsx("em",{children:"interpretability"}),". Linear regression is relatively inflexible but very interpretable. Methods like thin-plate splines or neural networks are more flexible but harder to interpret."]}),e.jsxs("div",{className:"my-6 p-4 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-dark-200 font-semibold mb-3 text-center",children:"Flexibility vs. Interpretability Spectrum"}),e.jsxs("div",{className:"flex items-center justify-between text-sm",children:[e.jsxs("div",{className:"text-center",children:[e.jsx("div",{className:"text-emerald-400 font-semibold",children:"Low Flexibility"}),e.jsx("div",{className:"text-dark-400",children:"High Interpretability"}),e.jsx("div",{className:"text-dark-500 mt-1",children:"Subset Selection, Lasso"})]}),e.jsx("div",{className:"flex-1 mx-4 h-2 bg-gradient-to-r from-emerald-500 via-amber-500 to-red-500 rounded-full"}),e.jsxs("div",{className:"text-center",children:[e.jsx("div",{className:"text-red-400 font-semibold",children:"High Flexibility"}),e.jsx("div",{className:"text-dark-400",children:"Low Interpretability"}),e.jsx("div",{className:"text-dark-500 mt-1",children:"Neural Networks, SVMs"})]})]})]}),e.jsx("p",{children:"Why would we ever choose a more restrictive method instead of a very flexible approach? There are several reasons:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsxs("li",{children:["If we are mainly interested in ",e.jsx("strong",{children:"inference"}),", restrictive models are much more interpretable"]}),e.jsxs("li",{children:["Highly flexible methods can ",e.jsx("strong",{children:"overfit"})," the training data, performing poorly on new data"]}),e.jsx("li",{children:"Simple models often perform just as well as complex ones, especially with limited data"})]}),e.jsx("h2",{children:"Supervised vs. Unsupervised Learning"}),e.jsxs("p",{children:["Most statistical learning problems fall into one of two categories:",e.jsx("em",{children:"supervised"})," or ",e.jsx("em",{children:"unsupervised"}),"."]}),e.jsxs(r,{title:"Supervised Learning",children:[e.jsxs("p",{children:["For each observation of the predictor measurement(s) ",e.jsx(i,{children:"x_i"}),", ",e.jsx(i,{children:"i = 1, \\ldots, n"}),", there is an associated response measurement ",e.jsx(i,{children:"y_i"}),"."]}),e.jsx("p",{className:"mt-2",children:"We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference)."})]}),e.jsxs(r,{title:"Unsupervised Learning",children:[e.jsxs("p",{children:["For every observation ",e.jsx(i,{children:"i = 1, \\ldots, n"}),", we observe a vector of measurements ",e.jsx(i,{children:"x_i"})," but no associated response ",e.jsx(i,{children:"y_i"}),"."]}),e.jsx("p",{className:"mt-2",children:"We cannot fit a linear regression model since there is no response variable to predict. Instead, we seek to understand the relationships between the variables or between the observations."})]}),e.jsxs(a,{title:"Clustering",children:[e.jsxs("p",{children:["One important tool in unsupervised learning is ",e.jsx("em",{children:"cluster analysis"}),". The goal is to ascertain, on the basis of ",e.jsx(i,{children:"x_1, \\ldots, x_n"}),", whether the observations fall into relatively distinct groups."]}),e.jsx("p",{className:"mt-2",children:"For example, in a market segmentation study, we might observe multiple characteristics for potential customers and try to identify clusters of similar customers."})]}),e.jsx("h2",{children:"Regression vs. Classification Problems"}),e.jsxs("p",{children:["Variables can be characterized as either ",e.jsx("em",{children:"quantitative"})," or ",e.jsx("em",{children:"qualitative"}),"(also known as ",e.jsx("em",{children:"categorical"}),")."]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4 my-6",children:[e.jsxs("div",{className:"bg-gradient-to-br from-emerald-500/10 to-emerald-600/5 rounded-xl p-5 border border-emerald-500/20",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Quantitative Variables"}),e.jsx("p",{className:"text-dark-300 text-sm mb-2",children:"Take on numerical values. Examples:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-400 text-sm",children:[e.jsx("li",{children:"Age, height, income"}),e.jsx("li",{children:"Temperature, price"}),e.jsx("li",{children:"Number of items sold"})]})]}),e.jsxs("div",{className:"bg-gradient-to-br from-blue-500/10 to-blue-600/5 rounded-xl p-5 border border-blue-500/20",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-2",children:"Qualitative Variables"}),e.jsxs("p",{className:"text-dark-300 text-sm mb-2",children:["Take on values in one of ",e.jsx(i,{children:"K"})," different classes. Examples:"]}),e.jsxs("ul",{className:"list-disc list-inside text-dark-400 text-sm",children:[e.jsx("li",{children:"Gender (male/female)"}),e.jsx("li",{children:"Brand purchased"}),e.jsx("li",{children:"Cancer diagnosis (yes/no)"})]})]})]}),e.jsxs(r,{title:"Regression and Classification",children:[e.jsxs("p",{children:["Problems with a ",e.jsx("strong",{children:"quantitative response"})," are referred to as ",e.jsx("strong",{children:"regression"})," problems."]}),e.jsxs("p",{className:"mt-2",children:["Problems involving a ",e.jsx("strong",{children:"qualitative response"})," are often referred to as ",e.jsx("strong",{children:"classification"})," problems."]})]}),e.jsxs(s,{type:"success",children:[e.jsx("strong",{children:"Up Next:"})," In the next section, we'll explore how to assess the accuracy of statistical learning models, introducing the crucial concept of the bias-variance tradeoff."]}),e.jsx(o,{sectionId:2,questions:d,title:"Statistical Learning Quiz"})]})}export{j as default};
