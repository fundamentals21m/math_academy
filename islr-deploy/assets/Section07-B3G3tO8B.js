import{j as e}from"./vendor-animation-0o8UKZ_1.js";import{L as n,C as t,S as l}from"./Callout-DEw1ryYC.js";import{D as r,E as a,R as o}from"./ContentBlocks-BGX5GcBB.js";import{M as i,I as s}from"./MathBlock-CH3Cx_8w.js";import"./vendor-react-Drj8qL0h.js";import"./index-DMCNBwIs.js";import"./vendor-firebase-core-_7V-tLLm.js";import"./vendor-firebase-auth-Br51EKMN.js";import"./vendor-firebase-functions-Bu93Ly_7.js";import"./vendor-math-p018AHG0.js";const d=[{id:"s07-e01",type:"multiple-choice",question:"Multiple linear regression extends simple linear regression by:",options:["Using non-linear functions","Removing the intercept","Including multiple predictor variables","Fitting multiple separate models"],correctIndex:2,difficulty:"easy",explanation:"Multiple regression uses multiple predictors: $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon$"},{id:"s07-e02",type:"multiple-choice",question:"In multiple regression, $\\beta_j$ represents the effect of $X_j$ on $Y$:",options:["When all other predictors are zero","Holding all other predictors constant","Only when $X_j$ is large","Ignoring all other predictors"],correctIndex:1,difficulty:"easy",explanation:"Each coefficient represents the average change in $Y$ for a one-unit change in $X_j$, holding all other predictors fixed."},{id:"s07-e03",type:"multiple-choice",question:"The F-statistic tests whether:",options:["R-squared equals 1","All coefficients equal each other","At least one predictor is useful","The intercept is zero"],correctIndex:2,difficulty:"easy",explanation:"The F-test tests $H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$ - whether at least one predictor has a non-zero effect."},{id:"s07-e04",type:"text",question:"What happens to $R^2$ when we add more predictors to a model? (one word: increases/decreases/stays)",correctAnswer:"increases",difficulty:"easy",explanation:"$R^2$ always increases (or stays the same) when predictors are added, even if those predictors are not useful."},{id:"s07-e05",type:"multiple-choice",question:"Variable selection refers to:",options:["Choosing between regression and classification","Selecting the training data","Determining which predictors to include","Choosing the best response variable"],correctIndex:2,difficulty:"easy",explanation:"Variable selection helps identify which of the $p$ predictors are truly associated with the response."},{id:"s07-m01",type:"multiple-choice",question:"With $p = 10$ predictors, how many possible models can be formed using subsets?",options:["1024","100","1000","10"],correctIndex:0,difficulty:"medium",explanation:"Each predictor can be included or excluded: $2^{10} = 1024$ possible models."},{id:"s07-m02",type:"multiple-choice",question:"Why can a predictor be significant in simple regression but not in multiple regression?",options:["The predictor is correlated with other predictors","Multiple regression has fewer degrees of freedom","The response variable changed","Simple regression is always wrong"],correctIndex:0,difficulty:"medium",explanation:'Correlation among predictors means one predictor can "borrow" the effect of another in simple regression.'},{id:"s07-m03",type:"multiple-choice",question:"Adjusted $R^2$ differs from $R^2$ because it:",options:["Penalizes for adding predictors","Is always larger","Uses a different formula entirely","Only applies to simple regression"],correctIndex:0,difficulty:"medium",explanation:"Adjusted $R^2$ penalizes model complexity, so it can decrease when uninformative predictors are added."},{id:"s07-m04",type:"multiple-choice",question:"Forward selection starts by:",options:["Randomly selecting predictors","Including no predictors and adding one at a time","Testing all possible subsets","Including all predictors and removing one at a time"],correctIndex:1,difficulty:"medium",explanation:"Forward selection starts with no predictors (just intercept) and adds the most significant one at each step."},{id:"s07-m05",type:"numeric",question:"In the F-statistic formula $F = \\frac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n-p-1)}$, if TSS = 1000, RSS = 400, $p = 3$, and $n = 100$, what is F? (Round to nearest integer)",correctAnswer:48,numericRange:{min:1,max:100,precision:0},difficulty:"medium",explanation:"$F = \\frac{(1000-400)/3}{400/(100-3-1)} = \\frac{600/3}{400/96} = \\frac{200}{4.17} \\approx 48$"},{id:"s07-h01",type:"multiple-choice",question:"When predictors are highly correlated (collinear), which is true?",options:["All coefficients will be positive","The model cannot be fit","Individual coefficient p-values may be high even if F-test is significant","R-squared will be low"],correctIndex:2,difficulty:"hard",explanation:"Collinearity inflates standard errors of individual coefficients, making them appear non-significant even when the predictors together are useful."},{id:"s07-h02",type:"multiple-choice",question:"Why shouldn't we rely solely on individual t-tests when $p$ is large?",options:["The degrees of freedom are wrong","T-tests require normally distributed data","Some predictors will appear significant by chance alone","T-tests are only valid for small $p$"],correctIndex:2,difficulty:"hard",explanation:"With many predictors, about 5% will have p-values below 0.05 by chance. The F-test avoids this multiple testing problem."},{id:"s07-h03",type:"multiple-choice",question:"Backward selection is problematic when:",options:["$n$ is large","The F-statistic is significant","$R^2$ is high","$p > n$"],correctIndex:3,difficulty:"hard",explanation:"Backward selection starts with all predictors included. If $p > n$, we cannot fit the full model to start with."}];function y(){return e.jsxs(n,{sectionId:7,children:[e.jsx("h2",{children:"Multiple Linear Regression"}),e.jsx("p",{children:"Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. We now extend simple linear regression to accommodate multiple predictors."}),e.jsxs(r,{title:"Multiple Linear Regression Model",children:[e.jsx("p",{children:"Instead of fitting a separate simple linear regression model for each predictor, we can extend our model to include multiple predictors:"}),e.jsx(i,{children:"Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"X_j"})," represents the ",e.jsx(s,{children:"j"}),"th predictor and ",e.jsx(s,{children:"\\beta_j"})," quantifies the association between that predictor and the response."]}),e.jsxs("p",{className:"mt-2",children:["We interpret ",e.jsx(s,{children:"\\beta_j"})," as the ",e.jsx("em",{children:"average"})," effect on ",e.jsx(s,{children:"Y"})," of a one unit increase in ",e.jsx(s,{children:"X_j"}),", ",e.jsx("strong",{children:"holding all other predictors fixed"}),"."]})]}),e.jsx("h2",{children:"Estimating the Regression Coefficients"}),e.jsxs("p",{children:["As with simple linear regression, the coefficients ",e.jsx(s,{children:"\\beta_0, \\beta_1, \\ldots, \\beta_p"})," are unknown and must be estimated. Given estimates ",e.jsx(s,{children:"\\hat\\beta_0, \\hat\\beta_1, \\ldots, \\hat\\beta_p"}),", we can make predictions using:"]}),e.jsx(i,{children:"\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_p x_p"}),e.jsx("p",{children:"The parameters are estimated using the same least squares approach: we choose coefficients to minimize the residual sum of squares (RSS):"}),e.jsx(r,{title:"RSS for Multiple Regression",children:e.jsx(i,{children:"\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\left( y_i - \\hat{\\beta}_0 - \\sum_{j=1}^{p} \\hat{\\beta}_j x_{ij} \\right)^2"})}),e.jsxs(t,{type:"info",children:[e.jsx("strong",{children:"Matrix Formulation:"})," Unlike simple linear regression, the formulas for the least squares coefficient estimates in multiple regression are most conveniently expressed using matrix algebra. In matrix form: ",e.jsx(s,{children:"\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}"})]}),e.jsxs(a,{title:"Advertising Data with Multiple Predictors",children:[e.jsx("p",{children:"Using the advertising data, we can regress sales onto TV, radio, and newspaper:"}),e.jsx(i,{children:"\\widehat{\\text{sales}} = 2.939 + 0.046 \\times \\text{TV} + 0.189 \\times \\text{radio} - 0.001 \\times \\text{newspaper}"}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Interpretation:"})," A $1,000 increase in TV advertising is associated with an increase in sales of about 46 units, holding radio and newspaper fixed. Similarly, spending an additional $1,000 on radio advertising is associated with an increase of about 189 units."]})]}),e.jsx("h2",{children:"Some Important Questions"}),e.jsx("p",{children:"When we perform multiple regression, we usually want to answer several important questions:"}),e.jsxs("div",{className:"space-y-4 my-6",children:[e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"1. Is at least one predictor useful?"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["We test ",e.jsx(s,{children:"H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0"})," using the F-statistic."]})]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-2",children:"2. Which predictors are important?"}),e.jsx("p",{className:"text-dark-300 text-sm",children:"Examine individual t-statistics and p-values for each coefficient."})]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-amber-400 font-semibold mb-2",children:"3. How well does the model fit?"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Use ",e.jsx(s,{children:"R^2"})," and RSE to assess overall model quality."]})]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-purple-400 font-semibold mb-2",children:"4. How accurate are predictions?"}),e.jsx("p",{className:"text-dark-300 text-sm",children:"Compute confidence intervals and prediction intervals."})]})]}),e.jsx("h3",{children:"The F-Statistic"}),e.jsx("p",{children:"To test whether at least one predictor is useful, we use the F-statistic:"}),e.jsxs(r,{title:"F-Statistic",children:[e.jsx(i,{children:"F = \\frac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n - p - 1)}"}),e.jsxs("p",{className:"mt-2",children:["where TSS = ",e.jsx(s,{children:"\\sum(y_i - \\bar{y})^2"})," and RSS = ",e.jsx(s,{children:"\\sum(y_i - \\hat{y}_i)^2"}),"."]}),e.jsxs("p",{className:"mt-2",children:["If ",e.jsx(s,{children:"H_0"})," is true and the errors are normally distributed, ",e.jsx(s,{children:"F"})," follows an F-distribution with ",e.jsx(s,{children:"(p, n-p-1)"})," degrees of freedom."]})]}),e.jsxs(t,{type:"warning",children:[e.jsx("strong",{children:"Why not just use individual t-tests?"})," When ",e.jsx(s,{children:"p"})," is large, about 5% of p-values will be below 0.05 by chance alone! The F-test avoids this multiple testing problem by testing all coefficients simultaneously."]}),e.jsx("h3",{children:"Deciding Which Variables to Include"}),e.jsxs("p",{children:["The task of determining which predictors are associated with the response is referred to as ",e.jsx("em",{children:"variable selection"}),". There are ",e.jsx(s,{children:"2^p"})," possible models containing subsets of the ",e.jsx(s,{children:"p"})," predictors!"]}),e.jsx("p",{children:"Common approaches include:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"Forward selection:"})," Start with no predictors, add one at a time"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Backward selection:"})," Start with all predictors, remove one at a time"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Mixed selection:"})," Combination of forward and backward"]})]}),e.jsx("h2",{children:"Model Fit"}),e.jsx("h3",{children:"R² in Multiple Regression"}),e.jsxs(r,{title:"R² for Multiple Regression",children:[e.jsx(i,{children:"R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}}"}),e.jsxs("p",{className:"mt-2",children:["As in simple regression, ",e.jsx(s,{children:"R^2"})," measures the proportion of variance explained by the model. However, ",e.jsx(s,{children:"R^2"})," will ",e.jsx("em",{children:"always"})," increase when more variables are added, even if those variables are not truly associated with the response!"]})]}),e.jsxs(t,{type:"info",children:[e.jsx("strong",{children:"Adjusted R²:"})," To account for the number of predictors, we can use",e.jsx(s,{children:"\\text{Adjusted } R^2 = 1 - \\frac{\\text{RSS}/(n-p-1)}{\\text{TSS}/(n-1)}"}),". Unlike ",e.jsx(s,{children:"R^2"}),", adjusted ",e.jsx(s,{children:"R^2"})," can decrease if we add uninformative predictors."]}),e.jsxs(a,{title:"Advertising Model Comparison",children:[e.jsx("div",{className:"overflow-x-auto my-4",children:e.jsxs("table",{className:"w-full text-sm",children:[e.jsx("thead",{children:e.jsxs("tr",{className:"border-b border-dark-700",children:[e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"Model"}),e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"RSE"}),e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"R²"})]})}),e.jsxs("tbody",{className:"text-dark-400",children:[e.jsxs("tr",{className:"border-b border-dark-800",children:[e.jsx("td",{className:"py-2 px-3",children:"TV only"}),e.jsx("td",{className:"py-2 px-3",children:"3.26"}),e.jsx("td",{className:"py-2 px-3",children:"0.612"})]}),e.jsxs("tr",{className:"border-b border-dark-800",children:[e.jsx("td",{className:"py-2 px-3",children:"TV + Radio"}),e.jsx("td",{className:"py-2 px-3",children:"1.68"}),e.jsx("td",{className:"py-2 px-3",children:"0.897"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-2 px-3",children:"TV + Radio + Newspaper"}),e.jsx("td",{className:"py-2 px-3",children:"1.69"}),e.jsx("td",{className:"py-2 px-3",children:"0.897"})]})]})]})}),e.jsxs("p",{className:"text-dark-400 text-sm",children:["Adding newspaper to the model barely improves ",e.jsx(s,{children:"R^2"})," and actually increases RSE slightly!"]})]}),e.jsx("h2",{children:"R Code Example"}),e.jsx(o,{title:"Multiple Regression in R",output:`Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.93889    0.31191   9.422  < 2e-16 ***
TV           0.04576    0.00139  32.809  < 2e-16 ***
radio        0.18853    0.00861  21.893  < 2e-16 ***
newspaper   -0.00104    0.00587  -0.177    0.860    

Residual standard error: 1.686 on 196 degrees of freedom
Multiple R-squared:  0.8972,	Adjusted R-squared:  0.8956 
F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16`,children:`# Fit multiple regression model
lm.fit <- lm(sales ~ TV + radio + newspaper, data = Advertising)

# View the summary
summary(lm.fit)

# Get confidence intervals
confint(lm.fit)

# Compare models using anova
lm.fit1 <- lm(sales ~ TV + radio, data = Advertising)
anova(lm.fit1, lm.fit)`}),e.jsx("h2",{children:"Correlation Among Predictors"}),e.jsxs("p",{children:["In multiple regression, the coefficients can behave very differently than in simple regression due to ",e.jsx("em",{children:"correlation"})," (or ",e.jsx("em",{children:"collinearity"}),") among predictors."]}),e.jsxs(a,{title:"Simple vs. Multiple Regression Coefficients",children:[e.jsx("div",{className:"overflow-x-auto my-4",children:e.jsxs("table",{className:"w-full text-sm",children:[e.jsx("thead",{children:e.jsxs("tr",{className:"border-b border-dark-700",children:[e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"Variable"}),e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"Simple Regression"}),e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"Multiple Regression"})]})}),e.jsxs("tbody",{className:"text-dark-400",children:[e.jsxs("tr",{className:"border-b border-dark-800",children:[e.jsx("td",{className:"py-2 px-3",children:"TV"}),e.jsx("td",{className:"py-2 px-3",children:"0.048 ***"}),e.jsx("td",{className:"py-2 px-3",children:"0.046 ***"})]}),e.jsxs("tr",{className:"border-b border-dark-800",children:[e.jsx("td",{className:"py-2 px-3",children:"Radio"}),e.jsx("td",{className:"py-2 px-3",children:"0.203 ***"}),e.jsx("td",{className:"py-2 px-3",children:"0.189 ***"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-2 px-3",children:"Newspaper"}),e.jsx("td",{className:"py-2 px-3 text-amber-400",children:"0.055 ***"}),e.jsx("td",{className:"py-2 px-3 text-dark-500",children:"-0.001"})]})]})]})}),e.jsx("p",{className:"text-dark-400 text-sm",children:'Newspaper appears significant in simple regression but not in multiple regression! This is because newspaper spending is correlated with radio spending. The simple regression coefficient for newspaper is "borrowing" the effect of radio.'})]}),e.jsxs(t,{type:"warning",children:[e.jsx("strong",{children:"Interpreting Correlated Predictors:"})," When predictors are correlated, it can be difficult to determine the individual contribution of each. The coefficient for one variable represents its effect ",e.jsx("em",{children:"holding others constant"}),", which may not reflect real-world scenarios where variables change together."]}),e.jsx("h2",{children:"Summary"}),e.jsx("p",{children:"Multiple linear regression extends simple linear regression to include multiple predictors:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsxs("li",{children:["Each coefficient ",e.jsx(s,{children:"\\beta_j"})," represents the effect of ",e.jsx(s,{children:"X_j"})," holding other predictors fixed"]}),e.jsx("li",{children:"The F-statistic tests whether at least one predictor is useful"}),e.jsxs("li",{children:[e.jsx(s,{children:"R^2"})," always increases with more predictors; use adjusted ",e.jsx(s,{children:"R^2"})," for comparison"]}),e.jsx("li",{children:"Correlation among predictors can make interpretation challenging"}),e.jsx("li",{children:"Variable selection helps identify the most important predictors"})]}),e.jsxs(t,{type:"success",children:[e.jsx("strong",{children:"Next:"})," We'll explore other important considerations in regression, including qualitative predictors, interaction effects, and potential problems like non-linearity and outliers."]}),e.jsx(l,{sectionId:7,questions:d,title:"Multiple Regression Quiz"})]})}export{y as default};
