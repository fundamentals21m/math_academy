import{j as e}from"./index-CsNFfZHI.js";import{L as n,C as i}from"./Callout-D30c_Ud6.js";import{A as a,E as r,T as l}from"./ContentBlocks-D_yuWJNS.js";import{a as s,M as t}from"./MathBlock-CllSYCcl.js";function x(){return e.jsxs(n,{sectionId:10,children:[e.jsx("h2",{children:"Comparison of Linear Regression with K-Nearest Neighbors"}),e.jsxs("p",{children:["Linear regression is an example of a ",e.jsx("em",{children:"parametric"})," approach—we assume a specific functional form (linear) for ",e.jsx(s,{children:"f(X)"}),". An alternative is a ",e.jsx("em",{children:"non-parametric"})," approach, which makes fewer assumptions about the form of ",e.jsx(s,{children:"f"}),". One of the simplest non-parametric methods is ",e.jsx("strong",{children:"K-nearest neighbors"})," (KNN) regression."]}),e.jsx("h2",{children:"K-Nearest Neighbors Regression"}),e.jsxs("p",{children:["KNN regression identifies the ",e.jsx(s,{children:"K"})," training observations that are closest to a test observation ",e.jsx(s,{children:"x_0"})," and estimates ",e.jsx(s,{children:"f(x_0)"})," using the average of their response values."]}),e.jsxs(a,{title:"KNN Regression",children:[e.jsxs("p",{children:["Given a value for ",e.jsx(s,{children:"K"})," and a prediction point ",e.jsx(s,{children:"x_0"}),":"]}),e.jsxs("ol",{className:"list-decimal list-inside mt-2 space-y-2",children:[e.jsxs("li",{children:["Identify the ",e.jsx(s,{children:"K"})," training observations closest to ",e.jsx(s,{children:"x_0"})," (call this set ",e.jsx(s,{children:"\\mathcal{N}_0"}),")"]}),e.jsxs("li",{children:["Estimate ",e.jsx(s,{children:"f(x_0)"})," as the average of the responses in ",e.jsx(s,{children:"\\mathcal{N}_0"}),":",e.jsx(t,{children:"\\hat{f}(x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_0} y_i"})]})]})]}),e.jsx("h3",{children:"Choosing K"}),e.jsxs("p",{children:["The choice of ",e.jsx(s,{children:"K"})," controls the flexibility of the KNN fit:"]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4 my-6",children:[e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-blue-500/30",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-2",children:"Small K (e.g., K = 1)"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsx("li",{children:"Very flexible fit"}),e.jsx("li",{children:"Low bias, high variance"}),e.jsx("li",{children:"Rough, jagged predictions"}),e.jsx("li",{children:"Risk of overfitting"})]})]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-amber-500/30",children:[e.jsx("h4",{className:"text-amber-400 font-semibold mb-2",children:"Large K (e.g., K = 100)"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsx("li",{children:"Inflexible fit"}),e.jsx("li",{children:"High bias, low variance"}),e.jsx("li",{children:"Smooth predictions"}),e.jsx("li",{children:"Risk of underfitting"})]})]})]}),e.jsx("h2",{children:"Comparing KNN and Linear Regression"}),e.jsxs("p",{children:["Which method is better? The answer depends on the true relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),"."]}),e.jsx("h3",{children:"When Linear Regression Wins"}),e.jsxs(r,{title:"True Linear Relationship",children:[e.jsx("p",{children:"When the true relationship is linear (or close to linear), linear regression will outperform KNN because:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 mt-2 space-y-1",children:[e.jsxs("li",{children:["Linear regression makes the correct assumption about ",e.jsx(s,{children:"f"})]}),e.jsx("li",{children:"KNN estimates are inherently more variable"}),e.jsx("li",{children:"Linear regression uses all data points to estimate the relationship"})]}),e.jsx("p",{className:"mt-2 text-dark-400 text-sm",children:"Even with slight non-linearity, linear regression often performs comparably because its lower variance offsets its higher bias."})]}),e.jsx("h3",{children:"When KNN Wins"}),e.jsxs(r,{title:"Highly Non-Linear Relationship",children:[e.jsx("p",{children:"When the true relationship is substantially non-linear:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 mt-2 space-y-1",children:[e.jsx("li",{children:"Linear regression's bias dominates, leading to poor predictions"}),e.jsx("li",{children:"KNN can adapt to the non-linearity"}),e.jsxs("li",{children:["With enough data, KNN can closely approximate any ",e.jsx(s,{children:"f"})]})]}),e.jsx("p",{className:"mt-2 text-dark-400 text-sm",children:"However, KNN requires more data to achieve good performance in non-linear settings."})]}),e.jsx("h2",{children:"The Curse of Dimensionality"}),e.jsxs("p",{children:["A major limitation of KNN is that it suffers from the ",e.jsx("em",{children:"curse of dimensionality"}),". As the number of predictors ",e.jsx(s,{children:"p"})," grows, the performance of KNN rapidly deteriorates."]}),e.jsxs(l,{title:"The Curse of Dimensionality",children:[e.jsxs("p",{children:['In high dimensions, even a large training set may not provide many "nearby" observations. As ',e.jsx(s,{children:"p"})," increases:"]}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[e.jsx("li",{children:"The nearest neighbors become increasingly far away"}),e.jsx("li",{children:"These neighbors may not be very similar to the test point"}),e.jsx("li",{children:"The bias of KNN increases"})]}),e.jsxs("p",{className:"mt-2",children:["With ",e.jsx(s,{children:"p = 20"})," predictors, KNN often performs poorly even with large ",e.jsx(s,{children:"n"}),"."]})]}),e.jsxs("div",{className:"my-6 p-5 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-dark-200 font-semibold mb-3",children:"Intuition: Why Does Dimension Hurt KNN?"}),e.jsx("p",{className:"text-dark-300 text-sm mb-3",children:'Imagine you want 10% of the data to be "near" a test point:'}),e.jsxs("ul",{className:"space-y-2 text-dark-400 text-sm",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"p = 1:"})," Need 10% of the range of each predictor → side length = 0.10"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"p = 2:"})," Need ",e.jsxs(s,{children:["\\sqrt","{0.10}"]})," = 0.32 of each dimension"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"p = 10:"})," Need ",e.jsxs(s,{children:["0.10^","{1/10}"]})," = 0.79 of each dimension!"]})]}),e.jsx("p",{className:"mt-3 text-dark-500 text-sm",children:`In high dimensions, to get any neighbors at all, we must look over almost the entire range of each predictor—the neighbors aren't really "near"!`})]}),e.jsx("h2",{children:"Practical Implications"}),e.jsxs("div",{className:"space-y-4 my-6",children:[e.jsxs("div",{className:"p-4 bg-gradient-to-br from-emerald-500/10 to-emerald-600/5 rounded-xl border border-emerald-500/20",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Use Linear Regression When:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsx("li",{children:"The relationship is approximately linear"}),e.jsxs("li",{children:["You have many predictors (high ",e.jsx(s,{children:"p"}),")"]}),e.jsx("li",{children:"Interpretability is important"}),e.jsx("li",{children:"You have limited training data"}),e.jsx("li",{children:"You need to understand which predictors matter"})]})]}),e.jsxs("div",{className:"p-4 bg-gradient-to-br from-blue-500/10 to-blue-600/5 rounded-xl border border-blue-500/20",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-2",children:"Consider KNN When:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsx("li",{children:"The relationship is highly non-linear"}),e.jsxs("li",{children:["You have few predictors (small ",e.jsx(s,{children:"p"}),")"]}),e.jsx("li",{children:"You have abundant training data"}),e.jsx("li",{children:"Interpretability is less important than prediction accuracy"}),e.jsx("li",{children:"You don't want to specify a functional form"})]})]})]}),e.jsx("h2",{children:"A Simulation Study"}),e.jsx("p",{children:"Consider a comparison where we generate data from different true functions:"}),e.jsx("div",{className:"my-6 overflow-x-auto",children:e.jsxs("table",{className:"w-full text-sm",children:[e.jsx("thead",{children:e.jsxs("tr",{className:"border-b border-dark-700",children:[e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"True Function"}),e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"Linear Reg MSE"}),e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"KNN-1 MSE"}),e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"KNN-10 MSE"}),e.jsx("th",{className:"text-left py-2 px-3 text-dark-300",children:"Winner"})]})}),e.jsxs("tbody",{className:"text-dark-400",children:[e.jsxs("tr",{className:"border-b border-dark-800",children:[e.jsx("td",{className:"py-2 px-3",children:"Linear"}),e.jsx("td",{className:"py-2 px-3 text-emerald-400",children:"1.0"}),e.jsx("td",{className:"py-2 px-3",children:"1.8"}),e.jsx("td",{className:"py-2 px-3",children:"1.3"}),e.jsx("td",{className:"py-2 px-3 text-emerald-400",children:"Linear Regression"})]}),e.jsxs("tr",{className:"border-b border-dark-800",children:[e.jsx("td",{className:"py-2 px-3",children:"Slightly Non-linear"}),e.jsx("td",{className:"py-2 px-3 text-emerald-400",children:"1.2"}),e.jsx("td",{className:"py-2 px-3",children:"2.0"}),e.jsx("td",{className:"py-2 px-3",children:"1.5"}),e.jsx("td",{className:"py-2 px-3 text-emerald-400",children:"Linear Regression"})]}),e.jsxs("tr",{className:"border-b border-dark-800",children:[e.jsx("td",{className:"py-2 px-3",children:"Moderately Non-linear"}),e.jsx("td",{className:"py-2 px-3",children:"2.5"}),e.jsx("td",{className:"py-2 px-3",children:"2.2"}),e.jsx("td",{className:"py-2 px-3 text-blue-400",children:"1.8"}),e.jsx("td",{className:"py-2 px-3 text-blue-400",children:"KNN-10"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-2 px-3",children:"Highly Non-linear"}),e.jsx("td",{className:"py-2 px-3",children:"5.0"}),e.jsx("td",{className:"py-2 px-3 text-blue-400",children:"1.5"}),e.jsx("td",{className:"py-2 px-3",children:"2.0"}),e.jsx("td",{className:"py-2 px-3 text-blue-400",children:"KNN-1"})]})]})]})}),e.jsxs(i,{type:"info",children:[e.jsx("strong",{children:"Key Insight:"})," Even when the true relationship is somewhat non-linear, linear regression can perform well due to its lower variance. KNN only wins decisively when the non-linearity is substantial and we have enough data."]}),e.jsx("h2",{children:"Summary"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"Linear regression"})," is parametric: it assumes a linear functional form and estimates a fixed number of parameters"]}),e.jsxs("li",{children:[e.jsx("strong",{children:"KNN regression"})," is non-parametric: it makes no assumptions about the form of ",e.jsx(s,{children:"f"})]}),e.jsxs("li",{children:["Linear regression tends to outperform KNN when the relationship is linear or when ",e.jsx(s,{children:"p"})," is large (curse of dimensionality)"]}),e.jsxs("li",{children:["KNN can outperform linear regression when the relationship is highly non-linear and ",e.jsx(s,{children:"p"})," is small"]}),e.jsxs("li",{children:["The optimal choice of ",e.jsx(s,{children:"K"})," in KNN involves a bias-variance tradeoff"]})]}),e.jsxs(i,{type:"success",children:[e.jsx("strong",{children:"Chapter Complete!"})," You've now learned the fundamentals of linear regression—from simple regression with one predictor to multiple regression with interactions and diagnostics. In Chapter 4, we'll explore classification methods for qualitative response variables."]})]})}export{x as default};
