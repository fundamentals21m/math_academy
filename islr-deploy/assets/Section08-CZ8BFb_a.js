import{j as e}from"./vendor-animation-0o8UKZ_1.js";import{L as n,C as r,S as o}from"./Callout-DEw1ryYC.js";import{D as s,R as a,E as l}from"./ContentBlocks-BGX5GcBB.js";import{M as t,I as i}from"./MathBlock-CH3Cx_8w.js";import"./vendor-react-Drj8qL0h.js";import"./index-DMCNBwIs.js";import"./vendor-firebase-core-_7V-tLLm.js";import"./vendor-firebase-auth-Br51EKMN.js";import"./vendor-firebase-functions-Bu93Ly_7.js";import"./vendor-math-p018AHG0.js";const c=[{id:"s08-e01",type:"multiple-choice",question:"A dummy variable is used to represent:",options:["Qualitative (categorical) predictors","Missing values","The error term","The intercept"],correctIndex:0,difficulty:"easy",explanation:"Dummy variables (indicator variables) encode categorical predictors as 0/1 values for use in regression."},{id:"s08-e02",type:"multiple-choice",question:"For a categorical variable with 4 levels, how many dummy variables are needed?",options:["3","4","2","5"],correctIndex:0,difficulty:"easy",explanation:"With $k$ levels, we need $k-1$ dummy variables. One level serves as the baseline (intercept)."},{id:"s08-e03",type:"multiple-choice",question:"An interaction term in regression allows:",options:["Missing data imputation","Non-linear transformations of Y","The effect of one predictor to depend on another","The intercept to vary"],correctIndex:2,difficulty:"easy",explanation:"Interaction terms like $X_1 \\times X_2$ allow the effect of $X_1$ on $Y$ to change depending on the value of $X_2$."},{id:"s08-e04",type:"text",question:"What do we call the situation when error variance is not constant? (one word)",correctAnswer:"heteroscedasticity",difficulty:"easy",explanation:"Heteroscedasticity occurs when $\\text{Var}(\\epsilon_i)$ is not constant across observations."},{id:"s08-e05",type:"multiple-choice",question:"A residual plot is used to detect:",options:["The number of predictors","The sample size","The value of $R^2$","Problems like non-linearity or heteroscedasticity"],correctIndex:3,difficulty:"easy",explanation:"Residual plots (residuals vs. fitted values) reveal patterns that indicate model violations."},{id:"s08-m01",type:"multiple-choice",question:"In the model $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon$, the effect of $X_1$ on $Y$ is:",options:["$\\beta_1 + \\beta_2$","$\\beta_1 + \\beta_3 X_2$","$\\beta_1$","$\\beta_3$"],correctIndex:1,difficulty:"medium",explanation:"With the interaction, the effect of $X_1$ depends on $X_2$: $\\frac{\\partial Y}{\\partial X_1} = \\beta_1 + \\beta_3 X_2$."},{id:"s08-m02",type:"multiple-choice",question:"Polynomial regression $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$ is:",options:["Only valid for binary outcomes","Always overfits","A non-linear model","A linear model (in the parameters)"],correctIndex:3,difficulty:"medium",explanation:"Despite modeling a curved relationship, the model is linear in the coefficients $\\beta_0, \\beta_1, \\beta_2$."},{id:"s08-m03",type:"multiple-choice",question:"VIF (Variance Inflation Factor) measures:",options:["Non-linearity in the data","The size of residuals","Model accuracy","Collinearity among predictors"],correctIndex:3,difficulty:"medium",explanation:"VIF quantifies how much the variance of a coefficient is inflated due to correlation with other predictors."},{id:"s08-m04",type:"multiple-choice",question:"A funnel-shaped residual plot (residuals spread out as fitted values increase) indicates:",options:["Outliers","Non-linearity","Perfect fit","Heteroscedasticity"],correctIndex:3,difficulty:"medium",explanation:"A funnel shape means variance increases with the response level - classic heteroscedasticity."},{id:"s08-m05",type:"multiple-choice",question:"The hierarchical principle says that if we include an interaction $X_1 X_2$, we should:",options:["Add higher-order terms like $X_1^2$","Remove the main effects $X_1$ and $X_2$","Use a different response variable","Include the main effects $X_1$ and $X_2$"],correctIndex:3,difficulty:"medium",explanation:"Main effects should be included even if not significant, because the interaction only makes sense in their context."},{id:"s08-h01",type:"multiple-choice",question:"An observation with high leverage but small residual:",options:["Should always be removed","Has little effect on the fit","Pulls the regression line toward itself","Indicates model misspecification"],correctIndex:2,difficulty:"hard",explanation:"High leverage points with small residuals fit well BECAUSE they pull the line toward themselves - they strongly influence the fit."},{id:"s08-h02",type:"multiple-choice",question:"If VIF for $X_1$ is 10, approximately what proportion of $X_1$'s variance is explained by other predictors?",options:["99%","10%","50%","90%"],correctIndex:3,difficulty:"hard",explanation:"VIF = $\\frac{1}{1-R^2_{X_1|X_{-1}}}$. If VIF = 10, then $R^2_{X_1|X_{-1}} = 1 - 0.1 = 0.9$ (90%)."},{id:"s08-h03",type:"multiple-choice",question:"Studentized residuals greater than 3 in absolute value suggest:",options:["Collinearity","Potential outliers","Perfect predictions","High leverage"],correctIndex:1,difficulty:"hard",explanation:"Studentized residuals standardize residuals by their estimated standard deviation. Values beyond $\\pm 3$ are unusual."},{id:"s08-h04",type:"multiple-choice",question:"To address heteroscedasticity, a common transformation of the response is:",options:["Squaring Y","Taking log(Y) or sqrt(Y)","Standardizing Y","Removing Y"],correctIndex:1,difficulty:"hard",explanation:"Log or square root transformations often stabilize variance, particularly when variance increases with the mean."}];function y(){return e.jsxs(n,{sectionId:8,children:[e.jsx("h2",{children:"Other Considerations in the Regression Model"}),e.jsxs("p",{children:["So far we have assumed that the predictors are quantitative. But often some predictors are ",e.jsx("em",{children:"qualitative"}),". We also need to consider extensions like interactions and potential problems that can arise in regression."]}),e.jsx("h2",{children:"Qualitative Predictors"}),e.jsxs("p",{children:["Qualitative variables, also called ",e.jsx("em",{children:"categorical variables"})," or ",e.jsx("em",{children:"factors"}),", take on discrete values. Examples include gender, region, or brand."]}),e.jsx("h3",{children:"Two-Level Qualitative Variables"}),e.jsxs("p",{children:["For a qualitative variable with two levels (e.g., male/female), we create a",e.jsx("em",{children:"dummy variable"})," (or ",e.jsx("em",{children:"indicator variable"}),"):"]}),e.jsxs(s,{title:"Dummy Variable",children:[e.jsx(t,{children:"x_i = \\begin{cases} 1 & \\text{if } i\\text{th person is female} \\\\ 0 & \\text{if } i\\text{th person is male} \\end{cases}"}),e.jsx("p",{className:"mt-2",children:"The resulting model is:"}),e.jsx(t,{children:"y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i = \\begin{cases} \\beta_0 + \\beta_1 + \\epsilon_i & \\text{if female} \\\\ \\beta_0 + \\epsilon_i & \\text{if male} \\end{cases}"}),e.jsxs("p",{className:"mt-2",children:["Here ",e.jsx(i,{children:"\\beta_0"})," is the average value for males (the ",e.jsx("em",{children:"baseline"}),"), and ",e.jsx(i,{children:"\\beta_1"})," is the difference between females and males."]})]}),e.jsx("h3",{children:"Qualitative Variables with More Than Two Levels"}),e.jsx("p",{children:"When a qualitative predictor has more than two levels, we create multiple dummy variables. For example, for ethnicity with three levels (Asian, Caucasian, African American):"}),e.jsx(t,{children:"x_{i1} = \\begin{cases} 1 & \\text{if Asian} \\\\ 0 & \\text{otherwise} \\end{cases} \\quad x_{i2} = \\begin{cases} 1 & \\text{if Caucasian} \\\\ 0 & \\text{otherwise} \\end{cases}"}),e.jsx("p",{children:"The model becomes:"}),e.jsx(t,{children:"y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i"}),e.jsxs(r,{type:"info",children:[e.jsx("strong",{children:"Baseline Category:"})," With ",e.jsx(i,{children:"k"})," levels, we create ",e.jsx(i,{children:"k-1"})," dummy variables. The level without a dummy variable is the ",e.jsx("em",{children:"baseline"}),". The coefficients represent differences from this baseline."]}),e.jsx(a,{title:"Qualitative Predictors in R",output:`Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        5.7066     0.5314  10.739  < 2e-16 ***
ShelveLoc Good     4.8487     0.1528  31.724  < 2e-16 ***
ShelveLoc Medium   1.9533     0.1258  15.531  < 2e-16 ***`,children:`# R automatically creates dummy variables for factors
lm.fit <- lm(Sales ~ ShelveLoc, data = Carseats)
summary(lm.fit)

# ShelveLoc has 3 levels: Bad, Good, Medium
# Bad is the baseline (intercept)
# Coefficients show difference from Bad`}),e.jsx("h2",{children:"Interaction Terms"}),e.jsxs("p",{children:["The standard linear model assumes that the effect of one predictor on the response is",e.jsx("em",{children:"additive"}),"—it doesn't depend on the values of other predictors. But this is often unrealistic."]}),e.jsxs(s,{title:"Interaction Effect",children:[e.jsxs("p",{children:["An ",e.jsx("strong",{children:"interaction"})," occurs when the effect of one predictor on the response depends on the value of another predictor. We include an interaction term:"]}),e.jsx(t,{children:"Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon"}),e.jsx("p",{className:"mt-2",children:"This can be rewritten as:"}),e.jsx(t,{children:"Y = \\beta_0 + (\\beta_1 + \\beta_3 X_2) X_1 + \\beta_2 X_2 + \\epsilon"}),e.jsxs("p",{className:"mt-2",children:["The effect of ",e.jsx(i,{children:"X_1"})," on ",e.jsx(i,{children:"Y"})," is now ",e.jsx(i,{children:"\\beta_1 + \\beta_3 X_2"}),", which depends on ",e.jsx(i,{children:"X_2"}),"."]})]}),e.jsxs(l,{title:"TV and Radio Interaction",children:[e.jsxs("p",{children:["In the advertising data, there may be ",e.jsx("em",{children:"synergy"})," between TV and radio advertising. Spending on radio might be more effective when TV spending is also high."]}),e.jsx(t,{children:"\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{TV} \\times \\text{radio}) + \\epsilon"}),e.jsxs("p",{className:"mt-2",children:["The coefficient ",e.jsx(i,{children:"\\beta_3"})," for the interaction term is positive and highly significant, confirming that TV and radio advertising are more effective together than the sum of their individual effects."]})]}),e.jsx(a,{title:"Interaction in R",output:`Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) 6.750e+00  2.479e-01  27.233  < 2e-16 ***
TV          1.910e-02  1.504e-03  12.699  < 2e-16 ***
radio       2.886e-02  8.905e-03   3.241  0.00142 ** 
TV:radio    1.086e-03  5.242e-05  20.727  < 2e-16 ***

Multiple R-squared:  0.9678`,children:`# Include interaction term
lm.fit <- lm(sales ~ TV * radio, data = Advertising)
summary(lm.fit)

# TV * radio is shorthand for TV + radio + TV:radio
# The interaction is highly significant!`}),e.jsxs(r,{type:"warning",children:[e.jsx("strong",{children:"Hierarchical Principle:"})," If we include an interaction in a model, we should also include the main effects, even if the p-values for the main effects are not significant. The interaction term only makes sense in the context of the main effects."]}),e.jsx("h2",{children:"Non-linear Relationships"}),e.jsx("p",{children:"Linear regression can model some non-linear relationships by including transformed versions of the predictors. For example, we can include polynomial terms."}),e.jsxs(s,{title:"Polynomial Regression",children:[e.jsx(t,{children:"y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\cdots + \\beta_d x_i^d + \\epsilon_i"}),e.jsxs("p",{className:"mt-2",children:["This is still a ",e.jsx("em",{children:"linear"})," model because it is linear in the coefficients",e.jsx(i,{children:"\\beta_0, \\beta_1, \\ldots, \\beta_d"}),". The predictors are ",e.jsx(i,{children:"x, x^2, \\ldots, x^d"}),"."]})]}),e.jsx(a,{title:"Polynomial Regression in R",output:`Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.42e+01   2.37e+00  -5.986 3.28e-09 ***
horsepower   1.98e-01   1.18e-02  16.737  < 2e-16 ***
I(horsepower^2)  -7.69e-04   1.38e-05 -55.658  < 2e-16 ***

Multiple R-squared:  0.6876`,children:`# Quadratic fit
lm.fit2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)
summary(lm.fit2)

# The squared term is highly significant
# This indicates a non-linear relationship`}),e.jsx("h2",{children:"Potential Problems"}),e.jsx("p",{children:"When fitting a linear regression model, several problems may arise. Here are the most common ones and how to detect them."}),e.jsx("h3",{children:"1. Non-linearity of the Data"}),e.jsxs("p",{children:["If the true relationship is non-linear, the linear model will systematically over- or under-predict. Use ",e.jsx("em",{children:"residual plots"})," to detect this."]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700 my-4",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Detection"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Plot residuals ",e.jsx(i,{children:"e_i = y_i - \\hat y_i"})," against fitted values ",e.jsx(i,{children:"\\hat y_i"}),". Look for patterns—a curved pattern suggests non-linearity."]})]}),e.jsx("h3",{children:"2. Correlation of Error Terms"}),e.jsx("p",{children:"An important assumption is that the error terms are uncorrelated. If there is correlation, the estimated standard errors will be too small, leading to overly narrow confidence intervals."}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700 my-4",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Detection"}),e.jsx("p",{className:"text-dark-300 text-sm",children:"Common with time series data. Plot residuals against time or observation order and look for patterns. The Durbin-Watson test can formally test for autocorrelation."})]}),e.jsx("h3",{children:"3. Non-constant Variance (Heteroscedasticity)"}),e.jsxs("p",{children:["We assume ",e.jsx(i,{children:"\\text{Var}(\\epsilon_i) = \\sigma^2"})," is constant. If the variance of errors increases with the response, we have ",e.jsx("em",{children:"heteroscedasticity"}),"."]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700 my-4",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Detection & Solution"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Look for a funnel shape in the residual plot. Common solutions include transforming the response using ",e.jsx(i,{children:"\\log(Y)"})," or ",e.jsx(i,{children:"\\sqrt{Y}"}),"."]})]}),e.jsx("h3",{children:"4. Outliers"}),e.jsxs("p",{children:["An ",e.jsx("em",{children:"outlier"})," is a point for which ",e.jsx(i,{children:"y_i"})," is far from the value predicted by the model. Outliers can have large effects on RSE and ",e.jsx(i,{children:"R^2"}),"."]}),e.jsxs("div",{className:"p-4 bg-dark-800/50 rounded-xl border border-dark-700 my-4",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Detection"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Examine ",e.jsx("em",{children:"studentized residuals"}),". Observations with studentized residuals greater than 3 in absolute value are potential outliers."]})]}),e.jsx("h3",{children:"5. High-Leverage Points"}),e.jsxs("p",{children:["Observations with unusual predictor values have ",e.jsx("em",{children:"high leverage"}),". They can have a large impact on the fitted regression line."]}),e.jsxs(s,{title:"Leverage Statistic",children:[e.jsxs("p",{children:["The leverage statistic ",e.jsx(i,{children:"h_i"})," quantifies the leverage of observation ",e.jsx(i,{children:"i"}),". It always lies between ",e.jsx(i,{children:"1/n"})," and 1, and the average leverage is ",e.jsx(i,{children:"(p+1)/n"}),"."]}),e.jsxs("p",{className:"mt-2",children:["Observations with ",e.jsx(i,{children:"h_i"})," greatly exceeding ",e.jsx(i,{children:"(p+1)/n"})," are high-leverage points."]})]}),e.jsx("h3",{children:"6. Collinearity"}),e.jsxs("p",{children:[e.jsx("em",{children:"Collinearity"})," refers to the situation in which two or more predictor variables are closely related to one another. It can make it difficult to determine the individual effects of collinear variables."]}),e.jsxs(s,{title:"Variance Inflation Factor (VIF)",children:[e.jsx("p",{children:"The VIF measures how much the variance of a coefficient is inflated due to collinearity:"}),e.jsx(t,{children:"\\text{VIF}(\\hat{\\beta}_j) = \\frac{1}{1 - R^2_{X_j | X_{-j}}}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(i,{children:"R^2_{X_j | X_{-j}}"})," is the ",e.jsx(i,{children:"R^2"})," from regressing ",e.jsx(i,{children:"X_j"})," onto all other predictors. A VIF of 1 indicates no collinearity; values exceeding 5 or 10 indicate problematic collinearity."]})]}),e.jsx(a,{title:"Diagnostic Plots in R",children:`# Fit model
lm.fit <- lm(sales ~ TV + radio + newspaper, data = Advertising)

# Diagnostic plots
par(mfrow = c(2, 2))
plot(lm.fit)

# Calculate VIF (requires car package)
library(car)
vif(lm.fit)

# Studentized residuals
rstudent(lm.fit)

# Leverage values
hatvalues(lm.fit)`}),e.jsx("h2",{children:"Summary"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsx("li",{children:"Qualitative predictors are included using dummy variables"}),e.jsx("li",{children:"Interaction terms model non-additive effects between predictors"}),e.jsx("li",{children:"Polynomial terms can capture non-linear relationships"}),e.jsx("li",{children:"Always check residual plots for violations of assumptions"}),e.jsx("li",{children:"Use VIF to detect collinearity among predictors"})]}),e.jsxs(r,{type:"success",children:[e.jsx("strong",{children:"Coming Up:"})," Next we'll examine a case study applying these concepts to a marketing problem, followed by a comparison of linear regression with K-nearest neighbors."]}),e.jsx(o,{sectionId:8,questions:c,title:"Regression Considerations Quiz"})]})}export{y as default};
