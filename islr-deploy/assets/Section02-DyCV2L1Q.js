import{j as e}from"./index-DhQdOyFW.js";import{L as a,D as r,E as n,T as l,C as t}from"./ContentBlocks-D_nQn2bo.js";import{a as s,M as i}from"./MathBlock-DvLdyvTH.js";function h(){return e.jsxs(a,{sectionId:2,children:[e.jsx("h2",{children:"The Statistical Learning Framework"}),e.jsxs("p",{children:["Suppose we observe a quantitative response ",e.jsx(s,{children:"Y"})," and ",e.jsx(s,{children:"p"})," different predictors, ",e.jsx(s,{children:"X_1, X_2, \\ldots, X_p"}),". We assume that there is some relationship between ",e.jsx(s,{children:"Y"})," and ",e.jsx(s,{children:"X = (X_1, X_2, \\ldots, X_p)"}),", which can be written in the very general form:"]}),e.jsx(i,{children:"Y = f(X) + \\epsilon"}),e.jsxs(r,{title:"The Statistical Learning Model",children:[e.jsxs("p",{children:["Here ",e.jsx(s,{children:"f"})," is some fixed but unknown function of ",e.jsx(s,{children:"X_1, \\ldots, X_p"}),", and ",e.jsx(s,{children:"\\epsilon"})," is a random ",e.jsx("em",{children:"error term"}),", which is independent of ",e.jsx(s,{children:"X"})," and has mean zero."]}),e.jsxs("p",{className:"mt-2",children:["In this formulation, ",e.jsx(s,{children:"f"})," represents the ",e.jsx("em",{children:"systematic"})," information that ",e.jsx(s,{children:"X"})," provides about ",e.jsx(s,{children:"Y"}),"."]})]}),e.jsx("h2",{children:"Example: Income and Education"}),e.jsxs("p",{children:["Consider a study examining the relationship between years of education, seniority, and income. The response variable is ",e.jsx(s,{children:"Y = \\text{income}"}),", and the predictors are ",e.jsx(s,{children:"X_1 = \\text{years of education}"})," and ",e.jsx(s,{children:"X_2 = \\text{seniority}"}),"."]}),e.jsxs(n,{title:"Income Prediction",children:[e.jsx("p",{children:"Suppose we have collected data on 30 individuals. We might model income as:"}),e.jsx(i,{children:"\\text{Income} = f(\\text{Education}, \\text{Seniority}) + \\epsilon"}),e.jsxs("p",{className:"mt-2",children:["The function ",e.jsx(s,{children:"f"})," might be a surface in 3D space, where different combinations of education and seniority map to different expected incomes."]})]}),e.jsxs("h2",{children:["Why Estimate ",e.jsx(s,{children:"f"}),"?"]}),e.jsxs("p",{children:["There are two main reasons we may wish to estimate ",e.jsx(s,{children:"f"}),": ",e.jsx("strong",{children:"prediction"})," and ",e.jsx("strong",{children:"inference"}),". Depending on whether our ultimate goal is prediction, inference, or some combination of the two, different methods for estimating ",e.jsx(s,{children:"f"})," may be appropriate."]}),e.jsx("h3",{children:"Prediction"}),e.jsxs("p",{children:["In many situations, a set of inputs ",e.jsx(s,{children:"X"})," are readily available, but the output ",e.jsx(s,{children:"Y"})," cannot be easily obtained. In this setting, since the error term averages to zero, we can predict ",e.jsx(s,{children:"Y"})," using:"]}),e.jsx(i,{children:"\\hat{Y} = \\hat{f}(X)"}),e.jsxs("p",{children:["where ",e.jsx(s,{children:"\\hat f"})," represents our estimate for ",e.jsx(s,{children:"f"}),", and ",e.jsx(s,{children:"\\hat Y"})," represents the resulting prediction for ",e.jsx(s,{children:"Y"}),"."]}),e.jsxs(r,{title:"Reducible and Irreducible Error",children:[e.jsxs("p",{children:["The accuracy of ",e.jsx(s,{children:"\\hat Y"})," as a prediction for ",e.jsx(s,{children:"Y"})," depends on two quantities:"]}),e.jsxs("ul",{className:"list-disc list-inside mt-2 space-y-1",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"Reducible error:"})," The error that arises because ",e.jsx(s,{children:"\\hat f"})," is not a perfect estimate for ",e.jsx(s,{children:"f"}),". This can potentially be reduced by using a more appropriate statistical learning technique."]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Irreducible error:"})," The error that arises from ",e.jsx(s,{children:"\\epsilon"}),". Even if we could perfectly estimate ",e.jsx(s,{children:"f"}),", we could not perfectly predict ",e.jsx(s,{children:"Y"})," because ",e.jsx(s,{children:"\\epsilon"})," cannot be predicted using ",e.jsx(s,{children:"X"}),"."]})]})]}),e.jsxs("p",{children:["Why is the irreducible error larger than zero? The quantity ",e.jsx(s,{children:"\\epsilon"})," may contain unmeasured variables that are useful in predicting ",e.jsx(s,{children:"Y"}),": since we don't measure them, ",e.jsx(s,{children:"f"})," cannot use them for its prediction. It may also contain unmeasurable variation."]}),e.jsxs(l,{title:"Decomposition of Expected Prediction Error",children:[e.jsxs("p",{children:["Consider a given estimate ",e.jsx(s,{children:"\\hat f"})," and a set of predictors ",e.jsx(s,{children:"X"}),", which yields the prediction ",e.jsx(s,{children:"\\hat Y = \\hat f(X)"}),". Assuming ",e.jsx(s,{children:"\\hat f"})," and ",e.jsx(s,{children:"X"})," are fixed, we can show that:"]}),e.jsx(i,{children:"E[(Y - \\hat{Y})^2] = E[(f(X) + \\epsilon - \\hat{f}(X))^2]"}),e.jsx(i,{children:"= \\underbrace{[f(X) - \\hat{f}(X)]^2}_{\\text{Reducible}} + \\underbrace{\\text{Var}(\\epsilon)}_{\\text{Irreducible}}"})]}),e.jsxs(t,{type:"warning",children:[e.jsx("strong",{children:"Key Insight:"})," The irreducible error provides an upper bound on the accuracy of our prediction for ",e.jsx(s,{children:"Y"}),". This bound is almost always unknown in practice, but it reminds us that prediction will never be perfect."]}),e.jsx("h3",{children:"Inference"}),e.jsxs("p",{children:["We are often interested in understanding the ",e.jsx("em",{children:"relationship"})," between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),", rather than just predicting ",e.jsx(s,{children:"Y"}),". In this situation we wish to estimate ",e.jsx(s,{children:"f"}),", but our goal is not necessarily to make predictions."]}),e.jsx("p",{children:"We instead want to understand:"}),e.jsxs("div",{className:"space-y-3 my-6",children:[e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-dark-700",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Which predictors are associated with the response?"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["It is often the case that only a small fraction of the available predictors are substantially associated with ",e.jsx(s,{children:"Y"}),". Identifying these predictors is often valuable."]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-dark-700",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-2",children:"What is the relationship between the response and each predictor?"}),e.jsxs("p",{className:"text-dark-300 text-sm",children:["Some predictors may have a positive relationship with ",e.jsx(s,{children:"Y"}),", while others have a negative relationship. The relationship may also depend on the values of other predictors."]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-dark-700",children:[e.jsx("h4",{className:"text-amber-400 font-semibold mb-2",children:"Can the relationship be summarized using a linear equation?"}),e.jsx("p",{className:"text-dark-300 text-sm",children:"Or is the relationship more complicated? Linear models are easier to interpret, but may not capture the true relationship accurately."})]})]}),e.jsxs("h2",{children:["How Do We Estimate ",e.jsx(s,{children:"f"}),"?"]}),e.jsxs("p",{children:["Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function ",e.jsx(s,{children:"f"}),". In broad terms, most statistical learning methods can be characterized as either ",e.jsx("em",{children:"parametric"})," or ",e.jsx("em",{children:"non-parametric"}),"."]}),e.jsx("h3",{children:"Parametric Methods"}),e.jsx("p",{children:"Parametric methods involve a two-step model-based approach:"}),e.jsxs("div",{className:"my-6 p-5 bg-gradient-to-br from-blue-500/10 to-blue-600/5 rounded-xl border border-blue-500/20",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-3",children:"Two-Step Parametric Approach"}),e.jsxs("ol",{className:"list-decimal list-inside space-y-3 text-dark-300",children:[e.jsxs("li",{children:[e.jsxs("strong",{children:["Make an assumption about the functional form of ",e.jsx(s,{children:"f"}),"."]}),e.jsxs("p",{className:"ml-6 mt-1 text-dark-400 text-sm",children:["For example, we might assume that ",e.jsx(s,{children:"f"})," is linear in ",e.jsx(s,{children:"X"}),":"]}),e.jsx("div",{className:"ml-6 mt-2",children:e.jsx(i,{children:"f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p"})})]}),e.jsxs("li",{children:[e.jsxs("strong",{children:["Use the training data to ",e.jsx("em",{children:"fit"})," or ",e.jsx("em",{children:"train"})," the model."]}),e.jsxs("p",{className:"ml-6 mt-1 text-dark-400 text-sm",children:["Estimate the parameters ",e.jsx(s,{children:"\\beta_0, \\beta_1, \\ldots, \\beta_p"})," such that:"]}),e.jsx("div",{className:"ml-6 mt-2",children:e.jsx(i,{children:"Y \\approx \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p"})})]})]})]}),e.jsxs(t,{type:"info",children:[e.jsx("strong",{children:"Advantage:"})," Estimating a set of parameters is much simpler than fitting an entirely arbitrary function ",e.jsx(s,{children:"f"}),".",e.jsx("br",{}),e.jsx("br",{}),e.jsx("strong",{children:"Disadvantage:"})," The model we choose will usually not match the true unknown form of ",e.jsx(s,{children:"f"}),". If the chosen model is too far from the true ",e.jsx(s,{children:"f"}),", our estimate will be poor."]}),e.jsx("h3",{children:"Non-Parametric Methods"}),e.jsxs("p",{children:["Non-parametric methods do not make explicit assumptions about the functional form of ",e.jsx(s,{children:"f"}),". Instead, they seek an estimate of ",e.jsx(s,{children:"f"})," that gets as close to the data points as possible without being too rough or wiggly."]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4 my-6",children:[e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-emerald-500/30",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Advantages"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsxs("li",{children:["Can accurately fit a wider range of shapes for ",e.jsx(s,{children:"f"})]}),e.jsx("li",{children:"No assumptions about functional form needed"}),e.jsx("li",{children:"More flexible than parametric approaches"})]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-amber-500/30",children:[e.jsx("h4",{className:"text-amber-400 font-semibold mb-2",children:"Disadvantages"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsx("li",{children:"Requires a very large number of observations"}),e.jsx("li",{children:"Can overfit if not carefully tuned"}),e.jsx("li",{children:"Harder to interpret than parametric models"})]})]})]}),e.jsx("h2",{children:"The Trade-Off Between Flexibility and Interpretability"}),e.jsxs("p",{children:["There is often a trade-off between ",e.jsx("em",{children:"flexibility"})," and ",e.jsx("em",{children:"interpretability"}),". Linear regression is relatively inflexible but very interpretable. Methods like thin-plate splines or neural networks are more flexible but harder to interpret."]}),e.jsxs("div",{className:"my-6 p-4 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-dark-200 font-semibold mb-3 text-center",children:"Flexibility vs. Interpretability Spectrum"}),e.jsxs("div",{className:"flex items-center justify-between text-sm",children:[e.jsxs("div",{className:"text-center",children:[e.jsx("div",{className:"text-emerald-400 font-semibold",children:"Low Flexibility"}),e.jsx("div",{className:"text-dark-400",children:"High Interpretability"}),e.jsx("div",{className:"text-dark-500 mt-1",children:"Subset Selection, Lasso"})]}),e.jsx("div",{className:"flex-1 mx-4 h-2 bg-gradient-to-r from-emerald-500 via-amber-500 to-red-500 rounded-full"}),e.jsxs("div",{className:"text-center",children:[e.jsx("div",{className:"text-red-400 font-semibold",children:"High Flexibility"}),e.jsx("div",{className:"text-dark-400",children:"Low Interpretability"}),e.jsx("div",{className:"text-dark-500 mt-1",children:"Neural Networks, SVMs"})]})]})]}),e.jsx("p",{children:"Why would we ever choose a more restrictive method instead of a very flexible approach? There are several reasons:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsxs("li",{children:["If we are mainly interested in ",e.jsx("strong",{children:"inference"}),", restrictive models are much more interpretable"]}),e.jsxs("li",{children:["Highly flexible methods can ",e.jsx("strong",{children:"overfit"})," the training data, performing poorly on new data"]}),e.jsx("li",{children:"Simple models often perform just as well as complex ones, especially with limited data"})]}),e.jsx("h2",{children:"Supervised vs. Unsupervised Learning"}),e.jsxs("p",{children:["Most statistical learning problems fall into one of two categories:",e.jsx("em",{children:"supervised"})," or ",e.jsx("em",{children:"unsupervised"}),"."]}),e.jsxs(r,{title:"Supervised Learning",children:[e.jsxs("p",{children:["For each observation of the predictor measurement(s) ",e.jsx(s,{children:"x_i"}),", ",e.jsx(s,{children:"i = 1, \\ldots, n"}),", there is an associated response measurement ",e.jsx(s,{children:"y_i"}),"."]}),e.jsx("p",{className:"mt-2",children:"We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference)."})]}),e.jsxs(r,{title:"Unsupervised Learning",children:[e.jsxs("p",{children:["For every observation ",e.jsx(s,{children:"i = 1, \\ldots, n"}),", we observe a vector of measurements ",e.jsx(s,{children:"x_i"})," but no associated response ",e.jsx(s,{children:"y_i"}),"."]}),e.jsx("p",{className:"mt-2",children:"We cannot fit a linear regression model since there is no response variable to predict. Instead, we seek to understand the relationships between the variables or between the observations."})]}),e.jsxs(n,{title:"Clustering",children:[e.jsxs("p",{children:["One important tool in unsupervised learning is ",e.jsx("em",{children:"cluster analysis"}),". The goal is to ascertain, on the basis of ",e.jsx(s,{children:"x_1, \\ldots, x_n"}),", whether the observations fall into relatively distinct groups."]}),e.jsx("p",{className:"mt-2",children:"For example, in a market segmentation study, we might observe multiple characteristics for potential customers and try to identify clusters of similar customers."})]}),e.jsx("h2",{children:"Regression vs. Classification Problems"}),e.jsxs("p",{children:["Variables can be characterized as either ",e.jsx("em",{children:"quantitative"})," or ",e.jsx("em",{children:"qualitative"}),"(also known as ",e.jsx("em",{children:"categorical"}),")."]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4 my-6",children:[e.jsxs("div",{className:"bg-gradient-to-br from-emerald-500/10 to-emerald-600/5 rounded-xl p-5 border border-emerald-500/20",children:[e.jsx("h4",{className:"text-emerald-400 font-semibold mb-2",children:"Quantitative Variables"}),e.jsx("p",{className:"text-dark-300 text-sm mb-2",children:"Take on numerical values. Examples:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-400 text-sm",children:[e.jsx("li",{children:"Age, height, income"}),e.jsx("li",{children:"Temperature, price"}),e.jsx("li",{children:"Number of items sold"})]})]}),e.jsxs("div",{className:"bg-gradient-to-br from-blue-500/10 to-blue-600/5 rounded-xl p-5 border border-blue-500/20",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-2",children:"Qualitative Variables"}),e.jsxs("p",{className:"text-dark-300 text-sm mb-2",children:["Take on values in one of ",e.jsx(s,{children:"K"})," different classes. Examples:"]}),e.jsxs("ul",{className:"list-disc list-inside text-dark-400 text-sm",children:[e.jsx("li",{children:"Gender (male/female)"}),e.jsx("li",{children:"Brand purchased"}),e.jsx("li",{children:"Cancer diagnosis (yes/no)"})]})]})]}),e.jsxs(r,{title:"Regression and Classification",children:[e.jsxs("p",{children:["Problems with a ",e.jsx("strong",{children:"quantitative response"})," are referred to as ",e.jsx("strong",{children:"regression"})," problems."]}),e.jsxs("p",{className:"mt-2",children:["Problems involving a ",e.jsx("strong",{children:"qualitative response"})," are often referred to as ",e.jsx("strong",{children:"classification"})," problems."]})]}),e.jsxs(t,{type:"success",children:[e.jsx("strong",{children:"Up Next:"})," In the next section, we'll explore how to assess the accuracy of statistical learning models, introducing the crucial concept of the bias-variance tradeoff."]})]})}export{h as default};
