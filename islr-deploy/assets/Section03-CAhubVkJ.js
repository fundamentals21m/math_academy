import{r as m,j as e}from"./index-D-anHz-v.js";import{L as j,C as a}from"./Callout-S0Ir82k9.js";import{D as t,E as f,T as o,A as p}from"./ContentBlocks-DqG_oog1.js";import{M as i,a as s}from"./MathBlock-D_BNZVWb.js";function g(){const[r,c]=m.useState(50),n=Math.pow((100-r)/100,2)*60,l=Math.pow(r/100,2)*60,h=10,d=n+l+h;return e.jsxs("div",{className:"p-6 bg-dark-800/50 rounded-xl",children:[e.jsx("h3",{className:"text-lg font-semibold text-dark-100 mb-4",children:"Bias-Variance Tradeoff"}),e.jsxs("div",{className:"mb-6",children:[e.jsxs("label",{className:"block text-sm text-dark-300 mb-2",children:["Model Flexibility: ",r,"%"]}),e.jsx("input",{type:"range",min:"0",max:"100",value:r,onChange:x=>c(Number(x.target.value)),className:"w-full h-2 bg-dark-700 rounded-lg appearance-none cursor-pointer"}),e.jsxs("div",{className:"flex justify-between text-xs text-dark-500 mt-1",children:[e.jsx("span",{children:"Simple (High Bias)"}),e.jsx("span",{children:"Complex (High Variance)"})]})]}),e.jsxs("div",{className:"space-y-3",children:[e.jsxs("div",{className:"flex items-center gap-3",children:[e.jsx("span",{className:"w-24 text-sm text-dark-400",children:"Bias²"}),e.jsx("div",{className:"flex-1 h-6 bg-dark-700 rounded-full overflow-hidden",children:e.jsx("div",{className:"h-full bg-blue-500 transition-all duration-300",style:{width:`${n/80*100}%`}})}),e.jsx("span",{className:"w-12 text-sm text-dark-300 text-right",children:n.toFixed(1)})]}),e.jsxs("div",{className:"flex items-center gap-3",children:[e.jsx("span",{className:"w-24 text-sm text-dark-400",children:"Variance"}),e.jsx("div",{className:"flex-1 h-6 bg-dark-700 rounded-full overflow-hidden",children:e.jsx("div",{className:"h-full bg-amber-500 transition-all duration-300",style:{width:`${l/80*100}%`}})}),e.jsx("span",{className:"w-12 text-sm text-dark-300 text-right",children:l.toFixed(1)})]}),e.jsxs("div",{className:"flex items-center gap-3",children:[e.jsx("span",{className:"w-24 text-sm text-dark-400",children:"Irreducible"}),e.jsx("div",{className:"flex-1 h-6 bg-dark-700 rounded-full overflow-hidden",children:e.jsx("div",{className:"h-full bg-dark-500",style:{width:`${h/80*100}%`}})}),e.jsx("span",{className:"w-12 text-sm text-dark-300 text-right",children:h.toFixed(1)})]}),e.jsx("div",{className:"border-t border-dark-600 pt-3 mt-3",children:e.jsxs("div",{className:"flex items-center gap-3",children:[e.jsx("span",{className:"w-24 text-sm text-dark-200 font-semibold",children:"Total MSE"}),e.jsx("div",{className:"flex-1 h-6 bg-dark-700 rounded-full overflow-hidden",children:e.jsx("div",{className:"h-full bg-emerald-500 transition-all duration-300",style:{width:`${d/80*100}%`}})}),e.jsx("span",{className:"w-12 text-sm text-emerald-400 text-right font-semibold",children:d.toFixed(1)})]})})]}),e.jsx("p",{className:"mt-4 text-xs text-dark-500",children:"The optimal model complexity minimizes total test error (MSE). Too simple = underfitting (high bias). Too complex = overfitting (high variance)."})]})}function w(){return e.jsxs(j,{sectionId:3,children:[e.jsx("h2",{children:"Measuring the Quality of Fit"}),e.jsx("p",{children:"In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation."}),e.jsx("h3",{children:"Mean Squared Error for Regression"}),e.jsxs("p",{children:["In the regression setting, the most commonly-used measure is the ",e.jsx("em",{children:"mean squared error"})," (MSE):"]}),e.jsxs(t,{title:"Mean Squared Error (MSE)",children:[e.jsx(i,{children:"\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"\\hat f(x_i)"})," is the prediction that ",e.jsx(s,{children:"\\hat f"})," gives for the ",e.jsx(s,{children:"i"}),"th observation."]}),e.jsx("p",{className:"mt-2",children:"The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially."})]}),e.jsx("h3",{children:"Training MSE vs. Test MSE"}),e.jsxs("p",{children:["The MSE computed using the training data is known as the ",e.jsx("em",{children:"training MSE"}),". But in general, we do not really care how well the method works on the training data. Instead, we are interested in the accuracy of the predictions when we apply our method to previously unseen ",e.jsx("em",{children:"test data"}),"."]}),e.jsxs(a,{type:"warning",children:[e.jsx("strong",{children:"Critical Distinction:"})," We want to choose the method that gives the lowest ",e.jsx("em",{children:"test MSE"}),", as opposed to the lowest training MSE. In general, there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE."]}),e.jsxs(f,{title:"Overfitting",children:[e.jsxs("p",{children:["As model flexibility increases, training MSE will decrease, but the test MSE may not. When a method yields a small training MSE but a large test MSE, we are said to be ",e.jsx("strong",{children:"overfitting"})," the data."]}),e.jsxs("p",{className:"mt-2",children:["This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up patterns that are just caused by random chance rather than true properties of ",e.jsx(s,{children:"f"}),"."]})]}),e.jsx("h2",{children:"The Bias-Variance Trade-Off"}),e.jsxs("p",{children:["The expected test MSE for a given value ",e.jsx(s,{children:"x_0"})," can always be decomposed into the sum of three fundamental quantities: the ",e.jsx("em",{children:"variance"})," of ",e.jsx(s,{children:"\\hat f(x_0)"}),", the squared ",e.jsx("em",{children:"bias"})," of ",e.jsx(s,{children:"\\hat f(x_0)"}),", and the variance of the error terms ",e.jsx(s,{children:"\\epsilon"}),"."]}),e.jsxs(o,{title:"Bias-Variance Decomposition",children:[e.jsx(i,{children:"E\\left[(y_0 - \\hat{f}(x_0))^2\\right] = \\text{Var}(\\hat{f}(x_0)) + [\\text{Bias}(\\hat{f}(x_0))]^2 + \\text{Var}(\\epsilon)"}),e.jsxs("p",{className:"mt-3",children:["Here the notation ",e.jsx(s,{children:"E[(y_0 - \\hat f(x_0))^2]"})," defines the ",e.jsx("em",{children:"expected test MSE"}),"at ",e.jsx(s,{children:"x_0"}),", and refers to the average test MSE that we would obtain if we repeatedly estimated ",e.jsx(s,{children:"f"})," using a large number of training sets, and tested each at ",e.jsx(s,{children:"x_0"}),"."]})]}),e.jsxs(t,{title:"Variance",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Variance"})," refers to the amount by which ",e.jsx(s,{children:"\\hat f"})," would change if we estimated it using a different training data set."]}),e.jsxs("p",{className:"mt-2",children:["Since the training data are used to fit the statistical learning method, different training data sets will result in a different ",e.jsx(s,{children:"\\hat f"}),". Ideally, the estimate should not vary too much between training sets."]}),e.jsxs("p",{className:"mt-2 text-dark-400",children:["In general, ",e.jsx("strong",{children:"more flexible methods have higher variance"}),"."]})]}),e.jsxs(t,{title:"Bias",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Bias"})," refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model."]}),e.jsxs("p",{className:"mt-2",children:["For example, linear regression assumes that there is a linear relationship between ",e.jsx(s,{children:"Y"})," and ",e.jsx(s,{children:"X"}),". It is unlikely that any real-life problem truly has such a simple linear relationship, and so the linear regression will introduce some bias."]}),e.jsxs("p",{className:"mt-2 text-dark-400",children:["In general, ",e.jsx("strong",{children:"more flexible methods result in less bias"}),"."]})]}),e.jsx("h3",{children:"Interactive Visualization: The Trade-Off"}),e.jsx("p",{children:"The relationship between model flexibility, bias, and variance is fundamental to understanding statistical learning. Use the slider below to see how changing model flexibility affects these components:"}),e.jsx("div",{className:"my-6",children:e.jsx(g,{})}),e.jsxs(a,{type:"info",children:[e.jsx("strong",{children:"The Challenge:"})," As we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases."]}),e.jsx("h3",{children:"Finding the Sweet Spot"}),e.jsx("p",{children:"As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance."}),e.jsxs("div",{className:"my-6 p-5 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-dark-200 font-semibold mb-3",children:"Key Observations"}),e.jsxs("ul",{className:"space-y-2 text-dark-300",children:[e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-emerald-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx("strong",{children:"Too simple"})," (high bias, low variance): Underfitting - fails to capture important patterns"]})]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-amber-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx("strong",{children:"Just right"}),": Optimal balance between bias and variance"]})]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-red-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx("strong",{children:"Too complex"})," (low bias, high variance): Overfitting - picks up random noise as patterns"]})]})]})]}),e.jsx("h2",{children:"The Classification Setting"}),e.jsxs("p",{children:["The concepts discussed so far have focused on the regression setting. We now consider the classification setting, where the response variable ",e.jsx(s,{children:"Y"})," is qualitative."]}),e.jsx("h3",{children:"Training Error Rate"}),e.jsxs("p",{children:["The most common approach for quantifying the accuracy of our estimate ",e.jsx(s,{children:"\\hat f"})," is the ",e.jsx("em",{children:"training error rate"}),", the proportion of mistakes made when applying",e.jsx(s,{children:"\\hat f"})," to the training observations:"]}),e.jsxs(t,{title:"Training Error Rate",children:[e.jsx(i,{children:"\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)"}),e.jsxs("p",{className:"mt-2",children:["Here ",e.jsx(s,{children:"\\hat y_i"})," is the predicted class label for the ",e.jsx(s,{children:"i"}),"th observation using ",e.jsx(s,{children:"\\hat f"}),", and ",e.jsx(s,{children:"I(y_i \\neq \\hat y_i)"})," is an ",e.jsx("em",{children:"indicator variable"})," that equals 1 if ",e.jsx(s,{children:"y_i \\neq \\hat y_i"})," and zero if ",e.jsx(s,{children:"y_i = \\hat y_i"}),"."]})]}),e.jsx("p",{children:"As in the regression setting, we are most interested in the error rates that result from applying our classifier to test observations that were not used in training."}),e.jsxs(t,{title:"Test Error Rate",children:[e.jsx(i,{children:"\\text{Ave}(I(y_0 \\neq \\hat{y}_0))"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"\\hat y_0"})," is the predicted class label that results from applying the classifier to the test observation with predictor ",e.jsx(s,{children:"x_0"}),"."]})]}),e.jsx("h3",{children:"The Bayes Classifier"}),e.jsx("p",{children:"It is possible to show that the test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values."}),e.jsxs(t,{title:"Bayes Classifier",children:[e.jsxs("p",{children:["The Bayes classifier assigns a test observation with predictor vector ",e.jsx(s,{children:"x_0"})," to the class ",e.jsx(s,{children:"j"})," for which the conditional probability is largest:"]}),e.jsx(i,{children:"\\Pr(Y = j \\,|\\, X = x_0)"}),e.jsxs("p",{className:"mt-2",children:["In a two-class problem where there are only two possible response values (0 or 1), the Bayes classifier corresponds to predicting class 1 if ",e.jsx(s,{children:"\\Pr(Y = 1 | X = x_0) > 0.5"}),", and class 0 otherwise."]})]}),e.jsxs(o,{title:"Bayes Error Rate",children:[e.jsxs("p",{children:["The Bayes classifier produces the lowest possible test error rate, called the",e.jsx("strong",{children:" Bayes error rate"}),":"]}),e.jsx(i,{children:"1 - E\\left[\\max_j \\Pr(Y = j \\,|\\, X)\\right]"}),e.jsx("p",{className:"mt-2",children:"The Bayes error rate is analogous to the irreducible error in regression. It represents the best possible classification error rate."})]}),e.jsxs(a,{type:"warning",children:[e.jsx("strong",{children:"In Practice:"})," For real data, we do not know the conditional distribution of ",e.jsx(s,{children:"Y"})," given ",e.jsx(s,{children:"X"}),", so computing the Bayes classifier is impossible. The Bayes classifier serves as an unattainable gold standard against which to compare other methods."]}),e.jsx("h3",{children:"K-Nearest Neighbors"}),e.jsxs("p",{children:["One of the most intuitive classifiers is ",e.jsx("em",{children:"K-Nearest Neighbors"})," (KNN). Given a positive integer ",e.jsx(s,{children:"K"})," and a test observation ",e.jsx(s,{children:"x_0"}),", the KNN classifier first identifies the ",e.jsx(s,{children:"K"})," points in the training data that are closest to ",e.jsx(s,{children:"x_0"}),", then estimates the conditional probability as the fraction of points in this neighborhood belonging to each class."]}),e.jsx(p,{title:"K-Nearest Neighbors Classifier",children:e.jsxs("ol",{className:"list-decimal list-inside space-y-2",children:[e.jsxs("li",{children:["Given a test observation ",e.jsx(s,{children:"x_0"}),", identify the ",e.jsx(s,{children:"K"})," training observations closest to ",e.jsx(s,{children:"x_0"})," (call this set ",e.jsx(s,{children:"\\mathcal N_0"}),")"]}),e.jsxs("li",{children:["Estimate the conditional probability for class ",e.jsx(s,{children:"j"})," as:",e.jsx(i,{children:"\\Pr(Y = j \\,|\\, X = x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_0} I(y_i = j)"})]}),e.jsxs("li",{children:["Apply Bayes rule: classify ",e.jsx(s,{children:"x_0"})," to the class with the largest probability"]})]})}),e.jsxs("p",{children:["The choice of ",e.jsx(s,{children:"K"})," has a drastic effect on the KNN classifier:"]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4 my-6",children:[e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-blue-500/30",children:[e.jsx("h4",{className:"text-blue-400 font-semibold mb-2",children:"Small K (e.g., K = 1)"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsx("li",{children:"Very flexible decision boundary"}),e.jsx("li",{children:"Low bias, high variance"}),e.jsx("li",{children:"Risk of overfitting"})]})]}),e.jsxs("div",{className:"bg-dark-800/50 rounded-xl p-4 border border-amber-500/30",children:[e.jsx("h4",{className:"text-amber-400 font-semibold mb-2",children:"Large K (e.g., K = 100)"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 text-sm space-y-1",children:[e.jsx("li",{children:"Nearly linear decision boundary"}),e.jsx("li",{children:"High bias, low variance"}),e.jsx("li",{children:"Risk of underfitting"})]})]})]}),e.jsxs(a,{type:"success",children:[e.jsx("strong",{children:"Connection to Regression:"})," Just as in regression, there is a bias-variance trade-off in classification. The training error rate declines as",e.jsx(s,{children:"K"})," decreases (more flexibility), but the test error rate may not."]}),e.jsx("h2",{children:"Summary"}),e.jsx("p",{children:"This section introduced the fundamental concepts for assessing model accuracy:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsx("li",{children:"The distinction between training error and test error"}),e.jsx("li",{children:"The bias-variance trade-off and its implications for model selection"}),e.jsx("li",{children:"The Bayes classifier as the optimal (but unattainable) classifier"}),e.jsx("li",{children:"K-Nearest Neighbors as a simple, intuitive classification method"})]}),e.jsxs(a,{type:"info",children:[e.jsx("strong",{children:"Coming Up:"})," In the next section, we'll get hands-on with R and learn the basic commands needed to implement statistical learning methods. After that, we'll dive into linear regression—the foundation of many more advanced methods."]})]})}export{w as default};
