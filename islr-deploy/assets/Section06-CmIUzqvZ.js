import{r as p,j as e}from"./index-D-anHz-v.js";import{L as M,C as y}from"./Callout-S0Ir82k9.js";import{D as x,T as X,E as I,R as A}from"./ContentBlocks-DqG_oog1.js";import{a as s,M as i}from"./MathBlock-D_BNZVWb.js";function L(){const[a,v]=p.useState([{x:50,y:180},{x:100,y:150},{x:150,y:140},{x:200,y:100},{x:250,y:80}]),[u,_]=p.useState(null),o=320,f=240,n=20,c=p.useMemo(()=>{if(a.length<2)return null;const t=a.length,l=a.reduce((d,r)=>d+r.x,0),h=a.reduce((d,r)=>d+r.y,0),m=a.reduce((d,r)=>d+r.x*r.y,0),j=a.reduce((d,r)=>d+r.x*r.x,0),g=(t*m-l*h)/(t*j-l*l),b=(h-g*l)/t,C=h/t,w=a.reduce((d,r)=>d+Math.pow(r.y-C,2),0),N=a.reduce((d,r)=>{const q=g*r.x+b;return d+Math.pow(r.y-q,2)},0),Y=w>0?1-N/w:0;return{slope:g,intercept:b,rss:N,rSquared:Y}},[a]),R=p.useCallback(t=>{if(u!==null)return;const h=t.currentTarget.getBoundingClientRect(),m=t.clientX-h.left,j=t.clientY-h.top;m>n&&m<o-n&&j>n&&j<f-n&&v([...a,{x:m,y:j}])},[a,u]),k=p.useCallback(t=>l=>{l.stopPropagation(),_(t)},[]),T=p.useCallback(t=>{if(u===null)return;const h=t.currentTarget.getBoundingClientRect(),m=Math.max(n,Math.min(o-n,t.clientX-h.left)),j=Math.max(n,Math.min(f-n,t.clientY-h.top));v(a.map((g,b)=>b===u?{x:m,y:j}:g))},[u,a]),S=p.useCallback(()=>{_(null)},[]),E=()=>v([]);return e.jsxs("div",{className:"p-6 bg-dark-800/50 rounded-xl",children:[e.jsx("h3",{className:"text-lg font-semibold text-dark-100 mb-4",children:"Linear Regression Fitter"}),e.jsxs("div",{className:"flex gap-4",children:[e.jsxs("svg",{width:o,height:f,className:"bg-dark-900 rounded-lg cursor-crosshair",onClick:R,onMouseMove:T,onMouseUp:S,onMouseLeave:S,children:[[.25,.5,.75].map(t=>e.jsxs("g",{children:[e.jsx("line",{x1:n,y1:f*t,x2:o-n,y2:f*t,stroke:"#374151",strokeDasharray:"4,4"}),e.jsx("line",{x1:o*t,y1:n,x2:o*t,y2:f-n,stroke:"#374151",strokeDasharray:"4,4"})]},t)),c&&e.jsx("line",{x1:n,y1:c.slope*n+c.intercept,x2:o-n,y2:c.slope*(o-n)+c.intercept,stroke:"#10b981",strokeWidth:2}),a.map((t,l)=>e.jsx("circle",{cx:t.x,cy:t.y,r:8,fill:u===l?"#60a5fa":"#3b82f6",stroke:"#1e3a8a",strokeWidth:2,className:"cursor-move",onMouseDown:k(l)},l))]}),e.jsxs("div",{className:"flex-1 space-y-3",children:[e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"Points:"}),e.jsx("span",{className:"text-dark-200 ml-2",children:a.length})]}),c&&e.jsxs(e.Fragment,{children:[e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"Slope (β₁):"}),e.jsx("span",{className:"text-emerald-400 ml-2 font-mono",children:c.slope.toFixed(3)})]}),e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"Intercept (β₀):"}),e.jsx("span",{className:"text-emerald-400 ml-2 font-mono",children:c.intercept.toFixed(1)})]}),e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"RSS:"}),e.jsx("span",{className:"text-amber-400 ml-2 font-mono",children:c.rss.toFixed(1)})]}),e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"R²:"}),e.jsx("span",{className:"text-blue-400 ml-2 font-mono",children:c.rSquared.toFixed(3)})]})]}),e.jsx("button",{onClick:E,className:"mt-4 px-3 py-1.5 text-xs bg-dark-700 hover:bg-dark-600 text-dark-300 rounded-lg transition-colors",children:"Clear Points"})]})]}),e.jsx("p",{className:"mt-4 text-xs text-dark-500",children:"Click to add points. Drag points to adjust. The green line shows the least squares fit."})]})}function P(){return e.jsxs(M,{sectionId:6,children:[e.jsx("h2",{children:"Simple Linear Regression"}),e.jsxs("p",{children:["Simple linear regression is a very straightforward approach for predicting a quantitative response ",e.jsx(s,{children:"Y"})," on the basis of a single predictor variable ",e.jsx(s,{children:"X"}),". It assumes that there is approximately a linear relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),"."]}),e.jsxs(x,{title:"Simple Linear Regression Model",children:[e.jsx("p",{children:"We can write this relationship as:"}),e.jsx(i,{children:"Y \\approx \\beta_0 + \\beta_1 X"}),e.jsxs("p",{className:"mt-2",children:["Here ",e.jsx(s,{children:"\\beta_0"})," and ",e.jsx(s,{children:"\\beta_1"})," are two unknown constants that represent the ",e.jsx("em",{children:"intercept"})," and ",e.jsx("em",{children:"slope"})," terms in the linear model."]}),e.jsxs("p",{className:"mt-2",children:["Together, ",e.jsx(s,{children:"\\beta_0"})," and ",e.jsx(s,{children:"\\beta_1"})," are known as the model",e.jsx("strong",{children:" coefficients"})," or ",e.jsx("strong",{children:"parameters"}),"."]})]}),e.jsxs("p",{children:["Once we have used our training data to produce estimates ",e.jsx(s,{children:"\\hat{\\beta}_0"})," and ",e.jsx(s,{children:"\\hat{\\beta}_1"})," for the model coefficients, we can predict future values using:"]}),e.jsx(i,{children:"\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x"}),e.jsxs("p",{children:["where ",e.jsx(s,{children:"\\hat{y}"})," indicates a prediction of ",e.jsx(s,{children:"Y"})," on the basis of ",e.jsx(s,{children:"X = x"}),". The hat symbol denotes the estimated value."]}),e.jsx("h2",{children:"Estimating the Coefficients"}),e.jsxs("p",{children:["Let ",e.jsx(s,{children:"(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)"})," represent ",e.jsx(s,{children:"n"})," observation pairs. We want to find coefficient estimates ",e.jsx(s,{children:"\\hat{\\beta}_0"})," and ",e.jsx(s,{children:"\\hat{\\beta}_1"})," such that the resulting line is as close as possible to the ",e.jsx(s,{children:"n"})," data points."]}),e.jsxs(x,{title:"Residual Sum of Squares (RSS)",children:[e.jsxs("p",{children:["The most common approach is to minimize the ",e.jsx("em",{children:"residual sum of squares"}),":"]}),e.jsx(i,{children:"\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2"}),e.jsxs("p",{className:"mt-2",children:["The RSS measures the total squared deviation between the observed values ",e.jsx(s,{children:"y_i"})," and the values predicted by the linear model."]})]}),e.jsx("h3",{children:"Interactive Visualization"}),e.jsx("p",{children:"Try adding and moving points in the visualization below to see how the least squares regression line changes in real-time:"}),e.jsx("div",{className:"my-6",children:e.jsx(L,{})}),e.jsxs(X,{title:"Least Squares Coefficient Estimates",children:[e.jsx("p",{children:"Using calculus, one can show that the minimizers of RSS are:"}),e.jsx(i,{children:"\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}"}),e.jsx(i,{children:"\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i"})," and ",e.jsx(s,{children:"\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i"})," are the sample means."]})]}),e.jsxs(I,{title:"Advertising and Sales",children:[e.jsxs("p",{children:["Consider predicting ",e.jsx("strong",{children:"sales"})," based on ",e.jsx("strong",{children:"TV advertising budget"}),". Using the advertising dataset with 200 markets, the least squares fit gives:"]}),e.jsx(i,{children:"\\widehat{\\text{sales}} = 7.03 + 0.0475 \\times \\text{TV}"}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Interpretation:"})," An additional $1,000 spent on TV advertising is associated with selling approximately 47.5 additional units of the product."]})]}),e.jsx("h2",{children:"Assessing the Accuracy of the Coefficient Estimates"}),e.jsxs("p",{children:["The true relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"})," takes the form ",e.jsx(s,{children:"Y = f(X) + \\epsilon"})," for some unknown function ",e.jsx(s,{children:"f"}),". If ",e.jsx(s,{children:"f"})," is approximated by a linear function, we can write:"]}),e.jsx(i,{children:"Y = \\beta_0 + \\beta_1 X + \\epsilon"}),e.jsxs("p",{children:["The error term ",e.jsx(s,{children:"\\epsilon"})," is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that affect ",e.jsx(s,{children:"Y"}),", and there may be measurement error."]}),e.jsxs(x,{title:"Population Regression Line",children:[e.jsxs("p",{children:["The model ",e.jsx(s,{children:"Y = \\beta_0 + \\beta_1 X + \\epsilon"})," defines the ",e.jsx("em",{children:"population regression line"}),", which is the best linear approximation to the true relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),"."]}),e.jsxs("p",{className:"mt-2",children:["The least squares regression line ",e.jsx(s,{children:"\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x"})," is an estimate of this population line, based on our sample of observations."]})]}),e.jsx("h3",{children:"Standard Errors"}),e.jsxs("p",{children:["If we estimate ",e.jsx(s,{children:"\\beta_0"})," and ",e.jsx(s,{children:"\\beta_1"})," using a large number of different data sets drawn from the same population, we would get different estimates each time. The ",e.jsx("em",{children:"standard error"})," tells us the average amount by which these estimates differ from the actual value."]}),e.jsxs(x,{title:"Standard Errors of Coefficient Estimates",children:[e.jsx(i,{children:"\\text{SE}(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\right]"}),e.jsx(i,{children:"\\text{SE}(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"\\sigma^2 = \\text{Var}(\\epsilon)"}),". In practice, ",e.jsx(s,{children:"\\sigma"})," is estimated using the ",e.jsx("em",{children:"residual standard error"}),":"]}),e.jsx(i,{children:"\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n-2}}"})]}),e.jsx("h3",{children:"Confidence Intervals"}),e.jsxs("p",{children:["Standard errors can be used to compute ",e.jsx("em",{children:"confidence intervals"}),". A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter."]}),e.jsxs(x,{title:"95% Confidence Interval for Coefficients",children:[e.jsxs("p",{children:["For ",e.jsx(s,{children:"\\beta_1"}),", the approximate 95% confidence interval is:"]}),e.jsx(i,{children:"\\hat{\\beta}_1 \\pm 2 \\cdot \\text{SE}(\\hat{\\beta}_1)"}),e.jsxs("p",{className:"mt-2",children:["Similarly for ",e.jsx(s,{children:"\\beta_0"}),":"]}),e.jsx(i,{children:"\\hat{\\beta}_0 \\pm 2 \\cdot \\text{SE}(\\hat{\\beta}_0)"})]}),e.jsx("h3",{children:"Hypothesis Tests"}),e.jsxs("p",{children:["Standard errors can also be used to perform ",e.jsx("em",{children:"hypothesis tests"})," on the coefficients. The most common hypothesis test involves testing:"]}),e.jsxs("div",{className:"my-6 p-4 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsxs("div",{className:"grid grid-cols-2 gap-4",children:[e.jsxs("div",{children:[e.jsx("h4",{className:"text-dark-400 text-sm font-semibold mb-1",children:"Null Hypothesis"}),e.jsx(i,{children:"H_0: \\beta_1 = 0"})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"text-dark-400 text-sm font-semibold mb-1",children:"Alternative Hypothesis"}),e.jsx(i,{children:"H_a: \\beta_1 \\neq 0"})]})]}),e.jsxs("p",{className:"text-dark-400 text-sm mt-4",children:[e.jsx(s,{children:"H_0"})," corresponds to no relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),"."]})]}),e.jsxs(x,{title:"t-Statistic",children:[e.jsxs("p",{children:["To test the null hypothesis, we compute a ",e.jsx("em",{children:"t-statistic"}),":"]}),e.jsx(i,{children:"t = \\frac{\\hat{\\beta}_1 - 0}{\\text{SE}(\\hat{\\beta}_1)} = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}"}),e.jsxs("p",{className:"mt-2",children:["This measures the number of standard deviations that ",e.jsx(s,{children:"\\hat{\\beta}_1"})," is away from 0. If there truly is no relationship, we expect this to have a ",e.jsx(s,{children:"t"}),"-distribution with ",e.jsx(s,{children:"n-2"})," degrees of freedom."]})]}),e.jsxs(y,{type:"info",children:[e.jsx("strong",{children:"p-value:"})," The p-value is the probability of observing a value of ",e.jsx(s,{children:"|t|"})," equal to or larger than what we observed, assuming ",e.jsx(s,{children:"H_0"})," is true. A small p-value indicates that it is unlikely to observe such a substantial association between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"})," due to chance alone."]}),e.jsx("h2",{children:"Assessing the Accuracy of the Model"}),e.jsxs("p",{children:["Once we have rejected the null hypothesis and concluded that there is a relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),", we want to quantify how well the model fits the data. Two related quantities are commonly used:"]}),e.jsx("h3",{children:"Residual Standard Error (RSE)"}),e.jsxs(x,{title:"Residual Standard Error",children:[e.jsx(i,{children:"\\text{RSE} = \\sqrt{\\frac{1}{n-2}\\text{RSS}} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}"}),e.jsxs("p",{className:"mt-2",children:["The RSE is an estimate of the standard deviation of ",e.jsx(s,{children:"\\epsilon"}),". Roughly speaking, it is the average amount that the response will deviate from the true regression line."]})]}),e.jsxs("p",{children:["The RSE is measured in the units of ",e.jsx(s,{children:"Y"}),". In the advertising example, RSE = 3.26, meaning actual sales deviate from the true regression line by approximately 3,260 units on average."]}),e.jsx("h3",{children:"R-squared (R²)"}),e.jsxs(x,{title:"R-squared",children:[e.jsx(i,{children:"R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx("strong",{children:"TSS"})," (Total Sum of Squares) = ",e.jsx(s,{children:"\\sum(y_i - \\bar{y})^2"})," measures the total variance in the response ",e.jsx(s,{children:"Y"}),", and RSS measures the variance that is left unexplained after performing the regression."]}),e.jsxs("p",{className:"mt-2",children:["Hence, ",e.jsx(s,{children:"R^2"})," measures the ",e.jsx("strong",{children:"proportion of variance explained"})," by the model."]})]}),e.jsxs("div",{className:"my-6 p-5 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-dark-200 font-semibold mb-3",children:"Interpreting R²"}),e.jsxs("ul",{className:"space-y-2 text-dark-300",children:[e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-emerald-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx(s,{children:"R^2 = 0"}),": The model explains none of the variability in ",e.jsx(s,{children:"Y"})]})]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-emerald-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx(s,{children:"R^2 = 1"}),": The model explains all of the variability in ",e.jsx(s,{children:"Y"})]})]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-amber-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx(s,{children:"R^2"})," close to 1: Regression explains most of the variance (good fit)"]})]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-red-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx(s,{children:"R^2"})," close to 0: Regression explains little of the variance (poor fit)"]})]})]})]}),e.jsxs(y,{type:"warning",children:[e.jsx("strong",{children:"R² and Correlation:"})," In simple linear regression with one predictor,",e.jsx(s,{children:"R^2 = r^2"})," where ",e.jsx(s,{children:"r"})," is the correlation between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),". This relationship does not extend to multiple regression."]}),e.jsx("h2",{children:"R Code Example"}),e.jsx("p",{children:"Here's how to fit a simple linear regression model in R:"}),e.jsx(A,{title:"Simple Linear Regression in R",output:`Call:
lm(formula = sales ~ TV, data = Advertising)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 7.032594   0.457843  15.360  < 2e-16 ***
TV          0.047537   0.002691  17.668  < 2e-16 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.259 on 198 degrees of freedom
Multiple R-squared: 0.6119`,children:`# Load the data
Advertising <- read.csv("Advertising.csv")

# Fit simple linear regression
lm.fit <- lm(sales ~ TV, data = Advertising)

# View the summary
summary(lm.fit)

# Get confidence intervals for coefficients
confint(lm.fit)

# Make predictions
predict(lm.fit, data.frame(TV = c(50, 100, 150)))`}),e.jsx("h2",{children:"Summary"}),e.jsx("p",{children:"This section covered the fundamentals of simple linear regression:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsxs("li",{children:["The model: ",e.jsx(s,{children:"Y = \\beta_0 + \\beta_1 X + \\epsilon"})]}),e.jsx("li",{children:"Estimating coefficients by minimizing RSS (least squares)"}),e.jsx("li",{children:"Standard errors, confidence intervals, and hypothesis tests"}),e.jsxs("li",{children:["Assessing fit with RSE and ",e.jsx(s,{children:"R^2"})]})]}),e.jsxs(y,{type:"success",children:[e.jsx("strong",{children:"Next Steps:"})," In the next section, we'll extend these ideas to",e.jsx("em",{children:"multiple linear regression"}),", where we predict ",e.jsx(s,{children:"Y"})," using multiple predictors ",e.jsx(s,{children:"X_1, X_2, \\ldots, X_p"}),"."]})]})}export{P as default};
