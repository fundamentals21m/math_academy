import{r as f,j as e}from"./index-CGfY7ygT.js";import{L as I,D as j,T as A,E as L,C as S,R as H}from"./ContentBlocks-DMsh46lg.js";import{a as s,M as a}from"./MathBlock-ZJ9jXmGe.js";function V(){const[l,_]=f.useState([{x:50,y:180},{x:100,y:150},{x:150,y:140},{x:200,y:100},{x:250,y:80}]),[g,w]=f.useState(null),m=320,b=240,r=20,o=f.useMemo(()=>{if(l.length<2)return null;const t=l.length,c=l.reduce((h,d)=>h+d.x,0),x=l.reduce((h,d)=>h+d.y,0),p=l.reduce((h,d)=>h+d.x*d.y,0),u=l.reduce((h,d)=>h+d.x*d.x,0),v=(t*p-c*x)/(t*u-c*c),y=(x-v*c)/t,q=x/t,R=l.reduce((h,d)=>h+Math.pow(d.y-q,2),0),k=l.reduce((h,d)=>{const X=v*d.x+y;return h+Math.pow(d.y-X,2)},0),M=R>0?1-k/R:0;return{slope:v,intercept:y,rss:k,rSquared:M}},[l]),T=f.useCallback(t=>{if(g!==null)return;const x=t.currentTarget.getBoundingClientRect(),p=t.clientX-x.left,u=t.clientY-x.top;p>r&&p<m-r&&u>r&&u<b-r&&_([...l,{x:p,y:u}])},[l,g]),E=f.useCallback(t=>c=>{c.stopPropagation(),w(t)},[]),C=f.useCallback(t=>{if(g===null)return;const x=t.currentTarget.getBoundingClientRect(),p=Math.max(r,Math.min(m-r,t.clientX-x.left)),u=Math.max(r,Math.min(b-r,t.clientY-x.top));_(l.map((v,y)=>y===g?{x:p,y:u}:v))},[g,l]),N=f.useCallback(()=>{w(null)},[]),Y=()=>_([]);return e.jsxs("div",{className:"p-6 bg-dark-800/50 rounded-xl",children:[e.jsx("h3",{className:"text-lg font-semibold text-dark-100 mb-4",children:"Linear Regression Fitter"}),e.jsxs("div",{className:"flex gap-4",children:[e.jsxs("svg",{width:m,height:b,className:"bg-dark-900 rounded-lg cursor-crosshair",onClick:T,onMouseMove:C,onMouseUp:N,onMouseLeave:N,children:[[.25,.5,.75].map(t=>e.jsxs("g",{children:[e.jsx("line",{x1:r,y1:b*t,x2:m-r,y2:b*t,stroke:"#374151",strokeDasharray:"4,4"}),e.jsx("line",{x1:m*t,y1:r,x2:m*t,y2:b-r,stroke:"#374151",strokeDasharray:"4,4"})]},t)),o&&e.jsx("line",{x1:r,y1:o.slope*r+o.intercept,x2:m-r,y2:o.slope*(m-r)+o.intercept,stroke:"#10b981",strokeWidth:2}),l.map((t,c)=>e.jsx("circle",{cx:t.x,cy:t.y,r:8,fill:g===c?"#60a5fa":"#3b82f6",stroke:"#1e3a8a",strokeWidth:2,className:"cursor-move",onMouseDown:E(c)},c))]}),e.jsxs("div",{className:"flex-1 space-y-3",children:[e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"Points:"}),e.jsx("span",{className:"text-dark-200 ml-2",children:l.length})]}),o&&e.jsxs(e.Fragment,{children:[e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"Slope (β₁):"}),e.jsx("span",{className:"text-emerald-400 ml-2 font-mono",children:o.slope.toFixed(3)})]}),e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"Intercept (β₀):"}),e.jsx("span",{className:"text-emerald-400 ml-2 font-mono",children:o.intercept.toFixed(1)})]}),e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"RSS:"}),e.jsx("span",{className:"text-amber-400 ml-2 font-mono",children:o.rss.toFixed(1)})]}),e.jsxs("div",{className:"text-sm",children:[e.jsx("span",{className:"text-dark-400",children:"R²:"}),e.jsx("span",{className:"text-blue-400 ml-2 font-mono",children:o.rSquared.toFixed(3)})]})]}),e.jsx("button",{onClick:Y,className:"mt-4 px-3 py-1.5 text-xs bg-dark-700 hover:bg-dark-600 text-dark-300 rounded-lg transition-colors",children:"Clear Points"})]})]}),e.jsx("p",{className:"mt-4 text-xs text-dark-500",children:"Click to add points. Drag points to adjust. The green line shows the least squares fit."})]})}function z(){return e.jsxs(I,{sectionId:6,children:[e.jsx("h2",{children:"Simple Linear Regression"}),e.jsxs("p",{children:["Simple linear regression is a very straightforward approach for predicting a quantitative response ",e.jsx(s,{children:"Y"})," on the basis of a single predictor variable ",e.jsx(s,{children:"X"}),". It assumes that there is approximately a linear relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),"."]}),e.jsxs(j,{title:"Simple Linear Regression Model",children:[e.jsx("p",{children:"We can write this relationship as:"}),e.jsx(a,{children:"Y \\approx \\beta_0 + \\beta_1 X"}),e.jsxs("p",{className:"mt-2",children:["Here ",e.jsx(s,{children:"\\beta_0"})," and ",e.jsx(s,{children:"\\beta_1"})," are two unknown constants that represent the ",e.jsx("em",{children:"intercept"})," and ",e.jsx("em",{children:"slope"})," terms in the linear model."]}),e.jsxs("p",{className:"mt-2",children:["Together, ",e.jsx(s,{children:"\\beta_0"})," and ",e.jsx(s,{children:"\\beta_1"})," are known as the model",e.jsx("strong",{children:" coefficients"})," or ",e.jsx("strong",{children:"parameters"}),"."]})]}),e.jsxs("p",{children:["Once we have used our training data to produce estimates ",e.jsx(s,{children:"\\hat\\beta_0"})," and ",e.jsx(s,{children:"\\hat\\beta_1"})," for the model coefficients, we can predict future values using:"]}),e.jsx(a,{children:"\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x"}),e.jsxs("p",{children:["where ",e.jsx(s,{children:"\\hat y"})," indicates a prediction of ",e.jsx(s,{children:"Y"})," on the basis of ",e.jsx(s,{children:"X = x"}),". The hat symbol denotes the estimated value."]}),e.jsx("h2",{children:"Estimating the Coefficients"}),e.jsxs("p",{children:["Let ",e.jsx(s,{children:"(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)"})," represent ",e.jsx(s,{children:"n"})," observation pairs. We want to find coefficient estimates ",e.jsx(s,{children:"\\hat\\beta_0"})," and ",e.jsx(s,{children:"\\hat\\beta_1"})," such that the resulting line is as close as possible to the ",e.jsx(s,{children:"n"})," data points."]}),e.jsxs(j,{title:"Residual Sum of Squares (RSS)",children:[e.jsxs("p",{children:["The most common approach is to minimize the ",e.jsx("em",{children:"residual sum of squares"}),":"]}),e.jsx(a,{children:"\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2"}),e.jsxs("p",{className:"mt-2",children:["The RSS measures the total squared deviation between the observed values ",e.jsx(s,{children:"y_i"})," and the values predicted by the linear model."]})]}),e.jsx("h3",{children:"Interactive Visualization"}),e.jsx("p",{children:"Try adding and moving points in the visualization below to see how the least squares regression line changes in real-time:"}),e.jsx("div",{className:"my-6",children:e.jsx(V,{})}),e.jsxs(A,{title:"Least Squares Coefficient Estimates",children:[e.jsx("p",{children:"Using calculus, one can show that the minimizers of RSS are:"}),e.jsx(a,{children:"\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}"}),e.jsx(a,{children:"\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsxs(s,{children:["\\bar y = \\frac",1,n,"\\sum_",i=1,"^n y_i"]})," and ",e.jsxs(s,{children:["\\bar x = \\frac",1,n,"\\sum_",i=1,"^n x_i"]})," are the sample means."]})]}),e.jsxs(L,{title:"Advertising and Sales",children:[e.jsxs("p",{children:["Consider predicting ",e.jsx("strong",{children:"sales"})," based on ",e.jsx("strong",{children:"TV advertising budget"}),". Using the advertising dataset with 200 markets, the least squares fit gives:"]}),e.jsx(a,{children:"\\widehat{\\text{sales}} = 7.03 + 0.0475 \\times \\text{TV}"}),e.jsxs("p",{className:"mt-2",children:[e.jsx("strong",{children:"Interpretation:"})," An additional $1,000 spent on TV advertising is associated with selling approximately 47.5 additional units of the product."]})]}),e.jsx("h2",{children:"Assessing the Accuracy of the Coefficient Estimates"}),e.jsxs("p",{children:["The true relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"})," takes the form ",e.jsx(s,{children:"Y = f(X) + \\epsilon"})," for some unknown function ",e.jsx(s,{children:"f"}),". If ",e.jsx(s,{children:"f"})," is approximated by a linear function, we can write:"]}),e.jsx(a,{children:"Y = \\beta_0 + \\beta_1 X + \\epsilon"}),e.jsxs("p",{children:["The error term ",e.jsx(s,{children:"\\epsilon"})," is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that affect ",e.jsx(s,{children:"Y"}),", and there may be measurement error."]}),e.jsxs(j,{title:"Population Regression Line",children:[e.jsxs("p",{children:["The model ",e.jsx(s,{children:"Y = \\beta_0 + \\beta_1 X + \\epsilon"})," defines the ",e.jsx("em",{children:"population regression line"}),", which is the best linear approximation to the true relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),"."]}),e.jsxs("p",{className:"mt-2",children:["The least squares regression line ",e.jsx(s,{children:"\\hat y = \\hat\\beta_0 + \\hat\\beta_1 x"})," is an estimate of this population line, based on our sample of observations."]})]}),e.jsx("h3",{children:"Standard Errors"}),e.jsxs("p",{children:["If we estimate ",e.jsx(s,{children:"\\beta_0"})," and ",e.jsx(s,{children:"\\beta_1"})," using a large number of different data sets drawn from the same population, we would get different estimates each time. The ",e.jsx("em",{children:"standard error"})," tells us the average amount by which these estimates differ from the actual value."]}),e.jsxs(j,{title:"Standard Errors of Coefficient Estimates",children:[e.jsx(a,{children:"\\text{SE}(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\right]"}),e.jsx(a,{children:"\\text{SE}(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx(s,{children:"\\sigma^2 = \\text{Var}(\\epsilon)"}),". In practice, ",e.jsx(s,{children:"\\sigma"})," is estimated using the ",e.jsx("em",{children:"residual standard error"}),":"]}),e.jsx(a,{children:"\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n-2}}"})]}),e.jsx("h3",{children:"Confidence Intervals"}),e.jsxs("p",{children:["Standard errors can be used to compute ",e.jsx("em",{children:"confidence intervals"}),". A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter."]}),e.jsxs(j,{title:"95% Confidence Interval for Coefficients",children:[e.jsxs("p",{children:["For ",e.jsx(s,{children:"\\beta_1"}),", the approximate 95% confidence interval is:"]}),e.jsx(a,{children:"\\hat{\\beta}_1 \\pm 2 \\cdot \\text{SE}(\\hat{\\beta}_1)"}),e.jsxs("p",{className:"mt-2",children:["Similarly for ",e.jsx(s,{children:"\\beta_0"}),":"]}),e.jsx(a,{children:"\\hat{\\beta}_0 \\pm 2 \\cdot \\text{SE}(\\hat{\\beta}_0)"})]}),e.jsx("h3",{children:"Hypothesis Tests"}),e.jsxs("p",{children:["Standard errors can also be used to perform ",e.jsx("em",{children:"hypothesis tests"})," on the coefficients. The most common hypothesis test involves testing:"]}),e.jsxs("div",{className:"my-6 p-4 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsxs("div",{className:"grid grid-cols-2 gap-4",children:[e.jsxs("div",{children:[e.jsx("h4",{className:"text-dark-400 text-sm font-semibold mb-1",children:"Null Hypothesis"}),e.jsx(a,{children:"H_0: \\beta_1 = 0"})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"text-dark-400 text-sm font-semibold mb-1",children:"Alternative Hypothesis"}),e.jsx(a,{children:"H_a: \\beta_1 \\neq 0"})]})]}),e.jsxs("p",{className:"text-dark-400 text-sm mt-4",children:[e.jsx(s,{children:"H_0"})," corresponds to no relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),"."]})]}),e.jsxs(j,{title:"t-Statistic",children:[e.jsxs("p",{children:["To test the null hypothesis, we compute a ",e.jsx("em",{children:"t-statistic"}),":"]}),e.jsx(a,{children:"t = \\frac{\\hat{\\beta}_1 - 0}{\\text{SE}(\\hat{\\beta}_1)} = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}"}),e.jsxs("p",{className:"mt-2",children:["This measures the number of standard deviations that ",e.jsx(s,{children:"\\hat\\beta_1"})," is away from 0. If there truly is no relationship, we expect this to have a ",e.jsx(s,{children:"t"}),"-distribution with ",e.jsx(s,{children:"n-2"})," degrees of freedom."]})]}),e.jsxs(S,{type:"info",children:[e.jsx("strong",{children:"p-value:"})," The p-value is the probability of observing a value of ",e.jsx(s,{children:"|t|"})," equal to or larger than what we observed, assuming ",e.jsx(s,{children:"H_0"})," is true. A small p-value indicates that it is unlikely to observe such a substantial association between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"})," due to chance alone."]}),e.jsx("h2",{children:"Assessing the Accuracy of the Model"}),e.jsxs("p",{children:["Once we have rejected the null hypothesis and concluded that there is a relationship between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),", we want to quantify how well the model fits the data. Two related quantities are commonly used:"]}),e.jsx("h3",{children:"Residual Standard Error (RSE)"}),e.jsxs(j,{title:"Residual Standard Error",children:[e.jsx(a,{children:"\\text{RSE} = \\sqrt{\\frac{1}{n-2}\\text{RSS}} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}"}),e.jsxs("p",{className:"mt-2",children:["The RSE is an estimate of the standard deviation of ",e.jsx(s,{children:"\\epsilon"}),". Roughly speaking, it is the average amount that the response will deviate from the true regression line."]})]}),e.jsxs("p",{children:["The RSE is measured in the units of ",e.jsx(s,{children:"Y"}),". In the advertising example, RSE = 3.26, meaning actual sales deviate from the true regression line by approximately 3,260 units on average."]}),e.jsx("h3",{children:"R-squared (R²)"}),e.jsxs(j,{title:"R-squared",children:[e.jsx(a,{children:"R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}"}),e.jsxs("p",{className:"mt-2",children:["where ",e.jsx("strong",{children:"TSS"})," (Total Sum of Squares) = ",e.jsx(s,{children:"\\sum(y_i - \\bar y)^2"})," measures the total variance in the response ",e.jsx(s,{children:"Y"}),", and RSS measures the variance that is left unexplained after performing the regression."]}),e.jsxs("p",{className:"mt-2",children:["Hence, ",e.jsx(s,{children:"R^2"})," measures the ",e.jsx("strong",{children:"proportion of variance explained"})," by the model."]})]}),e.jsxs("div",{className:"my-6 p-5 bg-dark-800/50 rounded-xl border border-dark-700",children:[e.jsx("h4",{className:"text-dark-200 font-semibold mb-3",children:"Interpreting R²"}),e.jsxs("ul",{className:"space-y-2 text-dark-300",children:[e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-emerald-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx(s,{children:"R^2 = 0"}),": The model explains none of the variability in ",e.jsx(s,{children:"Y"})]})]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-emerald-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx(s,{children:"R^2 = 1"}),": The model explains all of the variability in ",e.jsx(s,{children:"Y"})]})]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-amber-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx(s,{children:"R^2"})," close to 1: Regression explains most of the variance (good fit)"]})]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx("span",{className:"text-red-400 mt-1",children:"→"}),e.jsxs("span",{children:[e.jsx(s,{children:"R^2"})," close to 0: Regression explains little of the variance (poor fit)"]})]})]})]}),e.jsxs(S,{type:"warning",children:[e.jsx("strong",{children:"R² and Correlation:"})," In simple linear regression with one predictor,",e.jsx(s,{children:"R^2 = r^2"})," where ",e.jsx(s,{children:"r"})," is the correlation between ",e.jsx(s,{children:"X"})," and ",e.jsx(s,{children:"Y"}),". This relationship does not extend to multiple regression."]}),e.jsx("h2",{children:"R Code Example"}),e.jsx("p",{children:"Here's how to fit a simple linear regression model in R:"}),e.jsx(H,{title:"Simple Linear Regression in R",output:`Call:
lm(formula = sales ~ TV, data = Advertising)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 7.032594   0.457843  15.360  < 2e-16 ***
TV          0.047537   0.002691  17.668  < 2e-16 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.259 on 198 degrees of freedom
Multiple R-squared: 0.6119`,children:`# Load the data
Advertising <- read.csv("Advertising.csv")

# Fit simple linear regression
lm.fit <- lm(sales ~ TV, data = Advertising)

# View the summary
summary(lm.fit)

# Get confidence intervals for coefficients
confint(lm.fit)

# Make predictions
predict(lm.fit, data.frame(TV = c(50, 100, 150)))`}),e.jsx("h2",{children:"Summary"}),e.jsx("p",{children:"This section covered the fundamentals of simple linear regression:"}),e.jsxs("ul",{className:"list-disc list-inside text-dark-300 space-y-2 my-4",children:[e.jsxs("li",{children:["The model: ",e.jsx(s,{children:"Y = \\beta_0 + \\beta_1 X + \\epsilon"})]}),e.jsx("li",{children:"Estimating coefficients by minimizing RSS (least squares)"}),e.jsx("li",{children:"Standard errors, confidence intervals, and hypothesis tests"}),e.jsxs("li",{children:["Assessing fit with RSE and ",e.jsx(s,{children:"R^2"})]})]}),e.jsxs(S,{type:"success",children:[e.jsx("strong",{children:"Next Steps:"})," In the next section, we'll extend these ideas to",e.jsx("em",{children:"multiple linear regression"}),", where we predict ",e.jsx(s,{children:"Y"})," using multiple predictors ",e.jsx(s,{children:"X_1, X_2, \\ldots, X_p"}),"."]})]})}export{z as default};
