import{j as e,A as x,m as b}from"./vendor-animation-0o8UKZ_1.js";import{r as l,L as d}from"./vendor-react-Drj8qL0h.js";import{H as g,S as T}from"./Sidebar-YNcUv1-w.js";import{C as v}from"./index-dwDT9AaP.js";import"./vendor-math-p018AHG0.js";import{M as S}from"./MathBlock-uz1iP4cD.js";import"./vendor-firebase-core-BXWtuYvb.js";const r=[{id:"def-linear-space",title:"Definition of a Linear Space",statement:"Let $V$ denote a nonempty set of objects, called elements. The set $V$ is called a \\textbf{linear space} if it satisfies ten axioms: (1) Closure under addition, (2) Closure under scalar multiplication, (3) Commutative law: $x + y = y + x$, (4) Associative law: $(x + y) + z = x + (y + z)$, (5) Existence of zero element: $x + 0 = x$, (6) Existence of negatives: $x + (-1)x = 0$, (7) Associative law for scalars: $a(bx) = (ab)x$, (8) Distributive law: $a(x + y) = ax + ay$, (9) Distributive law: $(a + b)x = ax + bx$, (10) Identity: $1 \\cdot x = x$.",sectionId:0,sectionTitle:"Introduction to Linear Spaces",category:"Linear Spaces",type:"definition"},{id:"def-real-complex-linear-space",title:"Real and Complex Linear Spaces",statement:'A \\textbf{real linear space} has real numbers as scalars. If "real number" is replaced by "complex number" in the axioms, the resulting structure is called a \\textbf{complex linear space}. Linear spaces are also called \\textbf{vector spaces}, and the scalars are called multipliers.',sectionId:0,sectionTitle:"Introduction to Linear Spaces",category:"Linear Spaces",type:"definition"},{id:"thm-uniqueness-zero",title:"Uniqueness of the Zero Element",statement:"In any linear space there is one and only one zero element.",sectionId:2,sectionTitle:"Elementary Consequences of the Axioms",category:"Linear Spaces",type:"theorem",hasProof:!0,proof:"Axiom 5 tells us there is at least one zero element. Suppose there were two, say $O_1$ and $O_2$. Taking $x = O_1$ and $0 = O_2$ in Axiom 5, we obtain $O_1 + O_2 = O_1$. Similarly, taking $x = O_2$ and $0 = O_1$, we find $O_2 + O_1 = O_2$. But $O_1 + O_2 = O_2 + O_1$ by the commutative law, so $O_1 = O_2$."},{id:"thm-uniqueness-negatives",title:"Uniqueness of Negatives",statement:"In any linear space every element has exactly one negative. That is, for every $x$ there is one and only one $y$ such that $x + y = 0$.",sectionId:2,sectionTitle:"Elementary Consequences of the Axioms",category:"Linear Spaces",type:"theorem",hasProof:!0,proof:"Axiom 6 tells us that each $x$ has at least one negative, namely $(-1)x$. Suppose $x$ has two negatives, say $y_1$ and $y_2$. Then $x + y_1 = 0$ and $x + y_2 = 0$. Adding $y_2$ to both members of the first equation: $y_2 + (x + y_1) = (y_2 + x) + y_1 = 0 + y_1 = y_1$ and $y_2 + (x + y_1) = y_2 + 0 = y_2$. Therefore $y_1 = y_2$."},{id:"thm-algebraic-properties",title:"Algebraic Properties of Linear Spaces",statement:"In a given linear space, let $x$ and $y$ denote arbitrary elements and let $a$ and $b$ denote arbitrary scalars. Then: (a) $0 \\cdot x = 0$, (b) $a \\cdot 0 = 0$, (c) $(-a)x = -(ax) = a(-x)$, (d) If $ax = 0$, then either $a = 0$ or $x = 0$, (e) If $ax = ay$ and $a \\neq 0$, then $x = y$, (f) If $ax = bx$ and $x \\neq 0$, then $a = b$, (g) $-(x + y) = (-x) + (-y) = -x - y$, (h) $x + x = 2x$, and in general $\\sum_{i=1}^n x = nx$.",sectionId:2,sectionTitle:"Elementary Consequences of the Axioms",category:"Linear Spaces",type:"theorem"},{id:"def-subspace",title:"Definition of Subspace",statement:"Given a linear space $V$, let $S$ be a nonempty subset of $V$. If $S$ is also a linear space, with the same operations of addition and multiplication by scalars, then $S$ is called a \\textbf{subspace} of $V$.",sectionId:3,sectionTitle:"Subspaces of a Linear Space",category:"Linear Spaces",type:"definition"},{id:"thm-subspace-criterion",title:"Subspace Criterion",statement:"Let $S$ be a nonempty subset of a linear space $V$. Then $S$ is a subspace if and only if $S$ satisfies the closure axioms: (1) For every pair of elements $x$ and $y$ in $S$, the sum $x + y$ is in $S$. (2) For every $x$ in $S$ and every scalar $a$, the product $ax$ is in $S$.",sectionId:3,sectionTitle:"Subspaces of a Linear Space",category:"Linear Spaces",type:"theorem",hasProof:!0,proof:"If $S$ is a subspace, it satisfies all axioms, hence the closure axioms. Conversely, assume $S$ satisfies closure axioms. The commutative and associative laws, and axioms for multiplication by scalars, hold in $S$ because they hold in $V$. For Axiom 5 (zero element): let $x \\in S$. By Axiom 2, $0 \\cdot x \\in S$. But $0 \\cdot x = 0$, so $0 \\in S$. For Axiom 6 (negatives): $(-1)x \\in S$ by closure, and $x + (-1)x = 0$, so negatives exist in $S$."},{id:"def-linear-combination",title:"Linear Combination",statement:"Let $S$ be a nonempty subset of a linear space $V$. An element $x$ in $V$ of the form $x = \\sum_{i=1}^k c_i x_i$, where $x_1, \\ldots, x_k$ are all in $S$ and $c_1, \\ldots, c_k$ are scalars, is called a \\textbf{finite linear combination} of elements of $S$.",sectionId:3,sectionTitle:"Subspaces of a Linear Space",category:"Linear Spaces",type:"definition"},{id:"def-linear-span",title:"Linear Span",statement:"The set of all finite linear combinations of elements of $S$ satisfies the closure axioms and hence is a subspace of $V$. We call this the \\textbf{subspace spanned by S}, or the \\textbf{linear span} of $S$, and denote it by $L(S)$. If $S$ is empty, we define $L(S) = \\{0\\}$.",sectionId:3,sectionTitle:"Subspaces of a Linear Space",category:"Linear Spaces",type:"definition"},{id:"def-dependent-set",title:"Dependent Set",statement:"A set $S$ of elements in a linear space $V$ is called \\textbf{dependent} if there is a finite set of distinct elements in $S$, say $x_1, \\ldots, x_k$, and a corresponding set of scalars $c_1, \\ldots, c_k$, not all zero, such that $\\sum_{i=1}^k c_i x_i = 0$.",sectionId:4,sectionTitle:"Dependent and Independent Sets",category:"Linear Spaces",type:"definition"},{id:"def-independent-set",title:"Independent Set",statement:"The set $S$ is called \\textbf{independent} if it is not dependent. In this case, for all choices of distinct elements $x_1, \\ldots, x_k$ in $S$ and scalars $c_1, \\ldots, c_k$: $\\sum_{i=1}^k c_i x_i = 0 \\Rightarrow c_1 = c_2 = \\cdots = c_k = 0$.",sectionId:4,sectionTitle:"Dependent and Independent Sets",category:"Linear Spaces",type:"definition"},{id:"thm-independent-spanning-set",title:"Theorem on Dependent Sets",statement:"Let $S = \\{x_1, \\ldots, x_k\\}$ be an independent set consisting of $k$ elements in a linear space $V$, and let $L(S)$ be the subspace spanned by $S$. Then every set of $k + 1$ elements in $L(S)$ is dependent.",sectionId:4,sectionTitle:"Dependent and Independent Sets",category:"Linear Spaces",type:"theorem"},{id:"def-basis",title:"Definition of Basis",statement:"A finite set $S$ of elements in a linear space $V$ is called a \\textbf{finite basis} for $V$ if $S$ is independent and spans $V$. The space $V$ is called \\textbf{finite-dimensional} if it has a finite basis, or if $V = \\{0\\}$. Otherwise, $V$ is called \\textbf{infinite-dimensional}.",sectionId:5,sectionTitle:"Bases and Dimension",category:"Linear Spaces",type:"definition"},{id:"thm-uniqueness-dimension",title:"Uniqueness of Dimension",statement:"Let $V$ be a finite-dimensional linear space. Then every finite basis for $V$ has the same number of elements.",sectionId:5,sectionTitle:"Bases and Dimension",category:"Linear Spaces",type:"theorem",hasProof:!0,proof:"Let $S$ and $T$ be two finite bases for $V$. Suppose $S$ consists of $k$ elements and $T$ consists of $m$ elements. Since $S$ is independent and spans $V$, Theorem 1.5 tells us that every set of $k + 1$ elements in $V$ is dependent. Since $T$ is independent, $m \\leq k$. The same argument with $S$ and $T$ interchanged shows $k \\leq m$. Therefore $k = m$."},{id:"def-dimension",title:"Definition of Dimension",statement:"If a linear space $V$ has a basis of $n$ elements, the integer $n$ is called the \\textbf{dimension} of $V$. We write $n = \\dim V$. If $V = \\{0\\}$, we say $V$ has dimension $0$.",sectionId:5,sectionTitle:"Bases and Dimension",category:"Linear Spaces",type:"definition"},{id:"thm-basis-extension",title:"Basis Extension Theorem",statement:"Let $V$ be a finite-dimensional linear space with $\\dim V = n$. Then: (a) Any set of independent elements in $V$ is a subset of some basis for $V$. (b) Any set of $n$ independent elements is a basis for $V$.",sectionId:5,sectionTitle:"Bases and Dimension",category:"Linear Spaces",type:"theorem"},{id:"def-components",title:"Definition of Components",statement:"Let $V$ be a linear space of dimension $n$ and consider an ordered basis $(e_1, \\ldots, e_n)$. If $x \\in V$ can be expressed as $x = \\sum_{i=1}^n c_i e_i$, the coefficients $(c_1, \\ldots, c_n)$ are called the \\textbf{components} of $x$ relative to the ordered basis.",sectionId:5,sectionTitle:"Bases and Dimension",category:"Linear Spaces",type:"definition"},{id:"def-inner-product",title:"Definition of Inner Product",statement:"Let $V$ be a real linear space. A function that assigns to each pair of elements $x$ and $y$ in $V$ a real number $(x, y)$, called the \\textbf{inner product}, satisfies: (1) $(x, y) = (y, x)$ (symmetry), (2) $(x + y, z) = (x, z) + (y, z)$ (additivity), (3) $(cx, y) = c(x, y)$ (homogeneity), (4) $(x, x) \\geq 0$, and $(x, x) = 0$ iff $x = 0$ (positivity).",sectionId:6,sectionTitle:"Inner Products and Euclidean Spaces",category:"Linear Spaces",type:"definition"},{id:"def-euclidean-space",title:"Euclidean Space",statement:"A real linear space with an inner product is called a \\textbf{Euclidean space}.",sectionId:6,sectionTitle:"Inner Products and Euclidean Spaces",category:"Linear Spaces",type:"definition"},{id:"thm-cauchy-schwarz",title:"Cauchy-Schwarz Inequality",statement:"In a Euclidean space $V$, every inner product satisfies: $|(x, y)|^2 \\leq (x, x)(y, y)$ for all $x$ and $y$ in $V$. Moreover, equality holds if and only if $x$ and $y$ are dependent.",sectionId:6,sectionTitle:"Inner Products and Euclidean Spaces",category:"Linear Spaces",type:"theorem"},{id:"def-norm",title:"Definition of Norm",statement:"In a Euclidean space $V$, the \\textbf{norm} of an element $x$ is defined by $\\|x\\| = (x, x)^{1/2}$.",sectionId:6,sectionTitle:"Inner Products and Euclidean Spaces",category:"Linear Spaces",type:"definition"},{id:"thm-norm-properties",title:"Properties of the Norm",statement:"In a Euclidean space, every norm satisfies: (a) $\\|x\\| = 0$ if $x = 0$, (b) $\\|x\\| > 0$ if $x \\neq 0$ (positivity), (c) $\\|cx\\| = |c|\\|x\\|$ (homogeneity), (d) $\\|x + y\\| \\leq \\|x\\| + \\|y\\|$ (triangle inequality).",sectionId:6,sectionTitle:"Inner Products and Euclidean Spaces",category:"Linear Spaces",type:"theorem"},{id:"def-angle",title:"Definition of Angle",statement:"In a real Euclidean space $V$, the \\textbf{angle} between two nonzero elements $x$ and $y$ is the number $\\theta$ in $[0, \\pi]$ satisfying $\\cos \\theta = \\frac{(x, y)}{\\|x\\| \\|y\\|}$.",sectionId:6,sectionTitle:"Inner Products and Euclidean Spaces",category:"Linear Spaces",type:"definition"},{id:"def-orthogonal",title:"Definition of Orthogonality",statement:"In a Euclidean space $V$, two elements $x$ and $y$ are called \\textbf{orthogonal} if their inner product is zero: $(x, y) = 0$.",sectionId:7,sectionTitle:"Orthogonality in Euclidean Spaces",category:"Linear Spaces",type:"definition"},{id:"def-orthogonal-set",title:"Orthogonal and Orthonormal Sets",statement:"A subset $S$ of $V$ is called an \\textbf{orthogonal set} if $(x, y) = 0$ for every pair of distinct elements $x$ and $y$ in $S$. An orthogonal set is called \\textbf{orthonormal} if each of its elements has norm $1$.",sectionId:7,sectionTitle:"Orthogonality in Euclidean Spaces",category:"Linear Spaces",type:"definition"},{id:"thm-orthogonal-independent",title:"Orthogonal Sets Are Independent",statement:"In a Euclidean space $V$, every orthogonal set of nonzero elements is independent. In particular, in a finite-dimensional Euclidean space with $\\dim V = n$, every orthogonal set consisting of $n$ nonzero elements is a basis for $V$.",sectionId:7,sectionTitle:"Orthogonality in Euclidean Spaces",category:"Linear Spaces",type:"theorem",hasProof:!0,proof:"Let $S$ be an orthogonal set of nonzero elements in $V$, and suppose $\\sum_{i=1}^k c_i x_i = 0$ where each $x_i \\in S$. Taking the inner product with $x_1$ and using $(x_i, x_1) = 0$ if $i \\neq 1$, we find $c_1(x_1, x_1) = 0$. Since $(x_1, x_1) \\neq 0$ (because $x_1 \\neq 0$), we have $c_1 = 0$. Repeating for each $x_j$, all $c_j = 0$, proving independence."},{id:"thm-orthogonal-basis-components",title:"Components Relative to Orthogonal Basis",statement:"Let $V$ be a finite-dimensional Euclidean space with dimension $n$, and assume $S = \\{e_1, \\ldots, e_n\\}$ is an orthogonal basis for $V$. If $x = \\sum_{i=1}^n c_i e_i$, then $c_j = \\frac{(x, e_j)}{(e_j, e_j)}$ for $j = 1, \\ldots, n$. If $S$ is orthonormal, then $c_j = (x, e_j)$.",sectionId:7,sectionTitle:"Orthogonality in Euclidean Spaces",category:"Linear Spaces",type:"theorem"},{id:"thm-parseval",title:"Parseval's Formula",statement:"Let $V$ be a finite-dimensional Euclidean space of dimension $n$ with orthonormal basis $\\{e_1, \\ldots, e_n\\}$. Then for every pair of elements $x$ and $y$ in $V$: $(x, y) = \\sum_{i=1}^n (x, e_i)\\overline{(y, e_i)}$. In particular, $\\|x\\|^2 = \\sum_{i=1}^n |(x, e_i)|^2$.",sectionId:7,sectionTitle:"Orthogonality in Euclidean Spaces",category:"Linear Spaces",type:"theorem"},{id:"thm-gram-schmidt",title:"Gram-Schmidt Orthogonalization Theorem",statement:"Let $x_1, x_2, \\ldots$ be a finite or infinite sequence of elements in a Euclidean space $V$, and let $L(x_1, \\ldots, x_k)$ denote the subspace spanned by the first $k$ elements. Then there is a corresponding sequence $y_1, y_2, \\ldots$ with: (a) $y_k$ is orthogonal to every element in $L(y_1, \\ldots, y_{k-1})$. (b) $L(y_1, \\ldots, y_k) = L(x_1, \\ldots, x_k)$. (c) The sequence is unique up to scalar factors.",sectionId:8,sectionTitle:"The Gram-Schmidt Process",category:"Linear Spaces",type:"theorem"},{id:"def-projection",title:"Definition of Projection",statement:"If $x$ and $y$ are elements in a Euclidean space with $y \\neq 0$, the element $\\frac{(x, y)}{(y, y)} y$ is called the \\textbf{projection of $x$ along $y$}.",sectionId:8,sectionTitle:"The Gram-Schmidt Process",category:"Linear Spaces",type:"definition"},{id:"thm-orthonormal-basis-exists",title:"Existence of Orthonormal Basis",statement:"Every finite-dimensional Euclidean space has an orthonormal basis.",sectionId:8,sectionTitle:"The Gram-Schmidt Process",category:"Linear Spaces",type:"theorem"},{id:"def-orthogonal-complement",title:"Orthogonal Complement",statement:'Let $S$ be a subset of a Euclidean space $V$. The set of all elements orthogonal to $S$ is denoted by $S^\\perp$ and is called "$S$ perpendicular." When $S$ is a subspace, $S^\\perp$ is called the \\textbf{orthogonal complement} of $S$.',sectionId:9,sectionTitle:"Orthogonal Complements and Projections",category:"Linear Spaces",type:"definition"},{id:"thm-orthogonal-decomposition",title:"Orthogonal Decomposition Theorem",statement:"Let $V$ be a Euclidean space and let $S$ be a finite-dimensional subspace of $V$. Then every element $x$ in $V$ can be represented uniquely as a sum: $x = s + s^\\perp$, where $s \\in S$ and $s^\\perp \\in S^\\perp$. Moreover, $\\|x\\|^2 = \\|s\\|^2 + \\|s^\\perp\\|^2$.",sectionId:9,sectionTitle:"Orthogonal Complements and Projections",category:"Linear Spaces",type:"theorem"},{id:"def-projection-subspace",title:"Projection onto a Subspace",statement:"Let $S$ be a finite-dimensional subspace of a Euclidean space $V$, with orthonormal basis $\\{e_1, \\ldots, e_n\\}$. For $x \\in V$, the element $s = \\sum_{i=1}^n (x, e_i)e_i$ is called the \\textbf{projection of $x$ on the subspace $S$}.",sectionId:9,sectionTitle:"Orthogonal Complements and Projections",category:"Linear Spaces",type:"definition"},{id:"thm-best-approximation",title:"Best Approximation Theorem",statement:"Let $S$ be a finite-dimensional subspace of a Euclidean space $V$, and let $x$ be any element of $V$. Then the projection of $x$ on $S$ is nearer to $x$ than any other element of $S$. That is, if $s$ is the projection: $\\|x - s\\| \\leq \\|x - t\\|$ for all $t$ in $S$; equality holds if and only if $t = s$.",sectionId:9,sectionTitle:"Orthogonal Complements and Projections",category:"Linear Spaces",type:"theorem"},{id:"def-linear-transformation",title:"Definition of Linear Transformation",statement:"If $V$ and $W$ are linear spaces, a function $T: V \\to W$ is called a \\textbf{linear transformation} if: (a) $T(x + y) = T(x) + T(y)$ for all $x, y$ in $V$, (b) $T(cx) = cT(x)$ for all $x$ in $V$ and all scalars $c$. These combine into: $T(ax + by) = aT(x) + bT(y)$.",sectionId:10,sectionTitle:"Linear Transformations",category:"Linear Transformations",type:"definition"},{id:"def-null-space",title:"Null Space (Kernel)",statement:"The set of all elements in $V$ that $T$ maps onto $0$ is called the \\textbf{null space} (or \\textbf{kernel}) of $T$, denoted $N(T) = \\{x \\mid x \\in V \\text{ and } T(x) = 0\\}$.",sectionId:10,sectionTitle:"Linear Transformations",category:"Linear Transformations",type:"definition"},{id:"thm-range-subspace",title:"Range is a Subspace",statement:"The range $T(V)$ of a linear transformation $T: V \\to W$ is a subspace of $W$. Moreover, $T$ maps the zero element of $V$ onto the zero element of $W$.",sectionId:10,sectionTitle:"Linear Transformations",category:"Linear Transformations",type:"theorem"},{id:"thm-null-space-subspace",title:"Null Space is a Subspace",statement:"The null space of a linear transformation $T: V \\to W$ is a subspace of $V$.",sectionId:10,sectionTitle:"Linear Transformations",category:"Linear Transformations",type:"theorem"},{id:"def-nullity",title:"Definition of Nullity",statement:"If $V$ is finite-dimensional, the null space $N(T)$ is also finite-dimensional. The dimension of $N(T)$ is called the \\textbf{nullity} of $T$.",sectionId:11,sectionTitle:"Nullity and Rank",category:"Linear Transformations",type:"definition"},{id:"def-rank",title:"Definition of Rank",statement:"The dimension of the range $T(V)$ is called the \\textbf{rank} of $T$.",sectionId:11,sectionTitle:"Nullity and Rank",category:"Linear Transformations",type:"definition"},{id:"thm-rank-nullity",title:"Rank-Nullity Theorem (Dimension Theorem)",statement:"If $V$ is finite-dimensional, then $T(V)$ is also finite-dimensional, and: $\\dim N(T) + \\dim T(V) = \\dim V$. In other words, the nullity plus the rank of a linear transformation equals the dimension of its domain.",sectionId:11,sectionTitle:"Nullity and Rank",category:"Linear Transformations",type:"theorem",hasProof:!0,proof:"Let $n = \\dim V$ and let $e_1, \\ldots, e_k$ be a basis for $N(T)$ where $k = \\dim N(T)$. By Theorem 1.7, these are part of some basis for $V$: $e_1, \\ldots, e_k, e_{k+1}, \\ldots, e_n$ where $k + r = n$. We show that $T(e_{k+1}), \\ldots, T(e_n)$ form a basis for $T(V)$, proving $\\dim T(V) = r = n - k$."},{id:"def-composition",title:"Composition of Transformations",statement:"Let $U, V, W$ be sets. Let $T: U \\to V$ and $S: V \\to W$. The \\textbf{composition} $ST$ is the function $ST: U \\to W$ defined by $(ST)(x) = S[T(x)]$ for every $x$ in $U$.",sectionId:12,sectionTitle:"Algebraic Operations on Linear Transformations",category:"Linear Transformations",type:"definition"},{id:"thm-composition-linear",title:"Composition Preserves Linearity",statement:"If $U, V, W$ are linear spaces with the same scalars, and if $T: U \\to V$ and $S: V \\to W$ are linear transformations, then the composition $ST: U \\to W$ is linear.",sectionId:12,sectionTitle:"Algebraic Operations on Linear Transformations",category:"Linear Transformations",type:"theorem"},{id:"thm-transformation-space",title:"Space of Linear Transformations",statement:"The set $\\mathcal{L}(V, W)$ of all linear transformations from $V$ to $W$ is itself a linear space with operations: $(S + T)(x) = S(x) + T(x)$ and $(cT)(x) = cT(x)$.",sectionId:12,sectionTitle:"Algebraic Operations on Linear Transformations",category:"Linear Transformations",type:"theorem"},{id:"def-inverse",title:"Inverse of a Transformation",statement:"Let $T: V \\to W$ be one-to-one. The unique left inverse (which is also a right inverse) is denoted $T^{-1}$ and called the \\textbf{inverse} of $T$. We say $T$ is \\textbf{invertible}.",sectionId:13,sectionTitle:"Inverses and One-to-One Transformations",category:"Linear Transformations",type:"definition"},{id:"thm-one-to-one-characterization",title:"Characterization of One-to-One Linear Transformations",statement:"Let $T: V \\to W$ be a linear transformation. The following are equivalent: (a) $T$ is one-to-one on $V$. (b) $T$ is invertible and $T^{-1}: T(V) \\to V$ is linear. (c) For all $x$ in $V$, $T(x) = 0$ implies $x = 0$. That is, $N(T) = \\{0\\}$.",sectionId:13,sectionTitle:"Inverses and One-to-One Transformations",category:"Linear Transformations",type:"theorem"},{id:"thm-finite-dim-one-to-one",title:"Finite-Dimensional One-to-One Characterization",statement:"Let $T: V \\to W$ be linear with $\\dim V = n$. The following are equivalent: (a) $T$ is one-to-one. (b) If $e_1, \\ldots, e_p$ are independent in $V$, then $T(e_1), \\ldots, T(e_p)$ are independent. (c) $\\dim T(V) = n$. (d) If $\\{e_1, \\ldots, e_n\\}$ is a basis for $V$, then $\\{T(e_1), \\ldots, T(e_n)\\}$ is a basis for $T(V)$.",sectionId:13,sectionTitle:"Inverses and One-to-One Transformations",category:"Linear Transformations",type:"theorem"},{id:"def-matrix",title:"Definition of Matrix",statement:"A rectangular array with $m$ rows and $n$ columns is called an $m \\times n$ \\textbf{matrix}. The entry $t_{ik}$ is the \\textbf{ik-entry}, where $i$ indicates the row and $k$ the column.",sectionId:14,sectionTitle:"Matrix Representations",category:"Linear Transformations",type:"definition"},{id:"thm-matrix-representation",title:"Matrix Representation of Linear Transformations",statement:"Let $T \\in \\mathcal{L}(V, W)$ with $\\dim V = n$, $\\dim W = m$. Let $(t_{ik})$ be the matrix determined by $T(e_k) = \\sum_{i=1}^m t_{ik}w_i$. Then an element $x = \\sum_{k=1}^n x_k e_k$ is mapped to $T(x) = \\sum_{i=1}^m y_i w_i$ where $y_i = \\sum_{k=1}^n t_{ik}x_k$.",sectionId:14,sectionTitle:"Matrix Representations",category:"Linear Transformations",type:"theorem"},{id:"def-matrix-transpose",title:"Transpose of a Matrix",statement:"The \\textbf{transpose} of an $m \\times n$ matrix $A = (a_{ik})$ is the $n \\times m$ matrix $A^T = (a_{ki})$, obtained by interchanging rows and columns.",sectionId:15,sectionTitle:"Linear Spaces of Matrices",category:"Linear Transformations",type:"definition"},{id:"def-matrix-product",title:"Matrix Product",statement:"Let $A = (a_{ij})$ be an $m \\times n$ matrix and $B = (b_{jk})$ be an $n \\times p$ matrix. Their \\textbf{product} $AB$ is the $m \\times p$ matrix $C = (c_{ik})$ where $c_{ik} = \\sum_{j=1}^n a_{ij}b_{jk}$.",sectionId:16,sectionTitle:"Matrix Multiplication",category:"Linear Transformations",type:"definition"},{id:"thm-matrix-multiplication-properties",title:"Properties of Matrix Multiplication",statement:"Matrix multiplication satisfies: (a) Associativity: $(AB)C = A(BC)$, (b) Left distributivity: $A(B + C) = AB + AC$, (c) Right distributivity: $(A + B)C = AC + BC$, (d) Scalar compatibility: $c(AB) = (cA)B = A(cB)$, (e) Identity: $I_m A = A = AI_n$. Note: Matrix multiplication is generally \\textbf{not commutative}.",sectionId:16,sectionTitle:"Matrix Multiplication",category:"Linear Transformations",type:"theorem"},{id:"def-identity-matrix",title:"Identity Matrix",statement:"The $n \\times n$ \\textbf{identity matrix} $I_n$ has 1s on the main diagonal and 0s elsewhere.",sectionId:16,sectionTitle:"Matrix Multiplication",category:"Linear Transformations",type:"definition"},{id:"thm-transpose-product",title:"Transpose of a Product",statement:"$(AB)^T = B^T A^T$.",sectionId:16,sectionTitle:"Matrix Multiplication",category:"Linear Transformations",type:"theorem"},{id:"def-homogeneous-system",title:"Homogeneous System",statement:"A system $AX = 0$ is called \\textbf{homogeneous}. The set of all solutions forms the null space of the transformation represented by $A$.",sectionId:17,sectionTitle:"Systems of Linear Equations",category:"Linear Transformations",type:"definition"},{id:"thm-system-existence",title:"Existence of Solutions",statement:"The system $AX = B$ has a solution if and only if $B$ is in the column space (range) of $A$.",sectionId:17,sectionTitle:"Systems of Linear Equations",category:"Linear Transformations",type:"theorem"},{id:"thm-general-solution",title:"Structure of General Solution",statement:"If $X_p$ is a particular solution of $AX = B$, then the general solution is $X = X_p + X_h$, where $X_h$ is any solution of the homogeneous system $AX = 0$.",sectionId:17,sectionTitle:"Systems of Linear Equations",category:"Linear Transformations",type:"theorem"},{id:"thm-unique-solution",title:"Unique Solution Characterization",statement:"For an $n \\times n$ square matrix $A$, the following are equivalent: (a) $AX = B$ has a unique solution for every $B$, (b) $AX = 0$ has only the trivial solution, (c) The columns of $A$ are linearly independent, (d) $\\text{rank}(A) = n$, (e) $A$ is invertible.",sectionId:17,sectionTitle:"Systems of Linear Equations",category:"Linear Transformations",type:"theorem"},{id:"def-elementary-row-operations",title:"Elementary Row Operations",statement:"The three \\textbf{elementary row operations} on a matrix are: Type I: Interchange two rows, Type II: Multiply a row by a nonzero scalar, Type III: Add a multiple of one row to another row.",sectionId:18,sectionTitle:"Computation Techniques",category:"Linear Transformations",type:"definition"},{id:"def-row-echelon-form",title:"Row Echelon Form",statement:"A matrix is in \\textbf{row echelon form} if: (a) All zero rows are at the bottom, (b) The first nonzero entry (pivot) in each nonzero row is to the right of the pivot in the row above, (c) Each pivot is 1 and is the only nonzero entry in its column (for \\textbf{reduced} row echelon form).",sectionId:18,sectionTitle:"Computation Techniques",category:"Linear Transformations",type:"definition"},{id:"thm-rank-echelon",title:"Rank from Echelon Form",statement:"The rank of a matrix equals the number of nonzero rows (pivot rows) in its row echelon form.",sectionId:18,sectionTitle:"Computation Techniques",category:"Linear Transformations",type:"theorem"},{id:"def-invertible-matrix",title:"Invertible Matrix",statement:"An $n \\times n$ matrix $A$ is \\textbf{invertible} (or \\textbf{nonsingular}) if there exists an $n \\times n$ matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I_n$. A matrix that is not invertible is called \\textbf{singular}.",sectionId:19,sectionTitle:"Inverses of Square Matrices",category:"Linear Transformations",type:"definition"},{id:"thm-invertibility-characterization",title:"Characterization of Invertibility",statement:"For an $n \\times n$ matrix $A$, the following are equivalent: (a) $A$ is invertible, (b) The columns of $A$ are linearly independent, (c) The rows of $A$ are linearly independent, (d) $\\text{rank}(A) = n$, (e) $AX = 0$ has only the trivial solution, (f) $AX = B$ has a unique solution for every $B$, (g) The reduced row echelon form of $A$ is $I_n$.",sectionId:19,sectionTitle:"Inverses of Square Matrices",category:"Linear Transformations",type:"theorem"},{id:"thm-inverse-properties",title:"Properties of Matrix Inverses",statement:"If $A$ and $B$ are invertible $n \\times n$ matrices: (a) $(A^{-1})^{-1} = A$, (b) $(AB)^{-1} = B^{-1}A^{-1}$, (c) $(A^T)^{-1} = (A^{-1})^T$, (d) $(cA)^{-1} = c^{-1}A^{-1}$ for scalar $c \\neq 0$.",sectionId:19,sectionTitle:"Inverses of Square Matrices",category:"Linear Transformations",type:"theorem"},{id:"def-determinant-axioms",title:"Axiomatic Definition of Determinant",statement:"A function $d$, defined for each ordered $n$-tuple of vectors $A_1, \\ldots, A_n$ in $n$-space, is called a \\textbf{determinant function} of order $n$ if it satisfies: Axiom 1 (Homogeneity): $d(\\ldots, tA_k, \\ldots) = t \\cdot d(\\ldots, A_k, \\ldots)$, Axiom 2 (Additivity): $d(\\ldots, A_k + C, \\ldots) = d(\\ldots, A_k, \\ldots) + d(\\ldots, C, \\ldots)$, Axiom 3 (Vanishing): $d(A_1, \\ldots, A_n) = 0$ if $A_i = A_j$ for some $i \\neq j$, Axiom 4 (Normalization): $d(I_1, \\ldots, I_n) = 1$ where $I_k$ is the $k$th unit coordinate vector.",sectionId:21,sectionTitle:"Axioms for a Determinant Function",category:"Determinants",type:"definition"},{id:"thm-determinant-properties",title:"Consequences of Determinant Axioms",statement:"A determinant function satisfying Axioms 1, 2, and 3 has: (a) $d(A_1, \\ldots, A_n) = 0$ if some row is $0$, (b) Interchanging two adjacent rows reverses the sign, (c) Interchanging any two rows reverses the sign, (d) The determinant vanishes if any two rows are equal, (e) The determinant vanishes if the rows are linearly dependent.",sectionId:21,sectionTitle:"Axioms for a Determinant Function",category:"Determinants",type:"theorem"},{id:"thm-determinant-triangular",title:"Determinant of Triangular Matrix",statement:"The determinant of a diagonal or upper triangular matrix equals the product of its diagonal elements: $\\det A = a_{11}a_{22} \\cdots a_{nn}$.",sectionId:22,sectionTitle:"Computation of Determinants",category:"Determinants",type:"theorem"},{id:"thm-row-operations-determinant",title:"Row Operations and Determinants",statement:"The three elementary row operations affect the determinant as follows: (1) Interchanging two rows multiplies determinant by $-1$, (2) Multiplying a row by scalar $c \\neq 0$ multiplies determinant by $c$, (3) Adding a multiple of one row to another leaves determinant unchanged.",sectionId:22,sectionTitle:"Computation of Determinants",category:"Determinants",type:"theorem"},{id:"thm-determinant-uniqueness",title:"Uniqueness of Determinant",statement:"There exists exactly one function satisfying all four axioms for a determinant function of order $n$.",sectionId:23,sectionTitle:"The Uniqueness Theorem and Product Formula",category:"Determinants",type:"theorem"},{id:"thm-determinant-product",title:"Determinant of a Product",statement:"For any two $n \\times n$ matrices $A$ and $B$: $\\det(AB) = (\\det A)(\\det B)$.",sectionId:23,sectionTitle:"The Uniqueness Theorem and Product Formula",category:"Determinants",type:"theorem",hasProof:!0,proof:"For fixed matrix $B$, define $f(A_1, \\ldots, A_n) = \\det(AB)$ where $A$ has rows $A_1, \\ldots, A_n$. One can verify that $f$ satisfies Axioms 1, 2, and 3. By the uniqueness theorem: $f(A_1, \\ldots, A_n) = d(A_1, \\ldots, A_n) \\cdot f(I_1, \\ldots, I_n)$. But $f(I_1, \\ldots, I_n) = \\det(IB) = \\det B$, so $\\det(AB) = (\\det A)(\\det B)$."},{id:"cor-determinant-invertibility",title:"Determinant and Invertibility",statement:"Let $A$ be an $n \\times n$ matrix. Then: (a) $A$ is invertible if and only if $\\det A \\neq 0$, (b) If $A$ is invertible, then $\\det(A^{-1}) = (\\det A)^{-1}$, (c) $\\det(A^n) = (\\det A)^n$ for any positive integer $n$.",sectionId:23,sectionTitle:"The Uniqueness Theorem and Product Formula",category:"Determinants",type:"corollary"},{id:"def-minor-cofactor",title:"Minor and Cofactor",statement:"Given an $n \\times n$ matrix $A$ with $n \\geq 2$, the \\textbf{$k, j$ minor} $A_{kj}$ is the $(n-1) \\times (n-1)$ matrix obtained by deleting row $k$ and column $j$. The \\textbf{cofactor} of entry $a_{kj}$ is: $\\text{cof} \\, a_{kj} = (-1)^{k+j} \\det A_{kj}$.",sectionId:24,sectionTitle:"Cofactor Expansion and the Transpose",category:"Determinants",type:"definition"},{id:"thm-cofactor-expansion",title:"Cofactor Expansion",statement:"For any $n \\times n$ matrix $A$: Row expansion (by row $k$): $\\det A = \\sum_{j=1}^n a_{kj} \\cdot \\text{cof} \\, a_{kj} = \\sum_{j=1}^n (-1)^{k+j} a_{kj} \\det A_{kj}$. Column expansion (by column $k$): $\\det A = \\sum_{i=1}^n a_{ik} \\cdot \\text{cof} \\, a_{ik} = \\sum_{i=1}^n (-1)^{i+k} a_{ik} \\det A_{ik}$.",sectionId:24,sectionTitle:"Cofactor Expansion and the Transpose",category:"Determinants",type:"theorem"},{id:"thm-determinant-transpose",title:"Determinant of Transpose",statement:"For any $n \\times n$ matrix $A$: $\\det A = \\det A^T$.",sectionId:24,sectionTitle:"Cofactor Expansion and the Transpose",category:"Determinants",type:"theorem"},{id:"def-cofactor-matrix",title:"Cofactor Matrix (Adjugate)",statement:"The \\textbf{cofactor matrix} of an $n \\times n$ matrix $A$ is the matrix whose $(i, j)$ entry is $\\text{cof} \\, a_{ij}$. The matrix $(\\text{cof} \\, A)^T$ is called the \\textbf{adjugate} (or classical adjoint) of $A$.",sectionId:25,sectionTitle:"Cofactor Matrix and Cramers Rule",category:"Determinants",type:"definition"},{id:"thm-cofactor-identity",title:"Cofactor Matrix Identity",statement:"For any $n \\times n$ matrix $A$ with $n \\geq 2$: $A(\\text{cof} \\, A)^T = (\\det A) \\cdot I$.",sectionId:25,sectionTitle:"Cofactor Matrix and Cramers Rule",category:"Determinants",type:"theorem"},{id:"thm-inverse-formula",title:"Formula for the Inverse",statement:"A square matrix $A$ is nonsingular if and only if $\\det A \\neq 0$. When $\\det A \\neq 0$: $A^{-1} = \\frac{1}{\\det A} (\\text{cof} \\, A)^T$.",sectionId:25,sectionTitle:"Cofactor Matrix and Cramers Rule",category:"Determinants",type:"theorem"},{id:"thm-cramers-rule",title:"Cramer's Rule",statement:"If the system $AX = B$ of $n$ linear equations in $n$ unknowns has a nonsingular coefficient matrix $A$, then there is a unique solution given by: $x_j = \\frac{\\det C_j}{\\det A}$, where $C_j$ is the matrix obtained by replacing the $j$th column of $A$ with $B$.",sectionId:25,sectionTitle:"Cofactor Matrix and Cramers Rule",category:"Determinants",type:"theorem"},{id:"def-eigenvalue-eigenvector",title:"Eigenvalue and Eigenvector",statement:"Let $T: S \\to V$ be a linear transformation where $S$ is a subspace of $V$. A scalar $\\lambda$ is called an \\textbf{eigenvalue} of $T$ if there is a nonzero element $x$ in $S$ such that $T(x) = \\lambda x$. The element $x$ is called an \\textbf{eigenvector} of $T$ belonging to $\\lambda$.",sectionId:26,sectionTitle:"Eigenvalues and Eigenvectors",category:"Eigenvalues and Eigenvectors",type:"definition"},{id:"def-eigenspace",title:"Eigenspace",statement:"For an eigenvalue $\\lambda$, the \\textbf{eigenspace} $E(\\lambda)$ is the set of all $x$ such that $T(x) = \\lambda x$. This includes the zero vector and all eigenvectors belonging to $\\lambda$. Equivalently, $E(\\lambda)$ is the null space of $T - \\lambda I$.",sectionId:26,sectionTitle:"Eigenvalues and Eigenvectors",category:"Eigenvalues and Eigenvectors",type:"definition"},{id:"thm-diagonal-representation",title:"Diagonal Matrix Representation",statement:"A linear transformation $T: V \\to V$ where $\\dim V = n$ has a diagonal matrix representation if and only if there exist independent elements $u_1, \\ldots, u_n$ in $V$ and scalars $\\lambda_1, \\ldots, \\lambda_n$ such that $T(u_k) = \\lambda_k u_k$ for $k = 1, \\ldots, n$.",sectionId:26,sectionTitle:"Eigenvalues and Eigenvectors",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"def-characteristic-polynomial",title:"Characteristic Polynomial",statement:"The \\textbf{characteristic polynomial} of an $n \\times n$ matrix $A$ is $f(\\lambda) = \\det(\\lambda I - A)$. The eigenvalues of $A$ are the roots of this polynomial that lie in the underlying field of scalars.",sectionId:27,sectionTitle:"The Characteristic Polynomial",category:"Eigenvalues and Eigenvectors",type:"definition"},{id:"thm-characteristic-polynomial-properties",title:"Properties of Characteristic Polynomial",statement:"If $A$ is an $n \\times n$ matrix, then $f(\\lambda) = \\det(\\lambda I - A)$ is a polynomial of degree $n$. The leading term is $\\lambda^n$ and the constant term is $f(0) = (-1)^n \\det A$.",sectionId:27,sectionTitle:"The Characteristic Polynomial",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"def-trace",title:"Definition of Trace",statement:"The \\textbf{trace} of an $n \\times n$ matrix $A$ is the sum of its diagonal elements: $\\text{tr} \\, A = \\sum_{i=1}^n a_{ii}$.",sectionId:27,sectionTitle:"The Characteristic Polynomial",category:"Eigenvalues and Eigenvectors",type:"definition"},{id:"thm-eigenvalue-sum-product",title:"Sum and Product of Eigenvalues",statement:"For an $n \\times n$ matrix $A$ with eigenvalues $\\lambda_1, \\ldots, \\lambda_n$ (counted with multiplicity): (1) The product of all eigenvalues equals $\\det A$. (2) The sum of all eigenvalues equals $\\text{tr} \\, A$.",sectionId:27,sectionTitle:"The Characteristic Polynomial",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-distinct-eigenvalues-independent",title:"Eigenvectors of Distinct Eigenvalues",statement:"Let $u_1, \\ldots, u_k$ be eigenvectors of $T$ corresponding to distinct eigenvalues $\\lambda_1, \\ldots, \\lambda_k$. Then $u_1, \\ldots, u_k$ are linearly independent.",sectionId:28,sectionTitle:"Calculating Eigenvalues and Eigenvectors",category:"Eigenvalues and Eigenvectors",type:"theorem",hasProof:!0,proof:"By induction on $k$. If $\\sum c_i u_i = 0$, apply $T$ to get $\\sum c_i \\lambda_i u_i = 0$. Multiplying the first equation by $\\lambda_k$ and subtracting gives a relation involving only $u_1, \\ldots, u_{k-1}$. By induction, each $c_i = 0$."},{id:"thm-distinct-eigenvalues-diagonalizable",title:"Distinct Eigenvalues Imply Diagonalizability",statement:"If $\\dim V = n$ and $T: V \\to V$ has $n$ distinct eigenvalues, then: (a) The corresponding eigenvectors form a basis for $V$. (b) The matrix of $T$ relative to this basis is diagonal with eigenvalues on the diagonal.",sectionId:28,sectionTitle:"Calculating Eigenvalues and Eigenvectors",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"def-similar-matrices",title:"Similar Matrices",statement:"Two $n \\times n$ matrices $A$ and $B$ are called \\textbf{similar} if there exists a nonsingular matrix $C$ such that $B = C^{-1}AC$.",sectionId:29,sectionTitle:"Similar Matrices",category:"Eigenvalues and Eigenvectors",type:"definition"},{id:"thm-similar-same-representation",title:"Similarity and Representations",statement:"Two $n \\times n$ matrices are similar if and only if they represent the same linear transformation (relative to possibly different bases).",sectionId:29,sectionTitle:"Similar Matrices",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-similar-same-characteristic-polynomial",title:"Similar Matrices Have Same Characteristic Polynomial",statement:"Similar matrices have the same characteristic polynomial and therefore the same eigenvalues.",sectionId:29,sectionTitle:"Similar Matrices",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-diagonalization-criterion",title:"Diagonalization Criterion",statement:"An $n \\times n$ matrix $A$ is similar to a diagonal matrix if and only if $A$ has $n$ linearly independent eigenvectors. This happens if and only if for each eigenvalue $\\lambda$ of algebraic multiplicity $k$, the eigenspace $E(\\lambda)$ has dimension $k$.",sectionId:29,sectionTitle:"Similar Matrices",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-trace-properties",title:"Properties of Trace",statement:"The trace satisfies: (a) $\\text{tr}(A + B) = \\text{tr} A + \\text{tr} B$, (b) $\\text{tr}(cA) = c \\, \\text{tr} A$, (c) $\\text{tr}(AB) = \\text{tr}(BA)$, (d) $\\text{tr}(A^T) = \\text{tr} A$.",sectionId:30,sectionTitle:"Properties of the Trace",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"cor-similar-same-trace",title:"Similar Matrices Have Same Trace",statement:"Similar matrices have the same trace.",sectionId:30,sectionTitle:"Properties of the Trace",category:"Eigenvalues and Eigenvectors",type:"corollary"},{id:"thm-trace-sum-eigenvalues",title:"Trace Equals Sum of Eigenvalues",statement:"If $\\lambda_1, \\ldots, \\lambda_n$ are the eigenvalues of $A$ (counted with multiplicity), then $\\text{tr} A = \\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n$.",sectionId:30,sectionTitle:"Properties of the Trace",category:"Eigenvalues and Eigenvectors",type:"theorem"},{id:"thm-first-order-existence-uniqueness",title:"Existence and Uniqueness for First-Order Linear ODEs",statement:"Assume $P$ and $Q$ are continuous on an open interval $J$. Choose any point $a$ in $J$ and let $b$ be any real number. Then there is one and only one function $y = f(x)$ which satisfies the differential equation $y' + P(x)y = Q(x)$ and the initial condition $f(a) = b$.",sectionId:45,sectionTitle:"Review: Linear Equations of First Order",category:"Differential Equations",type:"theorem"},{id:"thm-first-order-solution-formula",title:"Solution Formula for First-Order Linear ODEs",statement:"The unique solution to $y' + P(x)y = Q(x)$ with $y(a) = b$ is given by: $f(x) = be^{-A(x)} + e^{-A(x)}\\int_a^x Q(t)e^{A(t)} \\, dt$, where $A(x) = \\int_a^x P(t) \\, dt$.",sectionId:45,sectionTitle:"Review: Linear Equations of First Order",category:"Differential Equations",type:"theorem"},{id:"def-scalar-vector-field",title:"Scalar and Vector Fields",statement:"Let $f: S \\to \\mathbb{R}^m$ where $S \\subseteq \\mathbb{R}^n$. If $n = 1$ and $m = 1$: a real-valued function of a real variable. If $n = 1$ and $m > 1$: a vector-valued function. If $n > 1$ and $m = 1$: a \\textbf{scalar field}. If $n > 1$ and $m > 1$: a \\textbf{vector field}.",sectionId:77,sectionTitle:"Functions from R^n to R^m",category:"Differential Calculus",type:"definition"},{id:"def-open-ball",title:"Open n-Ball",statement:"Let $a \\in \\mathbb{R}^n$ and $r > 0$. The \\textbf{open n-ball} of radius $r$ and center $a$ is $B(a; r) = \\{x \\in \\mathbb{R}^n : \\|x - a\\| < r\\}$.",sectionId:78,sectionTitle:"Open Balls and Open Sets",category:"Differential Calculus",type:"definition"},{id:"def-interior-point",title:"Interior Point",statement:"Let $S \\subseteq \\mathbb{R}^n$ and $a \\in S$. Then $a$ is an \\textbf{interior point} of $S$ if there exists an open n-ball $B(a; r)$ such that $B(a; r) \\subseteq S$. The set of all interior points of $S$ is called the \\textbf{interior} of $S$, denoted $\\text{int } S$.",sectionId:78,sectionTitle:"Open Balls and Open Sets",category:"Differential Calculus",type:"definition"},{id:"def-open-set",title:"Open Set",statement:"A set $S$ in $\\mathbb{R}^n$ is \\textbf{open} if all its points are interior points. That is, $S$ is open if and only if $S = \\text{int } S$.",sectionId:78,sectionTitle:"Open Balls and Open Sets",category:"Differential Calculus",type:"definition"},{id:"def-boundary",title:"Boundary",statement:"A point that is neither interior nor exterior to a set $S$ is a \\textbf{boundary point}. The \\textbf{boundary} of $S$, denoted $\\partial S$, is the set of all boundary points.",sectionId:78,sectionTitle:"Open Balls and Open Sets",category:"Differential Calculus",type:"definition"},{id:"def-line-integral",title:"Line Integral",statement:"The integral $\\int f \\cdot d\\alpha$, where $f$ is a vector field defined on a curve traced by $\\alpha$, is called a \\textbf{line integral}, \\textbf{curvilinear integral}, or \\textbf{contour integral}. The curve is called a \\textbf{path of integration}.",sectionId:101,sectionTitle:"Introduction to Line Integrals",category:"Line Integrals",type:"definition"},{id:"def-smooth-path",title:"Smooth and Piecewise Smooth Paths",statement:"Let $J = [a, b]$. A function $\\alpha: J \\to \\mathbb{R}^n$ which is continuous on $J$ is called a \\textbf{continuous path}. The path is \\textbf{smooth} if $\\alpha'$ exists and is continuous in $(a, b)$. The path is \\textbf{piecewise smooth} if $[a, b]$ can be partitioned into finitely many subintervals on each of which the path is smooth.",sectionId:101,sectionTitle:"Introduction to Line Integrals",category:"Line Integrals",type:"definition"},{id:"def-connected-set",title:"Connected Set",statement:"Let $S$ be an open set in $\\mathbb{R}^n$. The set $S$ is called \\textbf{connected} if every pair of points in $S$ can be joined by a piecewise smooth path whose graph lies in $S$.",sectionId:106,sectionTitle:"Open Connected Sets and Independence of Path",category:"Line Integrals",type:"definition"},{id:"def-path-independence",title:"Independence of Path",statement:"The line integral of $f$ is \\textbf{independent of the path in $S$} if it is independent of the path from $\\mathbf{a}$ to $\\mathbf{b}$ for every pair of points $\\mathbf{a}$ and $\\mathbf{b}$ in $S$.",sectionId:106,sectionTitle:"Open Connected Sets and Independence of Path",category:"Line Integrals",type:"definition"},{id:"thm-gradient-equivalence",title:"Gradient Equivalence Theorem",statement:"Let $f$ be a vector field continuous on an open connected set $S$ in $\\mathbb{R}^n$. Then the following three statements are equivalent: (a) $f$ is the gradient of some potential function in $S$. (b) The line integral of $f$ is independent of the path in $S$. (c) The line integral of $f$ is zero around every piecewise smooth closed path in $S$.",sectionId:110,sectionTitle:"Necessary and Sufficient Conditions for a Gradient",category:"Line Integrals",type:"theorem"},{id:"thm-component-test",title:"Component Test for Gradients",statement:"Let $f = (f_1, \\ldots, f_n)$ be a continuously differentiable vector field on an open set $S$ in $\\mathbb{R}^n$. If $f$ is a gradient on $S$, then $D_i f_j(\\mathbf{x}) = D_j f_i(\\mathbf{x})$ for $i, j = 1, \\ldots, n$ and every $\\mathbf{x}$ in $S$.",sectionId:110,sectionTitle:"Necessary and Sufficient Conditions for a Gradient",category:"Line Integrals",type:"theorem"},{id:"def-parametric-surface",title:"Parametric Surface",statement:"A \\textbf{parametric surface} is described by three equations: $x = X(u, v)$, $y = Y(u, v)$, $z = Z(u, v)$, where $(u, v)$ varies over a connected region $T$ in the uv-plane. The vector form is $\\mathbf{r}(u, v) = X(u, v)\\mathbf{i} + Y(u, v)\\mathbf{j} + Z(u, v)\\mathbf{k}$.",sectionId:137,sectionTitle:"Parametric Representation of a Surface",category:"Surface Integrals",type:"definition"},{id:"def-simple-parametric-surface",title:"Simple Parametric Surface",statement:"If the function $\\mathbf{r}$ is one-to-one on $T$, the image $\\mathbf{r}(T)$ is called a \\textbf{simple parametric surface}. In such a case, distinct points of $T$ map onto distinct points of the surface.",sectionId:137,sectionTitle:"Parametric Representation of a Surface",category:"Surface Integrals",type:"definition"},{id:"def-surface-integral",title:"Surface Integral",statement:"Let $S = \\mathbf{r}(T)$ be a parametric surface described by a differentiable function $\\mathbf{r}$, and let $f$ be a scalar field defined on $S$. The \\textbf{surface integral} of $f$ over $S$ is: $\\iint_S f \\, dS = \\iint_T f[\\mathbf{r}(u, v)] \\left\\| \\frac{\\partial \\mathbf{r}}{\\partial u} \\times \\frac{\\partial \\mathbf{r}}{\\partial v} \\right\\| du \\, dv$.",sectionId:140,sectionTitle:"Surface Integrals",category:"Surface Integrals",type:"definition"},{id:"thm-surface-area-integral",title:"Surface Area as Surface Integral",statement:"The area of a surface $S$ equals $\\iint_S dS = \\iint_T \\left\\| \\frac{\\partial \\mathbf{r}}{\\partial u} \\times \\frac{\\partial \\mathbf{r}}{\\partial v} \\right\\| du \\, dv$.",sectionId:140,sectionTitle:"Surface Integrals",category:"Surface Integrals",type:"theorem"},{id:"def-probability-space",title:"Probability Space",statement:"The mathematical theory of probability rests on three ingredients: (1) A \\textbf{sample space} $S$ representing all possible outcomes, (2) A \\textbf{Boolean algebra} $\\mathscr{B}$ of events (subsets of $S$), (3) A \\textbf{probability measure} $P$ assigning probabilities to events.",sectionId:149,sectionTitle:"Historical Introduction to Probability",category:"Probability",type:"definition"},{id:"def-exponential-distribution",title:"Exponential Distribution",statement:"A random variable $X$ has an \\textbf{exponential distribution} with parameter $\\lambda > 0$ if its distribution function is $F(t) = 1 - e^{-\\lambda t}$ for $t \\geq 0$, and $F(t) = 0$ for $t < 0$. The density function is $f(t) = \\lambda e^{-\\lambda t}$ for $t \\geq 0$.",sectionId:170,sectionTitle:"Exponential Distributions",category:"Probability",type:"definition"},{id:"thm-memoryless-property",title:"Memoryless Property",statement:"A probability distribution $F$ satisfies $\\frac{1 - F(t+s)}{1 - F(t)} = 1 - F(s)$ for all $t, s > 0$ with $F(t) < 1$ for all $t > 0$ if and only if $F$ is exponential with some parameter $\\lambda > 0$.",sectionId:170,sectionTitle:"Exponential Distributions",category:"Probability",type:"theorem"},{id:"thm-change-of-variables-density",title:"Change of Variables for Joint Densities",statement:"If $(X, Y)$ has continuous joint density $f$, and we define new random variables $U = M(X, Y)$, $V = N(X, Y)$ via a one-to-one mapping with inverse $x = Q(u,v)$, $y = R(u,v)$, then $(U, V)$ has density $g(u, v) = f[Q(u,v), R(u,v)] \\left|\\frac{\\partial(Q, R)}{\\partial(u, v)}\\right|$, where the second factor is the absolute value of the Jacobian.",sectionId:175,sectionTitle:"Distributions of Functions of Random Variables",category:"Probability",type:"theorem"},{id:"def-convolution",title:"Convolution",statement:"When $X$ and $Y$ are independent with densities $f_X$ and $f_Y$, the density of $X + Y$ is the \\textbf{convolution}: $f_{X+Y}(u) = \\int_{-\\infty}^{\\infty} f_X(x) f_Y(u - x)\\,dx = (f_X * f_Y)(u)$.",sectionId:175,sectionTitle:"Distributions of Functions of Random Variables",category:"Probability",type:"definition"},{id:"def-numerical-analysis",title:"Numerical Analysis",statement:"\\textbf{Numerical analysis} is the branch of mathematics concerned with devising and analyzing methods for obtaining numerical solutions to mathematical problems. Key questions include: How accurate is the approximation? How efficiently can it be computed? How do errors propagate through calculations?",sectionId:181,sectionTitle:"Historical Introduction to Numerical Analysis",category:"Numerical Analysis",type:"definition"}];function _(){return[...new Set(r.map(n=>n.category).filter(n=>!!n))]}function I(n){const o=n.toLowerCase();return r.filter(a=>a.title.toLowerCase().includes(o)||a.statement.toLowerCase().includes(o))}function q(){const[n,o]=l.useState(!1),[a,m]=l.useState(""),[$,f]=l.useState(null),c=a?I(a):r,h=a?[{category:"Search Results",items:c}]:_().map(i=>({category:i,items:r.filter(s=>s.category===i)})),p=i=>{f($===i?null:i)},u={theorem:"text-amber-400 bg-amber-500/10",definition:"text-blue-400 bg-blue-500/10",lemma:"text-purple-400 bg-purple-500/10",corollary:"text-green-400 bg-green-500/10",proposition:"text-cyan-400 bg-cyan-500/10"};return e.jsxs("div",{className:"min-h-screen bg-dark-950",children:[e.jsx(g,{onToggleSidebar:()=>o(!n),sidebarOpen:n}),e.jsx(T,{isOpen:n,onClose:()=>o(!1)}),e.jsx("main",{className:"pt-20 pb-12 px-4 lg:pl-80 lg:pr-8",children:e.jsxs("div",{className:"max-w-4xl mx-auto",children:[e.jsx("h1",{className:"text-3xl font-bold text-dark-100 mb-2",children:"Theorems & Definitions"}),e.jsxs("p",{className:"text-dark-400 mb-8",children:["Quick reference for all theorems and definitions in ",v]}),e.jsx("div",{className:"mb-8",children:e.jsx("input",{type:"text",placeholder:"Search theorems and definitions...",value:a,onChange:i=>m(i.target.value),className:"w-full px-4 py-3 rounded-xl bg-dark-800 border border-dark-700 text-dark-100 placeholder-dark-500 focus:outline-none focus:border-primary-500 transition-colors"})}),e.jsx("div",{className:"bg-gradient-to-br from-amber-500/10 to-dark-800/50 border border-amber-500/20 rounded-2xl p-4 mb-8",children:e.jsxs("p",{className:"text-amber-300 text-sm",children:[e.jsx("span",{className:"font-semibold",children:"Tip:"})," Click any theorem to go to its section. Many include expandable proofs with LaTeX!"]})}),e.jsx("div",{className:"space-y-8",children:h.map(({category:i,items:s})=>e.jsxs("div",{className:"space-y-3",children:[e.jsxs("h2",{className:"text-xl font-bold text-dark-200 border-b border-dark-700/50 pb-2 flex items-center gap-3",children:[e.jsx("span",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-amber-500/20 to-orange-500/10 flex items-center justify-center text-sm font-bold text-amber-400",children:s.length}),i]}),e.jsx("div",{className:"space-y-2",children:s.map(t=>e.jsx(d,{to:`/section/${t.sectionId}`,className:"block group",children:e.jsxs("div",{className:"relative overflow-hidden rounded-xl bg-dark-800/40 border border-dark-700/50 p-4 transition-all duration-200 hover:border-amber-500/30 hover:bg-dark-800/60",children:[e.jsx("div",{className:"absolute left-0 top-0 bottom-0 w-1 bg-gradient-to-b from-amber-500 to-orange-600 opacity-40 group-hover:opacity-100 transition-opacity"}),e.jsxs("div",{className:"pl-3",children:[e.jsxs("div",{className:"flex items-center gap-2 mb-2 flex-wrap",children:[t.type&&e.jsx("span",{className:`text-[10px] font-semibold uppercase tracking-wider px-2 py-0.5 rounded ${u[t.type]||"text-dark-400 bg-dark-700"}`,children:t.type}),e.jsxs("span",{className:"text-[10px] text-dark-500",children:["Section ",t.sectionId]}),t.sectionTitle&&e.jsx("span",{className:"text-[10px] text-dark-600",children:t.sectionTitle}),t.hasProof&&t.proof&&e.jsx("span",{onClick:y=>{y.preventDefault(),p(t.id)},className:"ml-auto text-[10px] text-amber-500/70 font-medium cursor-pointer hover:text-amber-400 transition-colors",children:$===t.id?"Hide Proof":"View Proof"})]}),e.jsx("h3",{className:"font-semibold text-amber-400 group-hover:text-amber-300 transition-colors",children:t.title}),e.jsx("p",{className:"text-sm text-dark-400 mt-1",children:t.statement}),t.hasProof&&t.proof&&e.jsx("div",{className:"mt-3",children:e.jsx(x,{children:$===t.id?e.jsx(b.div,{initial:{height:0,opacity:0},animate:{height:"auto",opacity:1},exit:{height:0,opacity:0},transition:{duration:.3,ease:"easeInOut"},className:"overflow-hidden",children:e.jsxs("div",{className:"pt-4 border-t border-dark-700/50",children:[e.jsx("h4",{className:"text-sm font-semibold text-amber-400 mb-2",children:"Proof:"}),e.jsx(S,{children:t.proof})]})}):null})})]})]})},t.id))})]},i))}),c.length===0&&e.jsx("div",{className:"text-center py-12 text-dark-400",children:"No theorems found matching your search."}),e.jsx("div",{className:"mt-12 pt-8 border-t border-dark-700/50 flex justify-between items-center",children:e.jsxs(d,{to:"/interactive",className:"text-primary-400 hover:text-primary-300 transition-colors flex items-center gap-2",children:[e.jsx("svg",{className:"w-4 h-4",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:e.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M12 19l9 2-9-5-9-5 5-2-9 0-9 5 5 9 12 19 19 2 12 0 0 0 9 5-2-9-5-9 0-9 5-9-5 5-2 9 0 9-5"})}),"Interactive Modules"]})})]})})]})}export{q as default};
