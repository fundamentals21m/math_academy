import{j as e}from"./vendor-animation-0o8UKZ_1.js";import{L as r,C as i}from"./Callout-BQHVajQg.js";import{I as n,M as s}from"./MathBlock-uz1iP4cD.js";import"./vendor-react-Drj8qL0h.js";import"./index-dwDT9AaP.js";import"./vendor-math-p018AHG0.js";import"./vendor-firebase-core-BXWtuYvb.js";import"./quizMap-JSsPSwPL.js";function p(){return e.jsxs(r,{sectionId:9,children:[e.jsx("h2",{children:"Orthogonal Complements and Projections"}),e.jsx("p",{children:"In this section, we study orthogonal complements and projections, which are essential tools for approximation problems in Euclidean spaces. The key result is the orthogonal decomposition theorem."}),e.jsx("h3",{children:"Orthogonal Complements"}),e.jsxs(i,{type:"info",children:[e.jsx("strong",{children:"Definition:"})," Let ",e.jsx(n,{children:"S"})," be a subset of a Euclidean space ",e.jsx(n,{children:"V"}),". An element in ",e.jsx(n,{children:"V"})," is said to be ",e.jsxs("strong",{children:["orthogonal to ",e.jsx(n,{children:"S"})]})," if it is orthogonal to every element of ",e.jsx(n,{children:"S"}),". The set of all elements orthogonal to"," ",e.jsx(n,{children:"S"})," is denoted by ",e.jsx(n,{children:"S^\\perp"}),' and is called "',e.jsx(n,{children:"S"}),' perpendicular."']}),e.jsxs("p",{children:["The set ",e.jsx(n,{children:"S^\\perp"})," is always a subspace of"," ",e.jsx(n,{children:"V"}),", whether or not ",e.jsx(n,{children:"S"})," itself is one. When ",e.jsx(n,{children:"S"})," is a subspace, ",e.jsx(n,{children:"S^\\perp"})," is called the ",e.jsx("strong",{children:"orthogonal complement"})," of ",e.jsx(n,{children:"S"}),"."]}),e.jsxs("p",{children:[e.jsx("strong",{children:"Example:"})," If ",e.jsx(n,{children:"S"})," is a plane through the origin in ",e.jsx(n,{children:"V_3"}),", then ",e.jsx(n,{children:"S^\\perp"})," is the line through the origin perpendicular to this plane."]}),e.jsx("h3",{children:"The Orthogonal Decomposition Theorem"}),e.jsxs(i,{type:"info",children:[e.jsx("strong",{children:"Theorem 1.15 (Orthogonal Decomposition Theorem):"})," Let"," ",e.jsx(n,{children:"V"})," be a Euclidean space and let ",e.jsx(n,{children:"S"})," be a finite-dimensional subspace of ",e.jsx(n,{children:"V"}),". Then every element"," ",e.jsx(n,{children:"x"})," in ",e.jsx(n,{children:"V"})," can be represented uniquely as a sum:",e.jsx(s,{children:"x = s + s^\\perp, \\quad \\text{where $s \\in S$ and $s^\\perp \\in S^\\perp$}"}),"Moreover, the norm of ",e.jsx(n,{children:"x"})," is given by the Pythagorean formula:",e.jsx(s,{children:"\\|x\\|^2 = \\|s\\|^2 + \\|s^\\perp\\|^2"})]}),e.jsxs("p",{children:[e.jsx("strong",{children:"Proof Outline:"})," Let ",e.jsx(n,{children:"\\{e_1, \\ldots, e_n\\}"})," ","be an orthonormal basis for ",e.jsx(n,{children:"S"}),". Define:"]}),e.jsx(s,{children:"s = \\sum_{i=1}^n (x, e_i)e_i, \\quad s^\\perp = x - s"}),e.jsxs("p",{children:["The element ",e.jsx(n,{children:"s"})," is the sum of the projections of"," ",e.jsx(n,{children:"x"})," along each basis element. Since ",e.jsx(n,{children:"s"})," ","is a linear combination of basis elements, ",e.jsx(n,{children:"s \\in S"}),". One can verify that ",e.jsx(n,{children:"s^\\perp"})," is orthogonal to each"," ",e.jsx(n,{children:"e_j"}),", hence to all of ",e.jsx(n,{children:"S"}),", so"," ",e.jsx(n,{children:"s^\\perp \\in S^\\perp"}),"."]}),e.jsx("h3",{children:"Projection onto a Subspace"}),e.jsxs(i,{type:"info",children:[e.jsx("strong",{children:"Definition:"})," Let ",e.jsx(n,{children:"S"})," be a finite-dimensional subspace of a Euclidean space ",e.jsx(n,{children:"V"}),", with orthonormal basis"," ",e.jsx(n,{children:"\\{e_1, \\ldots, e_n\\}"}),". For ",e.jsx(n,{children:"x \\in V"}),", the element:",e.jsx(s,{children:"s = \\sum_{i=1}^n (x, e_i)e_i"}),"is called the ",e.jsxs("strong",{children:["projection of ",e.jsx(n,{children:"x"})," on the subspace"," ",e.jsx(n,{children:"S"})]}),"."]}),e.jsx("h3",{children:"Best Approximation Theorem"}),e.jsxs(i,{type:"info",children:[e.jsx("strong",{children:"Theorem 1.16 (Approximation Theorem):"})," Let ",e.jsx(n,{children:"S"})," ","be a finite-dimensional subspace of a Euclidean space ",e.jsx(n,{children:"V"}),", and let ",e.jsx(n,{children:"x"})," be any element of ",e.jsx(n,{children:"V"}),". Then the projection of ",e.jsx(n,{children:"x"})," on ",e.jsx(n,{children:"S"})," is nearer to"," ",e.jsx(n,{children:"x"})," than any other element of ",e.jsx(n,{children:"S"}),". That is, if ",e.jsx(n,{children:"s"})," is the projection of ",e.jsx(n,{children:"x"})," on"," ",e.jsx(n,{children:"S"}),":",e.jsx(s,{children:"\\|x - s\\| \\leq \\|x - t\\|"}),"for all ",e.jsx(n,{children:"t"})," in ",e.jsx(n,{children:"S"}),"; equality holds if and only if ",e.jsx(n,{children:"t = s"}),"."]}),e.jsxs("p",{children:[e.jsx("strong",{children:"Proof:"})," By the orthogonal decomposition theorem,"," ",e.jsx(n,{children:"x = s + s^\\perp"}),". For any ",e.jsx(n,{children:"t \\in S"}),":"]}),e.jsx(s,{children:"x - t = (x - s) + (s - t) = s^\\perp + (s - t)"}),e.jsxs("p",{children:["Since ",e.jsx(n,{children:"s - t \\in S"})," and"," ",e.jsx(n,{children:"s^\\perp \\in S^\\perp"}),", this is an orthogonal decomposition:"]}),e.jsx(s,{children:"\\|x - t\\|^2 = \\|s^\\perp\\|^2 + \\|s - t\\|^2 \\geq \\|s^\\perp\\|^2 = \\|x - s\\|^2"}),e.jsx("h3",{children:"Applications"}),e.jsxs("p",{children:[e.jsx("strong",{children:"Example 1: Approximation by Trigonometric Polynomials."})," In"," ",e.jsx(n,{children:"C(0, 2\\pi)"})," with inner product"," ",e.jsx(n,{children:"(f, g) = \\int_0^{2\\pi} f(x)g(x)\\, dx"}),", let"," ",e.jsx(n,{children:"S"})," be the subspace spanned by the orthonormal functions:"]}),e.jsx(s,{children:"\\varphi_0(x) = \\frac{1}{\\sqrt{2\\pi}}, \\quad \\varphi_{2k-1}(x) = \\frac{\\cos kx}{\\sqrt{\\pi}}, \\quad \\varphi_{2k}(x) = \\frac{\\sin kx}{\\sqrt{\\pi}}"}),e.jsxs("p",{children:["The projection ",e.jsx(n,{children:"f_n"})," of ",e.jsx(n,{children:"f"})," on"," ",e.jsx(n,{children:"S"})," is the trigonometric polynomial that best approximates"," ",e.jsx(n,{children:"f"}),":"]}),e.jsx(s,{children:"f_n(x) = \\frac{1}{2}a_0 + \\sum_{k=1}^n (a_k \\cos kx + b_k \\sin kx)"}),e.jsx("p",{children:"where the Fourier coefficients are:"}),e.jsx(s,{children:"a_k = \\frac{1}{\\pi} \\int_0^{2\\pi} f(x)\\cos kx\\, dx, \\quad b_k = \\frac{1}{\\pi} \\int_0^{2\\pi} f(x)\\sin kx\\, dx"}),e.jsxs("p",{children:[e.jsx("strong",{children:"Example 2: Approximation by Polynomials."})," In"," ",e.jsx(n,{children:"C(-1, 1)"})," with ",e.jsx(n,{children:"(f, g) = \\int_{-1}^1 f(x)g(x)\\, dx"}),", using the normalized Legendre polynomials as an orthonormal basis for polynomials of degree ",e.jsx(n,{children:"\\leq n"}),", the projection of"," ",e.jsx(n,{children:"f"})," gives the best polynomial approximation of degree"," ",e.jsx(n,{children:"\\leq n"}),"."]}),e.jsxs("p",{children:["For example, the linear polynomial nearest to ",e.jsx(n,{children:"\\sin(\\pi t)"})," on"," ",e.jsx(n,{children:"[-1, 1]"})," is:"]}),e.jsx(s,{children:"f_1(t) = \\frac{3}{\\pi} t"}),e.jsxs(i,{type:"success",children:[e.jsx("strong",{children:"Key Insight:"})," The approximation theorem is one of the most useful results in applied mathematics. It tells us that the best approximation to an element in a Euclidean space by elements in a subspace is achieved by the orthogonal projection. This is the theoretical foundation for least-squares approximation, Fourier analysis, and many numerical methods."]})]})}export{p as default};
