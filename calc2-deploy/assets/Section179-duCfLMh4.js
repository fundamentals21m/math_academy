import{j as e}from"./vendor-animation-0o8UKZ_1.js";import{L as r,C as n}from"./Callout-BQHVajQg.js";import{I as i,M as s}from"./MathBlock-uz1iP4cD.js";import"./vendor-react-Drj8qL0h.js";import"./index-dwDT9AaP.js";import"./vendor-math-p018AHG0.js";import"./vendor-firebase-core-BXWtuYvb.js";import"./quizMap-JSsPSwPL.js";function m(){return e.jsxs(r,{sectionId:179,children:[e.jsx("h2",{children:"Laws of Large Numbers"}),e.jsxs("p",{children:["The ",e.jsx("strong",{children:"laws of large numbers"})," make precise the intuition that averages of many random observations stabilize near the expected value."]}),e.jsx("h3",{children:"The Setting"}),e.jsxs("p",{children:["Consider tossing a fair coin many times. If"," ",e.jsx(i,{children:"h(n)"})," is the number of heads in"," ",e.jsx(i,{children:"n"})," tosses, we expect"," ",e.jsx(i,{children:"h(n)/n \\approx 1/2"})," for large"," ",e.jsx(i,{children:"n"}),"."]}),e.jsxs("p",{children:["However, we cannot prove"," ",e.jsx(i,{children:"\\lim_{n \\to \\infty} h(n)/n = 1/2"})," in the ordinary senseâ€”it's possible (though very unlikely) for every toss to be heads! Instead, we prove a probabilistic limit."]}),e.jsx("h3",{children:"Law of Large Numbers for Bernoulli Trials"}),e.jsxs("p",{children:["Let ",e.jsx(i,{children:"X"})," count successes in"," ",e.jsx(i,{children:"n"})," independent Bernoulli trials with success probability ",e.jsx(i,{children:"p"}),". By Chebyshev's inequality:"]}),e.jsx(s,{children:"P\\left(\\left|\\frac{X}{n} - p\\right| > \\epsilon\\right) \\leq \\frac{pq}{n\\epsilon^2}"}),e.jsxs("p",{children:["Taking ",e.jsx(i,{children:"n \\to \\infty"}),":"]}),e.jsx(s,{children:"\\lim_{n \\to \\infty} P\\left(\\left|\\frac{X}{n} - p\\right| > \\epsilon\\right) = 0 \\quad \\text{for every } \\epsilon > 0"}),e.jsx("h3",{children:"The Weak Law of Large Numbers"}),e.jsxs(n,{type:"info",children:[e.jsx("strong",{children:"Theorem (Weak Law of Large Numbers):"})," Let"," ",e.jsx(i,{children:"X_1, X_2, \\ldots, X_n"})," be independent random variables with the same expectation"," ",e.jsx(i,{children:"m"})," and variance"," ",e.jsx(i,{children:"\\sigma^2"}),". Let"," ",e.jsx(i,{children:"\\bar{X} = \\frac{1}{n}\\sum_{k=1}^n X_k"}),". Then for every ",e.jsx(i,{children:"\\epsilon > 0"}),":",e.jsx(s,{children:"\\lim_{n \\to \\infty} P(|\\bar{X} - m| > \\epsilon) = 0"}),"Equivalently: ",e.jsx(i,{children:"\\lim_{n \\to \\infty} P(|\\bar{X} - m| \\leq \\epsilon) = 1"})]}),e.jsx("h3",{children:"Proof"}),e.jsxs("p",{children:["We have ",e.jsx(i,{children:"E(\\bar{X}) = m"})," and"," ",e.jsx(i,{children:"\\text{Var}(\\bar{X}) = \\sigma^2/n"}),". By Chebyshev's inequality:"]}),e.jsx(s,{children:"P(|\\bar{X} - m| > \\epsilon) \\leq \\frac{\\sigma^2}{n\\epsilon^2} \\to 0 \\quad \\text{as } n \\to \\infty"}),e.jsx("h3",{children:"Interpretation"}),e.jsxs("p",{children:["The Weak Law says that the probability of"," ",e.jsx(i,{children:"\\bar{X}"})," being far from"," ",e.jsx(i,{children:"m"})," becomes arbitrarily small as"," ",e.jsx(i,{children:"n"})," increases. This is"," ",e.jsx("strong",{children:"convergence in probability"}),"."]}),e.jsx("p",{children:"It justifies using sample averages to estimate population means, and explains why the relative frequency of an event approaches its probability over many trials."}),e.jsx("h3",{children:"The Strong Law"}),e.jsxs("p",{children:["There is also a ",e.jsx("strong",{children:"Strong Law of Large Numbers"}),", which states that ",e.jsx(i,{children:"\\bar{X} \\to m"})," with probability 1 (almost sure convergence). This is a stronger result requiring more sophisticated tools."]}),e.jsxs(n,{type:"success",children:[e.jsx("strong",{children:"Key Insight:"})," The Law of Large Numbers provides the mathematical foundation for probability's interpretation as long-run relative frequency. It justifies Monte Carlo methods, statistical estimation, and the entire edifice of applied statistics. The sample mean is a consistent estimator of the population mean."]})]})}export{m as default};
